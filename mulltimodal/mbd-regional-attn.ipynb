{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import hf_hub_download \n\nhf_hub_download(repo_id=\"ai-lab/MBD-mini\", filename=\"ptls.tar.gz\", repo_type=\"dataset\", local_dir=\"/kaggle/working/\")\nhf_hub_download(repo_id=\"ai-lab/MBD-mini\", filename=\"targets.tar.gz\", repo_type=\"dataset\", local_dir=\"/kaggle/working/\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:21:06.304521Z","iopub.execute_input":"2025-03-21T08:21:06.304815Z","iopub.status.idle":"2025-03-21T08:21:12.875684Z","shell.execute_reply.started":"2025-03-21T08:21:06.304789Z","shell.execute_reply":"2025-03-21T08:21:12.874961Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"ptls.tar.gz:   0%|          | 0.00/1.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef9038d25cab41169c37746facd0cfcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"targets.tar.gz:   0%|          | 0.00/7.93M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bf5b778ea304b5faadd8e3462ecf581"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/targets.tar.gz'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"!pip install lightning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:21:12.876554Z","iopub.execute_input":"2025-03-21T08:21:12.876814Z","iopub.status.idle":"2025-03-21T08:21:20.347062Z","shell.execute_reply.started":"2025-03-21T08:21:12.876792Z","shell.execute_reply":"2025-03-21T08:21:20.346234Z"}},"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.12.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.12.0)\nRequirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.1+cu121)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.0.post0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.12)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nDownloading lightning-2.5.1-py3-none-any.whl (818 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.5.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=\"79f2120f8d4212aceb2c60b3c89a1b6727c19cff\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:21:20.348743Z","iopub.execute_input":"2025-03-21T08:21:20.349011Z","iopub.status.idle":"2025-03-21T08:21:28.664141Z","shell.execute_reply.started":"2025-03-21T08:21:20.348988Z","shell.execute_reply":"2025-03-21T08:21:28.663469Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtoly-kiri4enko\u001b[0m (\u001b[33mbstu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!pip install pyspark\n!pip install pytorch-lifestream","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:21:28.665270Z","iopub.execute_input":"2025-03-21T08:21:28.665681Z","iopub.status.idle":"2025-03-21T08:21:42.393993Z","shell.execute_reply.started":"2025-03-21T08:21:28.665658Z","shell.execute_reply":"2025-03-21T08:21:42.392938Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\nCollecting pytorch-lifestream\n  Downloading pytorch-lifestream-0.6.0.tar.gz (163 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.4/163.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: duckdb in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (1.1.3)\nCollecting hydra-core>=1.1.2 (from pytorch-lifestream)\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: numpy>=1.21.5 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (1.26.4)\nRequirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (2.3.0)\nRequirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (2.2.3)\nRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (19.0.1)\nRequirement already satisfied: pytorch-lightning>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (2.5.0.post0)\nRequirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (1.2.2)\nRequirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (2.5.1+cu121)\nRequirement already satisfied: torchmetrics>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (1.6.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (4.47.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1.2->pytorch-lifestream) (4.9.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1.2->pytorch-lifestream) (24.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.5->pytorch-lifestream) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.5->pytorch-lifestream) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.5->pytorch-lifestream) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.5->pytorch-lifestream) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.5->pytorch-lifestream) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.5->pytorch-lifestream) (2.4.1)\nRequirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf->pytorch-lifestream) (6.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-lifestream) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-lifestream) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-lifestream) (2025.1)\nRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.0->pytorch-lifestream) (4.67.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (2024.12.0)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.0->pytorch-lifestream) (4.12.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.0->pytorch-lifestream) (0.12.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-lifestream) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-lifestream) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-lifestream) (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lifestream) (3.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lifestream) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lifestream) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lifestream) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.12.0->pytorch-lifestream) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-lifestream) (0.29.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-lifestream) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-lifestream) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-lifestream) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-lifestream) (0.4.5)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (3.11.12)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (75.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-lifestream) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lifestream) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.5->pytorch-lifestream) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.5->pytorch-lifestream) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.5->pytorch-lifestream) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.5->pytorch-lifestream) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->pytorch-lifestream) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->pytorch-lifestream) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->pytorch-lifestream) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->pytorch-lifestream) (2025.1.31)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (1.18.3)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.5->pytorch-lifestream) (2024.2.0)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pytorch-lifestream\n  Building wheel for pytorch-lifestream (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pytorch-lifestream: filename=pytorch_lifestream-0.6.0-py3-none-any.whl size=274670 sha256=159921ac49ba1610e1c1b53ce4b3b4d2fccca57889f2a84c21ba25648734b543\n  Stored in directory: /root/.cache/pip/wheels/90/76/b4/0a944bc7c5a69201e4d757cc54886971117a2a581740e7f11d\nSuccessfully built pytorch-lifestream\nInstalling collected packages: hydra-core, pytorch-lifestream\nSuccessfully installed hydra-core-1.3.2 pytorch-lifestream-0.6.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!tar -xf ptls.tar.gz\n!tar -xf targets.tar.gz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:21:42.395126Z","iopub.execute_input":"2025-03-21T08:21:42.395412Z","iopub.status.idle":"2025-03-21T08:21:59.438518Z","shell.execute_reply.started":"2025-03-21T08:21:42.395353Z","shell.execute_reply":"2025-03-21T08:21:59.437555Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom pyspark.sql import types as T\nimport time\nimport datetime\nfrom ptls.data_load.datasets import ParquetDataset, ParquetFiles\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType\nfrom tqdm.notebook import tqdm\nfrom ptls.preprocessing import PysparkDataPreprocessor\nimport pytorch_lightning as pl\nfrom ptls.data_load.datasets import MemoryMapDataset\nfrom ptls.data_load.iterable_processing import SeqLenFilter, FeatureFilter\nfrom ptls.data_load.iterable_processing.iterable_seq_len_limit import ISeqLenLimit\nfrom ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\nfrom ptls.frames.coles import CoLESModule\nfrom ptls.frames import PtlsDataModule\nfrom ptls.frames.coles import ColesDataset\nfrom ptls.frames.coles.split_strategy import SampleSlices\nimport torch\nimport numpy as np\nimport pandas as pd\nimport calendar\nfrom glob import glob\nfrom ptls.data_load.utils import collate_feature_dict\n\nfrom ptls.data_load.iterable_processing_dataset import IterableProcessingDataset\nfrom datetime import datetime\nfrom ptls.data_load.padded_batch import PaddedBatch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:21:59.439674Z","iopub.execute_input":"2025-03-21T08:21:59.439912Z","iopub.status.idle":"2025-03-21T08:22:22.377530Z","shell.execute_reply.started":"2025-03-21T08:21:59.439888Z","shell.execute_reply":"2025-03-21T08:22:22.376816Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"spark_conf = pyspark.SparkConf()\nspark_conf.setMaster(\"local[*]\").setAppName(\"JoinModality\")\nspark_conf.set(\"spark.driver.maxResultSize\", \"16g\")\nspark_conf.set(\"spark.executor.memory\", \"32g\")\nspark_conf.set(\"spark.executor.memoryOverhead\", \"16g\")\nspark_conf.set(\"spark.driver.memory\", \"32g\")\nspark_conf.set(\"spark.driver.memoryOverhead\", \"16g\")\nspark_conf.set(\"spark.cores.max\", \"8\")\nspark_conf.set(\"spark.sql.shuffle.partitions\", \"200\")\nspark_conf.set(\"spark.local.dir\", \"../../spark_local_dir\")\n\n\nspark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sparkContext.getConf().getAll()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:52:47.374930Z","iopub.execute_input":"2025-03-21T08:52:47.375290Z","iopub.status.idle":"2025-03-21T08:52:47.526698Z","shell.execute_reply.started":"2025-03-21T08:52:47.375260Z","shell.execute_reply":"2025-03-21T08:52:47.525804Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[('spark.driver.extraJavaOptions',\n  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n ('spark.driver.port', '38855'),\n ('spark.executor.memoryOverhead', '16g'),\n ('spark.driver.memory', '32g'),\n ('spark.driver.maxResultSize', '16g'),\n ('spark.app.name', 'JoinModality'),\n ('spark.local.dir', '../../spark_local_dir'),\n ('spark.executor.id', 'driver'),\n ('spark.driver.host', '1cbfe3ade29f'),\n ('spark.app.startTime', '1742547167398'),\n ('spark.executor.memory', '32g'),\n ('spark.rdd.compress', 'True'),\n ('spark.driver.memoryOverhead', '16g'),\n ('spark.executor.extraJavaOptions',\n  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.cores.max', '8'),\n ('spark.sql.shuffle.partitions', '200'),\n ('spark.master', 'local[*]'),\n ('spark.submit.pyFiles', ''),\n ('spark.submit.deployMode', 'client'),\n ('spark.app.submitTime', '1742545346005'),\n ('spark.ui.showConsoleProgress', 'true'),\n ('spark.app.id', 'local-1742547167450')]"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"!mkdir /kaggle/working/mm_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:22:28.409988Z","iopub.execute_input":"2025-03-21T08:22:28.410239Z","iopub.status.idle":"2025-03-21T08:22:28.551409Z","shell.execute_reply.started":"2025-03-21T08:22:28.410216Z","shell.execute_reply":"2025-03-21T08:22:28.550208Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"TRX_DATA_PATH = '/kaggle/working/ptls/trx/'\nGEO_DATA_PATH = '/kaggle/working/ptls/geo/'\nDIAL_DATA_PATH = '/kaggle/working/ptls/dialog/'\n\nMM_DATA_PATH = '/kaggle/working/mm_dataset'\nMMT_DATA_PATH = '/kaggle/working/mm_dataset_supervised'\n\nTARGETS_DATA_PATH = '/kaggle/working/targets/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:22:28.553554Z","iopub.execute_input":"2025-03-21T08:22:28.553815Z","iopub.status.idle":"2025-03-21T08:22:28.557963Z","shell.execute_reply.started":"2025-03-21T08:22:28.553790Z","shell.execute_reply":"2025-03-21T08:22:28.557138Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def rename_col(df, prefix, col_id='client_id'):\n    new_column_names = [f\"{prefix}_{col}\" for col in df.columns if col != col_id]\n    old_column_names = [col for col in df.columns if col != col_id]\n    for old_col, new_col in zip(old_column_names, new_column_names):\n        df = df.withColumnRenamed(old_col, new_col)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:22:28.559045Z","iopub.execute_input":"2025-03-21T08:22:28.559327Z","iopub.status.idle":"2025-03-21T08:22:28.575571Z","shell.execute_reply.started":"2025-03-21T08:22:28.559298Z","shell.execute_reply":"2025-03-21T08:22:28.574837Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from ptls.preprocessing import PysparkDataPreprocessor\nfrom pyspark.sql.functions import explode, col\n\n\nfor fold in tqdm(range(0, 5)):\n    trx = spark.read.parquet(os.path.join(TRX_DATA_PATH, f'fold={fold}'))\n    dial = spark.read.parquet(os.path.join(DIAL_DATA_PATH, f'fold={fold}'))\n    \n    trx = rename_col(trx, 'trx')\n    dial = rename_col(dial, 'dial')\n    \n    mm_dataset = trx.join(dial, on='client_id', how='outer').drop(*['trx_src_type21', 'trx_src_type31'])\n\n    mm_dataset.write.mode('overwrite').parquet(os.path.join(MM_DATA_PATH, f'fold={fold}'))\n    \n    del trx\n    del dial\n    del mm_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:22:28.576576Z","iopub.execute_input":"2025-03-21T08:22:28.576868Z","iopub.status.idle":"2025-03-21T08:23:32.573555Z","shell.execute_reply.started":"2025-03-21T08:22:28.576835Z","shell.execute_reply":"2025-03-21T08:23:32.572628Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b816752ada5e4b7186eee74d5d6de6d4"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"spark.stop()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:32.574229Z","iopub.execute_input":"2025-03-21T08:23:32.574555Z","iopub.status.idle":"2025-03-21T08:23:33.235935Z","shell.execute_reply.started":"2025-03-21T08:23:32.574519Z","shell.execute_reply":"2025-03-21T08:23:33.235183Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom ptls.data_load.iterable_processing_dataset import IterableProcessingDataset\nfrom ptls.data_load import IterableChain\nfrom datetime import datetime\nfrom ptls.data_load.datasets.parquet_dataset import ParquetDataset, ParquetFiles\nfrom ptls.data_load.iterable_processing.feature_filter import FeatureFilter\nfrom ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\nimport torch\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom ptls.data_load.utils import collate_feature_dict\nfrom tqdm import tqdm\n\n\nclass TargetToTorch(IterableProcessingDataset):\n    def __init__(self, col_target):\n        super().__init__()\n        self.col_target = col_target\n\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features[self.col_target] = np.stack(np.array(features[self.col_target]))\n            features[self.col_target] = torch.tensor(features[self.col_target])\n            yield features\n\nclass DeleteNan(IterableProcessingDataset):\n    def __init__(self, col_name):\n        super().__init__()\n        self.col_name = col_name\n    \n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            if features[self.col_name] is not None:\n                yield features\n\n\nclass DialToTorch(IterableProcessingDataset):\n    def __init__(self, col_time, col_embeds):\n        super().__init__()\n        self._year=2022\n        self.col_embeds = col_embeds\n        self.col_time = col_time\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features = features.copy()\n            if features[self.col_time] is None:\n                features[self.col_time] = torch.tensor([0])\n            if features[self.col_embeds] is None:\n                features[self.col_embeds] = torch.zeros(768)\n            \n            for key, tens in features.items():\n                if key == self.col_embeds:\n                    features[key] = torch.tensor(tens.tolist())\n\n            yield features\n\nclass GetSplit(IterableProcessingDataset):\n    def __init__(\n        self,\n        start_month,\n        end_month,\n        year=2022,\n        col_id='client_id',\n        col_time='event_time'\n    ):\n        super().__init__()\n        self.start_month = start_month\n        self.end_month = end_month\n        self._year = year\n        self._col_id = col_id\n        self._col_time = col_time\n        \n    def __iter__(self):\n        for rec in self._src:\n            for month in range(self.start_month, self.end_month+1):\n                features = rec[0] if type(rec) is tuple else rec\n                features = features.copy()\n                \n                if month == 12:\n                    month_event_time = datetime(self._year + 1, 1, 1).timestamp()\n                else:\n                    month_event_time = datetime(self._year, month + 1, 1).timestamp()\n                    \n                year_event_time = datetime(self._year, 1, 1).timestamp()\n                \n                mask = features[self._col_time] < month_event_time\n                \n                for key, tensor in features.items():\n                    if key.startswith('target'):\n                        features[key] = tensor[month - 1].tolist()    \n                    elif key != self._col_id:\n                        features[key] = tensor[mask] \n                            \n                features[self._col_id] += '_month=' + str(month)\n\n                yield features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:33.236897Z","iopub.execute_input":"2025-03-21T08:23:33.237205Z","iopub.status.idle":"2025-03-21T08:23:33.250620Z","shell.execute_reply.started":"2025-03-21T08:23:33.237172Z","shell.execute_reply":"2025-03-21T08:23:33.249799Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from ptls.data_load.datasets import ParquetDataset\nfrom ptls.data_load.iterable_processing import SeqLenFilter\nfrom ptls.data_load.iterable_processing.iterable_seq_len_limit import ISeqLenLimit\nfrom ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\ntrain = ParquetDataset(\n    data_files=[\n        os.path.join(MM_DATA_PATH, f'fold={0}'),\n        os.path.join(MM_DATA_PATH, f'fold={1}'),\n        os.path.join(MM_DATA_PATH, f'fold={2}')\n    ],\n    i_filters=[\n        DeleteNan('trx_event_time'),\n        DeleteNan('dial_event_time'),\n        SeqLenFilter(min_seq_len=8),\n        ISeqLenLimit(max_seq_len=128),\n        ToTorch(),\n        DialToTorch(col_time='dial_event_time', col_embeds='dial_embedding'),\n        # GetSplit(\n        #     start_month=1,\n        #     end_month=12,\n        #     col_id='client_id'\n        # )\n    ],\n    shuffle_files=True\n)\nvalid = ParquetDataset(\n    data_files=[\n        os.path.join(MM_DATA_PATH, f'fold={3}')\n    ],\n    i_filters=[\n        DeleteNan('trx_event_time'),\n        DeleteNan('dial_event_time'),\n        SeqLenFilter(min_seq_len=8),\n        ISeqLenLimit(max_seq_len=128),\n        ToTorch(),\n        DialToTorch(col_time='dial_event_time', col_embeds='dial_embedding'),\n        # GetSplit(\n        #     start_month=1,\n        #     end_month=12,\n        #     col_id='client_id'\n        # )\n    ],\n    shuffle_files=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:33.251526Z","iopub.execute_input":"2025-03-21T08:23:33.251792Z","iopub.status.idle":"2025-03-21T08:23:33.278798Z","shell.execute_reply.started":"2025-03-21T08:23:33.251771Z","shell.execute_reply":"2025-03-21T08:23:33.277632Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from ptls.data_load.feature_dict import FeatureDict\nfrom collections import defaultdict\nfrom functools import reduce\nimport torch.nn.functional as F\n\n\n# def split_and_pad(tensor: torch.Tensor, segment_length):\n#     segments = [tensor[i:i + segment_length] for i in range(0, len(tensor), segment_length)]\n#     padded_segments = [F.pad(segment, (0, segment_length - len(segment)), mode='constant') for segment in segments]\n#     return torch.vstack(padded_segments)\n\n# def get_regional_splits(batch: list[dict], segment_length):\n#     regional_tokens = []\n#     for item in batch:\n#         segmented_data = {key: split_and_pad(tensor, segment_length) for key, tensor in item.items()}\n#         regional_tokens.append(segmented_data)\n#     return regional_tokens\n\nclass MultiModalDiffSplitDataset(FeatureDict, torch.utils.data.Dataset):\n    def __init__(\n        self,\n        data,\n        splitters,\n        source_features,\n        col_id,\n        source_names,\n        col_time='event_time',\n        *args, **kwargs\n    ):\n        \"\"\"\n        Dataset for multimodal learning.\n        Parameters:\n        -----------\n        data:\n            concatinated data with feature dicts.\n        splitter:\n            object from from `ptls.frames.coles.split_strategy`.\n            Used to split original sequence into subsequences which are samples from one client.\n        source_features:\n            list of column names \n        col_id:\n            column name with user_id\n        source_names:\n            column name with name sources\n        col_time:\n            column name with event_time\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        \n        self.data = data\n        self.splitters = splitters\n        self.col_time = col_time\n        self.col_id = col_id\n        self.source_names = source_names\n        self.source_features = source_features\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        feature_arrays = self.data[idx]\n        split_data = self.split_source(feature_arrays)\n        # print(self.get_split(split_data))\n        return self.get_splits(split_data)\n    \n    def __iter__(self):\n        for feature_arrays in self.data:\n            split_data = self.split_source(feature_arrays)\n            yield self.get_splits(split_data)\n            \n    def split_source(self, feature_arrays):\n        res = defaultdict(dict)\n        for feature_name, feature_array in feature_arrays.items():\n            if feature_name == self.col_id:\n                res[self.col_id] = feature_array\n            else:\n                source_name, feature_name_transform = self.get_names(feature_name)\n                res[source_name][feature_name_transform] = feature_array\n        for source in self.source_names:\n            if source not in res:\n                res[source] = {source_feature: torch.tensor([]) for source_feature in self.source_features[source]}\n        # print(f'res = {res}')\n        return res\n    \n    def get_names(self, feature_name):\n        idx_del = feature_name.find('_')\n        return feature_name[:idx_del], feature_name[idx_del + 1:]\n    \n    def get_splits(self, feature_arrays):\n        res = {}\n        for source_name, feature_array in feature_arrays.items():\n            if source_name != self.col_id:\n                local_date = feature_array[self.col_time]\n                if source_name not in self.splitters:\n                    continue\n                indexes = self.splitters[source_name].split(local_date)\n                res[source_name] = [{k: v[ix] for k, v in feature_array.items() if self.is_seq_feature(k, v)} for ix in indexes]\n        return res\n    \n    #Вернуть диалоги с транзакциями (без таргетов)\n    def collate_fn(self, batch, return_dct_labels=False):\n        dict_class_labels = get_dict_class_labels(batch)\n        batch = reduce(lambda x, y: {k: x[k] + y[k] for k in x if k in y}, batch)\n        # Get regional split only for transactions from MBD\n        # batch['reg_trx_tokens'] = get_regional_splits(batch['trx'], segment_length=3)\n        # print(f\"{batch=}\")\n        padded_batch = collate_multimodal_feature_dict(batch)\n        if return_dct_labels:\n            return padded_batch, dict_class_labels\n        return padded_batch, dict_class_labels[list(dict_class_labels.keys())[0]]\n\ndef collate_multimodal_feature_dict(batch):\n    res = {}\n    for source, source_batch in batch.items():\n        res[source] = collate_feature_dict(source_batch)\n    # print(f\"multimodal_feature_dict = {res['trx'].payload['event_time'].size()}\")\n    # print()\n    return res\n\ndef collate_feature_dict(batch):\n    new_x_ = defaultdict(list)\n    for i, x in enumerate(batch):\n        for k, v in x.items():\n            new_x_[k].append(v)\n    \n    seq_col = next(k for k, v in batch[0].items() if FeatureDict.is_seq_feature(k, v))\n    lengths = torch.LongTensor([len(rec[seq_col]) for rec in batch])\n    new_x = {}\n    for k, v in new_x_.items():\n        if type(v[0]) is torch.Tensor:\n            if k.startswith('target'):\n                new_x[k] = torch.stack(v, dim=0)\n            else:\n                new_x[k] = torch.nn.utils.rnn.pad_sequence(v, batch_first=True)\n        elif type(v[0]) is np.ndarray:\n            new_x[k] = v  # list of arrays[object]\n        else:\n            v = np.array(v)\n            if v.dtype.kind == 'i':\n                new_x[k] = torch.from_numpy(v).long()\n            elif v.dtype.kind == 'f':\n                new_x[k] = torch.from_numpy(v).float()\n            elif v.dtype.kind == 'b':\n                new_x[k] = torch.from_numpy(v).bool()\n            else:\n                new_x[k] = v\n    return PaddedBatch(new_x, lengths)\n    \ndef get_dict_class_labels(batch):\n    res = defaultdict(list)\n    for i, samples in enumerate(batch):\n        for source, values in samples.items():\n            for _ in values:\n                res[source].append(i)\n    for source in res:\n        res[source] = torch.LongTensor(res[source])\n    return dict(res)\n\nclass MultiModalDiffSplitIterableDataset(MultiModalDiffSplitDataset, torch.utils.data.IterableDataset):\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:33.280017Z","iopub.execute_input":"2025-03-21T08:23:33.280272Z","iopub.status.idle":"2025-03-21T08:23:33.301547Z","shell.execute_reply.started":"2025-03-21T08:23:33.280249Z","shell.execute_reply":"2025-03-21T08:23:33.300702Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from ptls.frames.coles import MultiModalIterableDataset\nfrom lightning.pytorch.loggers import WandbLogger\nimport ptls\n\ndata_module = PtlsDataModule(\n    train_data=MultiModalDiffSplitIterableDataset(\n        data=train,\n        splitters= {\n            'trx': SampleSlices(\n                split_count=3,\n                cnt_min=16,\n                cnt_max=90\n            ),\n            'dial': SampleSlices(\n                split_count=3,\n                cnt_min=2,\n                cnt_max=10\n            ),\n        },\n        source_features={\n            \"trx\": [\n                \"event_type\",\n                \"event_subtype\",\n                \"src_type11\",\n                \"src_type12\",\n                \"dst_type11\",\n                \"dst_type12\",\n                \"src_type22\",\n                \"src_type32\"\n            ],\n            \"dial\": [\n                \"embedding\"\n            ],\n        },\n        col_id='client_id',\n        col_time='event_time',\n        source_names=['trx', 'dial'],\n    ),\n    valid_data=MultiModalDiffSplitIterableDataset(\n        data=valid,\n        splitters= {\n            'trx': SampleSlices(\n                split_count=2,\n                cnt_min=5,\n                cnt_max=64\n            ),\n            'dial': SampleSlices(\n                split_count=2,\n                cnt_min=2,\n                cnt_max=10\n            ),\n            },\n        source_features={\n            \"trx\": [\n                \"event_type\",\n                \"event_subtype\",\n                \"src_type11\",\n                \"src_type12\",\n                \"dst_type11\",\n                \"dst_type12\",\n                \"src_type22\",\n                \"src_type32\"\n            ],\n            \"dial\": [\n                \"embedding\"\n            ],\n        },\n        col_id='client_id',\n        col_time='event_time',\n        source_names=['trx', 'dial'],\n    ),\n    train_batch_size=64,\n    train_num_workers=0,\n    valid_batch_size=64,\n    valid_num_workers=0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:33.302312Z","iopub.execute_input":"2025-03-21T08:23:33.302563Z","iopub.status.idle":"2025-03-21T08:23:33.435261Z","shell.execute_reply.started":"2025-03-21T08:23:33.302542Z","shell.execute_reply":"2025-03-21T08:23:33.434571Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# train_data_test=MultiModalDiffSplitIterableDataset(\n#         data=train,\n#         splitters= {\n#             'trx': SampleSlices(\n#                 split_count=3,\n#                 cnt_min=16,\n#                 cnt_max=90\n#             ),\n#             'dial': SampleSlices(\n#                 split_count=3,\n#                 cnt_min=2,\n#                 cnt_max=10\n#             ),\n#         },\n#         source_features={\n#             \"trx\": [\n#                 \"event_type\",\n#                 \"event_subtype\",\n#                 \"src_type11\",\n#                 \"src_type12\",\n#                 \"dst_type11\",\n#                 \"dst_type12\",\n#                 \"src_type22\",\n#                 \"src_type32\"\n#             ],\n#             \"dial\": [\n#                 \"embedding\"\n#             ],\n#         },\n#         col_id='client_id',\n#         col_time='event_time',\n#         source_names=['trx', 'dial'],\n#     )\n\n# next(iter(data_module.train_dl(train_data_test)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:33.436065Z","iopub.execute_input":"2025-03-21T08:23:33.436343Z","iopub.status.idle":"2025-03-21T08:23:33.440058Z","shell.execute_reply.started":"2025-03-21T08:23:33.436313Z","shell.execute_reply":"2025-03-21T08:23:33.439134Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Add region attention in trx encoder","metadata":{}},{"cell_type":"code","source":"# x_new = torch.rand((192, 87, 32))\n# segment_length = 10\n# pad_length = (segment_length - (x_new.size()[1] % segment_length)) % segment_length\n# padded_x_new = F.pad(x_new, ((0, 0, 0, pad_length, 0, 0)), 'constant', 0)\n# segmented_tensors = torch.stack(torch.split(padded_x_new, segment_length, dim=1), dim=0)\n# segmented_tensors = segmented_tensors.permute(0, 2, 1, 3)\n# segmented_tensors.size()\n\n# a = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n# b = torch.Tensor([[4, 5, 6]])\n# a = torch.cat((a, b), dim=0)\n# a\n# torch.Tensor([])\n\na = torch.rand((128, 64, 32))\nb = torch.rand((128, 64))\n\nprint(b[:, :, None].size())\n\nc = a + b[:, :, None]\nprint(a)\nprint(c)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:33.440920Z","iopub.execute_input":"2025-03-21T08:23:33.441109Z","iopub.status.idle":"2025-03-21T08:23:33.544036Z","shell.execute_reply.started":"2025-03-21T08:23:33.441092Z","shell.execute_reply":"2025-03-21T08:23:33.543246Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128, 64, 1])\ntensor([[[0.7013, 0.0334, 0.5226,  ..., 0.6394, 0.8492, 0.1728],\n         [0.4475, 0.3207, 0.1012,  ..., 0.6254, 0.1747, 0.1070],\n         [0.0326, 0.3773, 0.8816,  ..., 0.2349, 0.8341, 0.6058],\n         ...,\n         [0.3506, 0.3308, 0.9347,  ..., 0.6848, 0.3605, 0.6000],\n         [0.2813, 0.0762, 0.4244,  ..., 0.4761, 0.5369, 0.7938],\n         [0.4455, 0.1188, 0.9870,  ..., 0.9745, 0.5996, 0.7400]],\n\n        [[0.2323, 0.3172, 0.5107,  ..., 0.6289, 0.1605, 0.5481],\n         [0.6961, 0.2836, 0.9045,  ..., 0.4382, 0.8358, 0.9867],\n         [0.8378, 0.6107, 0.6769,  ..., 0.6152, 0.3699, 0.6127],\n         ...,\n         [0.8280, 0.4650, 0.7116,  ..., 0.2794, 0.7824, 0.7994],\n         [0.0829, 0.2046, 0.2542,  ..., 0.4402, 0.1923, 0.5394],\n         [0.1528, 0.5118, 0.7991,  ..., 0.3359, 0.7828, 0.1948]],\n\n        [[0.2130, 0.5340, 0.2873,  ..., 0.0637, 0.3751, 0.8721],\n         [0.3937, 0.2277, 0.6594,  ..., 0.0507, 0.6336, 0.0556],\n         [0.8778, 0.4702, 0.1566,  ..., 0.4985, 0.0606, 0.9990],\n         ...,\n         [0.6017, 0.2061, 0.4481,  ..., 0.2980, 0.2406, 0.4157],\n         [0.6151, 0.4653, 0.1986,  ..., 0.4130, 0.0421, 0.6182],\n         [0.7542, 0.1202, 0.5008,  ..., 0.8635, 0.2430, 0.3343]],\n\n        ...,\n\n        [[0.4432, 0.5623, 0.7330,  ..., 0.3928, 0.6016, 0.1010],\n         [0.2487, 0.8149, 0.9149,  ..., 0.8102, 0.9197, 0.6096],\n         [0.1942, 0.0713, 0.9970,  ..., 0.1101, 0.6681, 0.6210],\n         ...,\n         [0.4429, 0.0590, 0.2519,  ..., 0.5166, 0.8839, 0.2325],\n         [0.5883, 0.9795, 0.8563,  ..., 0.6376, 0.0955, 0.8405],\n         [0.2831, 0.7224, 0.3734,  ..., 0.2783, 0.9773, 0.3830]],\n\n        [[0.5429, 0.0505, 0.7522,  ..., 0.6184, 0.2610, 0.2844],\n         [0.9134, 0.0526, 0.5171,  ..., 0.8980, 0.6414, 0.0683],\n         [0.4174, 0.6221, 0.5268,  ..., 0.5815, 0.9712, 0.2785],\n         ...,\n         [0.6565, 0.0865, 0.3406,  ..., 0.9100, 0.6470, 0.5546],\n         [0.2220, 0.9970, 0.3127,  ..., 0.4255, 0.4705, 0.4119],\n         [0.6515, 0.0425, 0.8917,  ..., 0.8203, 0.0284, 0.5850]],\n\n        [[0.5615, 0.5653, 0.4826,  ..., 0.6942, 0.1012, 0.1808],\n         [0.1945, 0.2554, 0.5100,  ..., 0.2309, 0.2271, 0.2731],\n         [0.1082, 0.2387, 0.1082,  ..., 0.9346, 0.5294, 0.4237],\n         ...,\n         [0.3635, 0.2222, 0.9522,  ..., 0.5099, 0.7922, 0.9370],\n         [0.5679, 0.6120, 0.9325,  ..., 0.9777, 0.9113, 0.6418],\n         [0.2083, 0.8483, 0.7993,  ..., 0.4914, 0.9311, 0.2787]]])\ntensor([[[1.1430, 0.4750, 0.9643,  ..., 1.0811, 1.2909, 0.6144],\n         [1.2583, 1.1316, 0.9121,  ..., 1.4362, 0.9856, 0.9178],\n         [0.4319, 0.7766, 1.2809,  ..., 0.6341, 1.2334, 1.0051],\n         ...,\n         [0.8455, 0.8257, 1.4296,  ..., 1.1797, 0.8554, 1.0949],\n         [1.0719, 0.8667, 1.2149,  ..., 1.2666, 1.3274, 1.5843],\n         [1.1128, 0.7860, 1.6543,  ..., 1.6417, 1.2668, 1.4073]],\n\n        [[0.7814, 0.8663, 1.0599,  ..., 1.1781, 0.7097, 1.0972],\n         [0.8504, 0.4379, 1.0588,  ..., 0.5925, 0.9901, 1.1410],\n         [1.1053, 0.8782, 0.9444,  ..., 0.8827, 0.6374, 0.8802],\n         ...,\n         [1.0361, 0.6732, 0.9197,  ..., 0.4876, 0.9906, 1.0076],\n         [0.2315, 0.3532, 0.4028,  ..., 0.5888, 0.3409, 0.6880],\n         [1.1396, 1.4986, 1.7859,  ..., 1.3227, 1.7696, 1.1816]],\n\n        [[0.4248, 0.7457, 0.4990,  ..., 0.2755, 0.5868, 1.0838],\n         [1.2562, 1.0903, 1.5220,  ..., 0.9132, 1.4961, 0.9182],\n         [0.9428, 0.5351, 0.2215,  ..., 0.5634, 0.1255, 1.0640],\n         ...,\n         [1.0174, 0.6219, 0.8639,  ..., 0.7137, 0.6563, 0.8314],\n         [1.6138, 1.4640, 1.1973,  ..., 1.4117, 1.0408, 1.6169],\n         [1.5141, 0.8801, 1.2607,  ..., 1.6234, 1.0029, 1.0943]],\n\n        ...,\n\n        [[0.9894, 1.1085, 1.2792,  ..., 0.9390, 1.1479, 0.6472],\n         [0.6125, 1.1787, 1.2787,  ..., 1.1740, 1.2835, 0.9734],\n         [0.4054, 0.2825, 1.2082,  ..., 0.3213, 0.8793, 0.8322],\n         ...,\n         [1.4057, 1.0218, 1.2147,  ..., 1.4795, 1.8468, 1.1954],\n         [1.1741, 1.5653, 1.4421,  ..., 1.2234, 0.6813, 1.4263],\n         [0.3006, 0.7398, 0.3908,  ..., 0.2957, 0.9948, 0.4005]],\n\n        [[0.6115, 0.1190, 0.8208,  ..., 0.6869, 0.3296, 0.3529],\n         [0.9991, 0.1383, 0.6027,  ..., 0.9836, 0.7270, 0.1539],\n         [1.3442, 1.5489, 1.4536,  ..., 1.5083, 1.8980, 1.2053],\n         ...,\n         [1.0234, 0.4534, 0.7075,  ..., 1.2769, 1.0139, 0.9215],\n         [0.4776, 1.2527, 0.5684,  ..., 0.6812, 0.7261, 0.6676],\n         [1.3269, 0.7179, 1.5671,  ..., 1.4957, 0.7039, 1.2604]],\n\n        [[1.1758, 1.1796, 1.0969,  ..., 1.3084, 0.7155, 0.7950],\n         [0.9624, 1.0233, 1.2779,  ..., 0.9987, 0.9949, 1.0409],\n         [0.3341, 0.4645, 0.3341,  ..., 1.1605, 0.7553, 0.6496],\n         ...,\n         [1.2472, 1.1059, 1.8359,  ..., 1.3936, 1.6759, 1.8207],\n         [0.6686, 0.7127, 1.0332,  ..., 1.0784, 1.0120, 0.7425],\n         [0.4986, 1.1385, 1.0895,  ..., 0.7817, 1.2214, 0.5689]]])\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch\n\n# from ptls.constant_repository import TORCH_EMB_DTYPE\nfrom ptls.data_load import PaddedBatch\nfrom ptls.nn.seq_encoder.rnn_encoder import RnnEncoder\nfrom ptls.nn.seq_encoder.transformer_encoder import TransformerEncoder\nfrom ptls.nn.seq_encoder.longformer_encoder import LongformerEncoder\nfrom ptls.nn.seq_encoder.custom_encoder import Encoder\nfrom ptls.nn.trx_encoder import TrxEncoder\nfrom ptls.nn.seq_encoder.containers import SeqEncoderContainer\n\nclass RnnSeqEncoderRegAttn(SeqEncoderContainer):\n    def __init__(self,\n                 trx_encoder=None,\n                 input_size=None,\n                 is_reduce_sequence=True,\n                 **seq_encoder_params,\n                 ):\n        super().__init__(\n            trx_encoder=trx_encoder,\n            seq_encoder_cls=RnnEncoder,\n            input_size=input_size,\n            seq_encoder_params=seq_encoder_params,\n            is_reduce_sequence=is_reduce_sequence,\n        )\n        \n        self.reg_seq_encoder = RnnEncoder(\n            input_size=input_size if input_size is not None else trx_encoder.output_size,\n            is_reduce_sequence=is_reduce_sequence,\n            **seq_encoder_params,\n        )\n\n        self.emb_dim = 192\n        self.regional_attention = nn.MultiheadAttention(\n            embed_dim=self.emb_dim,\n            num_heads=8,\n            dropout=0.3,\n            batch_first=True\n        )\n    \n    \n    def forward(self, x, names=None, seq_len=None, h_0=None):\n        # print(f\"x_in = {x.payload['amount'].size()}\")\n        x = self.trx_encoder(x)\n\n        x_new = x.payload\n        \n        segment_length = 10\n        pad_length = (segment_length - (x_new.size()[1] % segment_length)) % segment_length\n        padded_x_new = F.pad(x_new, ((0, 0, 0, pad_length, 0, 0)), 'constant', 0)\n        segmented_tensors = torch.stack(torch.split(padded_x_new, segment_length, dim=1)).to(x_new.device)\n        \n        regional_embeddings = torch.Tensor().to(x.device)\n        for tensor in segmented_tensors:\n            tensor = PaddedBatch(tensor.permute(1, 0, 2), [tensor.size()[0]] * tensor.size()[1])\n            regional_embed = self.reg_seq_encoder(tensor)\n            regional_embeddings = torch.cat((regional_embeddings, regional_embed), 0)\n        \n        regional_embeddings = regional_embeddings[:len(regional_embeddings) - pad_length, :]\n        layer_norm = nn.LayerNorm([regional_embeddings.size()[0], regional_embeddings.size()[1]])\n        layer_norm.to(x.device)\n        regional_embeddings = layer_norm(regional_embeddings)\n        \n        if regional_embeddings.size()[1] != self.emb_dim:\n            regional_embeddings = F.pad(regional_embeddings, ((0, abs(regional_embeddings.size()[1] - self.emb_dim), 0, 0)), 'constant', 0)\n        x_reg_embed, _ = self.regional_attention(regional_embeddings, regional_embeddings, regional_embeddings)\n        if regional_embeddings.size()[1] != x_new.size()[0]:\n            x_reg_embed = x_reg_embed[:, :-abs(regional_embeddings.size()[1] - x_new.size()[0])]\n        x_reg_embed = x_reg_embed.permute(1, 0)\n        x_reg_embed = x_reg_embed[:, :, None]\n        x_new = x_new + x_reg_embed\n        x_new.to(x.device)\n        x_new = PaddedBatch(x_new, x.seq_lens)\n        # x_new.to(x.device)\n        x = self.seq_encoder(x_new, h_0)\n        # print(f\"rnn_x_size = {x.size()}\")\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:33.544842Z","iopub.execute_input":"2025-03-21T08:23:33.545111Z","iopub.status.idle":"2025-03-21T08:23:33.556366Z","shell.execute_reply.started":"2025-03-21T08:23:33.545088Z","shell.execute_reply":"2025-03-21T08:23:33.555494Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from ptls.frames.abs_module import ABSModule\nfrom ptls.frames.coles.metric import metric_recall_top_K, outer_cosine_similarity, outer_pairwise_distance\nfrom ptls.frames.coles.losses import ContrastiveLoss\nfrom torch import nn\nfrom ptls.nn.seq_encoder.custom_encoder import MLP\nimport math\n\n\n# class CrossAttention(nn.Module):\n#     def __init__(self, d_in, d_out_kq, d_out_v):\n#         super().__init__()\n#         self.d_out_kq=d_out_kq\n#         self.W_query=nn.Parameter(torch.rand(d_in, d_out_kq))\n#         self.W_key  = nn.Parameter(torch.rand(d_in, d_out_kq))\n#         self.W_value=nn.Parameter(torch.rand(d_in, d_out_v))\n    \n#     def forward(self, x_1, x_2):\n#         queries_1=x_1.matmul(self.W_query)\n#         keys_2=x_2.matmul(self.W_key)\n#         values_2=x_2.matmul(self.W_value)\n        \n#         attn_scores=queries_1.matmul(keys_2.T)\n#         attn_weights=torch.softmax(\n#             attn_scores/self.d_out_kq**0.5, dim=-1\n#         )\n        \n#         context_vec=attn_weights.matmul(values_2)\n#         return context_vec\n\ndef first(iterable, default=None):\n    iterator = iter(iterable)\n    return next(iterator, default)\n\n\n# class PositionalEncoding(nn.Module):\n#     def __init__(self,\n#                  d_model,\n#                  use_start_random_shift=True,\n#                  max_len=5000,\n#                  ):\n#         super().__init__()\n#         self.use_start_random_shift = use_start_random_shift\n#         self.max_len = max_len\n\n#         pe = torch.zeros(max_len, d_model)\n#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n#         pe[:, 0::2] = torch.sin(position * div_term)\n#         pe[:, 1::2] = torch.cos(position * div_term)\n#         pe = pe.unsqueeze(0)\n#         self.register_buffer('pe', pe)\n\n#     def forward(self, x):\n#         T = x.size(0)\n#         if self.training and self.use_start_random_shift:\n#             start_pos = random.randint(0, self.max_len - T)\n#         else:\n#             start_pos = 0\n#         # print(f'{x.size()=}')\n#         # print(f'{self.pe.size()}')\n#         # print(f'{self.pe[:, start_pos:start_pos + T].size()=}')\n#         x = x + self.pe[:, start_pos:start_pos + T]\n#         return x\n\n# Проверить, что на каждой эпохе лежит разный юзер\n# Сделать выбор positive + negative случайно\n# Сделать self-attention на унимодальности\n\nclass M3CoLESModule(ABSModule):\n    \"\"\"\n    Multi-Modal Matching\n    Contrastive Learning for Event Sequences ([CoLES](https://arxiv.org/abs/2002.08232))\n\n    Subsequences are sampled from original sequence.\n    Samples from the same sequence are `positive` examples\n    Samples from the different sequences are `negative` examples\n    Embeddings for all samples are calculated.\n    Paired distances between all embeddings are calculated.\n    The loss function tends to make positive distances smaller and negative ones larger.\n\n    Parameters\n        seq_encoder:\n            Model which calculate embeddings for original raw transaction sequences\n            `seq_encoder` is trained by `CoLESModule` to get better representations of input sequences\n        head:\n            Model which helps to train. Not used during inference\n            Can be normalisation layer which make embedding l2 length equals 1\n            Can be MLP as `projection head` like in SymCLR framework.\n        loss:\n            loss object from `ptls.frames.coles.losses`.\n            There are paired and triplet loss. They are required sampling strategy\n            from `ptls.frames.coles.sampling_strategies`. Sampling strategy takes a relevant pairs or triplets from\n            pairwise distance matrix.\n        validation_metric:\n            Keep None. `ptls.frames.coles.metric.BatchRecallTopK` used by default.\n        optimizer_partial:\n            optimizer init partial. Network parameters are missed.\n        lr_scheduler_partial:\n            scheduler init partial. Optimizer are missed.\n\n    \"\"\"\n    def __init__(self,\n                 seq_encoders=None,\n                 mod_names=None,\n                 head=None,\n                 loss=None,\n                 validation_metric=None,\n                 optimizer_partial=None,\n                 lr_scheduler_partial=None):\n        torch.set_float32_matmul_precision('high')\n        if head is None:\n            head = ptls.nn.Head(use_norm_encoder=True)\n\n        if loss is None:\n            loss = ContrastiveLoss(margin=0.5,\n                                   sampling_strategy=HardNegativePairSelector(neg_count=5))\n\n        if validation_metric is None:\n            validation_metric = BatchRecallTopK(K=4, metric='cosine')\n        \n        for k in seq_encoders.keys():\n            if type(seq_encoders[k]) is str:\n                seq_encoders[k] = seq_encoders[seq_encoders[k]]\n\n        \n        super().__init__(validation_metric,\n                         first(seq_encoders.values()),\n                         loss,\n                         optimizer_partial,\n                         lr_scheduler_partial)\n\n        # cross_mha_MLP = ptls.nn.seq_encoder.\n        self.mha_trx_dial = nn.MultiheadAttention(\n            embed_dim=128,\n            num_heads=8,\n            dropout=0.3,\n            batch_first=True\n        )\n        self.mha_dial_trx = nn.MultiheadAttention(\n            embed_dim=128,\n            num_heads=8,\n            dropout=0.1,\n            batch_first=True\n        )\n\n        ## MLP variation\n        # self.head_trx = MLP(\n        #         n_in=128,\n        #         n_hidden=128,\n        #         n_out=128\n        #     )\n        # self.head_dial = MLP(\n        #         n_in=128,\n        #         n_hidden=128,\n        #         n_out=128\n        #     ) \n\n        #FFN variant\n        self.head_trx = MLP(\n                n_in=128,\n                n_hidden=256,\n                n_out=128\n            )\n        self.head_dial = MLP(\n                n_in=128,\n                n_hidden=256,\n                n_out=128\n            ) \n\n        self.seq_encoders = torch.nn.ModuleDict(seq_encoders)\n        self._head = head   \n        self.y_h_cache = {'train':[], 'valid': []}\n        \n    @property\n    def metric_name(self):\n        return 'recall_top_k'\n\n    @property\n    def is_requires_reduced_sequence(self):\n        return True\n    \n    def forward(self, x):\n        res = {}\n        for mod_name in x.keys():\n            res[mod_name] = self.seq_encoders[mod_name](x[mod_name])\n        return res\n\n    def shared_step(self, x, y):\n        y_h = self(x)\n        \n        if self._head is not None:\n            y_h_head = {k: self._head(y_h_k) for k, y_h_k in y_h.items()}\n            y_h = y_h_head\n        return y_h, y\n    \n    def _one_step(self, batch, _, stage):\n        y_h, y = self.shared_step(*batch)\n        y_h_list = list(y_h.values())\n        loss = self._loss(torch.cat(y_h_list), torch.cat([y, y]))\n        self.log(f'loss/{stage}', loss.detach())\n        \n        x, y = batch\n        for mod_name, mod_x in x.items():\n            self.log(f'seq_len/{stage}/{mod_name}', x[mod_name].seq_lens.float().mean().detach(), prog_bar=True)\n        \n        if stage == \"valid\":\n            n, d = y_h_list[0].shape\n            y_h_concat = torch.zeros((2*n, d), device = y_h_list[0].device)\n            \n            for i in range(2):\n                y_h_concat[range(i,2*n,2)] = y_h_list[i] \n            if len(self.y_h_cache[stage]) <= 380:\n                self.y_h_cache[stage].append((y_h_concat.cpu(), {k: y_h_k.cpu() for k, y_h_k in y_h.items()} , \n                                             {k:x_k.seq_lens.cpu() for k, x_k in x.items()})) \n        return loss\n    \n    def training_step(self, batch, _):\n        return self._one_step(batch, _, \"train\")\n    \n    def validation_step(self, batch, _):\n        return self._one_step(batch, _, \"valid\")\n    \n    def on_validation_epoch_end(self):        \n        #len_intervals = [(0, 10), (10, 20), (20, 30), (30, 40), (40, 60), (60, 80), (80, 120), (120, 160), (160, 240)]\n        self.log_recall_top_K(self.y_h_cache['valid'], len_intervals=None, stage=\"valid\", K=30)\n        self.log_recall_top_K(self.y_h_cache['valid'], len_intervals=None, stage=\"valid\", K=20)\n        self.log_recall_top_K(self.y_h_cache['valid'], len_intervals=None, stage=\"valid\", K=1)\n        \n        \n        del self.y_h_cache[\"valid\"]\n        self.y_h_cache[\"valid\"] = []\n        \n    def log_recall_top_K(self, y_h_cache, len_intervals=None, stage=\"valid\", K=15):\n        y_h = torch.cat([item[0] for item in y_h_cache], dim = 0)\n        y_h_mods = defaultdict(list)\n        seq_lens_dict = defaultdict(list)\n        \n        for item in y_h_cache:\n            for k, emb in item[1].items():\n                y_h_mods[k].append(emb)\n                \n            for k, l in item[2].items():\n                seq_lens_dict[k].append(l)\n        \n        y_h_mods = {k: torch.cat(el, dim=0) for k ,el in y_h_mods.items()}\n        seq_lens_dict = {k: torch.cat(el) for k ,el in seq_lens_dict.items()}\n\n        #n, _ = y_h.shape\n        #y = torch.zeros((n,)).cpu().long()\n        #y[range(0,n,2)] = torch.arange(0, n//2)\n        #y[range(1,n,2)] = torch.arange(0, n//2)\n        #computed_metric = metric_real_recall_top_K(y_h, y, K=100)\n        y_h_bank, y_h_rmb = list(y_h_mods.values())\n        computed_metric_b2r = metric_recall_top_K_for_embs(y_h_bank, y_h_rmb, torch.arange(y_h_rmb.shape[0]), K=K)\n        computed_metric_r2b = metric_recall_top_K_for_embs(y_h_rmb, y_h_bank, torch.arange(y_h_rmb.shape[0]), K=K)\n        \n        if len_intervals != None:\n            for mod, seq_lens in seq_lens_dict.items():\n                for start, end in len_intervals:\n                    mask = ((seq_lens > start) & (seq_lens <= end))\n\n                    if torch.any(mask):\n                        #y_h_filtered = y_h[mask.repeat_interleave(2)]\n                        y_h_bank_filtered = y_h_bank[mask]\n                        y_h_rmb_filtered = y_h_rmb[mask]\n\n                        #y = torch.div(torch.arange(len(y_h_filtered)), 2, rounding_mode='floor')\n                        #recall = metric_real_recall_top_K(y_h_filtered, y, K=100)\n                        recall_r2b = metric_recall_top_K_for_embs(y_h_rmb_filtered, y_h_bank_filtered, torch.arange(y_h_rmb_filtered.shape[0]), K=30)\n                        recall_b2r = metric_recall_top_K_for_embs(y_h_bank_filtered, y_h_rmb_filtered, torch.arange(y_h_rmb_filtered.shape[0]), K=30)\n\n                        #self.log(f\"{mode}/R@100_len_from_{start}_to_{end}\", recall, prog_bar=True)\n                        print(f\"{stage}/{mod}/r2b_R@100_len_from_{start}_to_{end}\", recall_r2b, prog_bar=True)\n                        self.log(f\"{stage}/{mod}/b2r_R@100_len_from_{start}_to_{end}\", recall_b2r, prog_bar=True)\n        \n        #self.log(f\"{mode}/R@100\", computed_metric, prog_bar=True)\n        self.log(f\"{stage}/click2trx_R@{K}\", computed_metric_r2b, prog_bar=True)\n        self.log(f\"{stage}/trx2click_R@{K}\", computed_metric_b2r, prog_bar=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:33.557583Z","iopub.execute_input":"2025-03-21T08:23:33.557938Z","iopub.status.idle":"2025-03-21T08:23:33.581701Z","shell.execute_reply.started":"2025-03-21T08:23:33.557905Z","shell.execute_reply":"2025-03-21T08:23:33.580845Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def metric_real_recall_top_K(X, y, K, num_pos=1, metric='cosine'):\n    \"\"\"\n        calculate metric R@K\n        X - tensor with size n x d, where n - number of examples, d - size of embedding vectors\n        y - true labels\n        N - count of closest examples, which we consider for recall calcualtion\n        metric: 'cosine' / 'euclidean'.\n            !!! 'euclidean' - to slow for datasets bigger than 100K rows\n    \"\"\"\n    K_adjusted = min(X.size(0) - 1, K)\n    \n    res = []\n\n    n = X.size(0)\n    d = X.size(1)\n    max_size = 2 ** 32\n    batch_size = max(1, max_size // (n * d))\n\n    with torch.no_grad():\n\n        for i in range(1 + (len(X) - 1) // batch_size):\n\n            id_left = i * batch_size\n            id_right = min((i + 1) * batch_size, len(y))\n            y_batch = y[id_left:id_right]\n            # print(f\"X = {X}\")\n            # print(f\"X[] = {X[id_left:id_right]}\")\n            if metric == 'cosine':\n                pdist = -1 * outer_cosine_similarity(X, X[id_left:id_right])\n            elif metric == 'euclidean':\n                pdist = outer_pairwise_distance(X, X[id_left:id_right])\n            else:\n                raise AttributeError(f'wrong metric \"{metric}\"')\n\n            values, indices = pdist.topk(K_adjusted + 1, 0, largest=False)\n\n            y_rep = y_batch.repeat(K_adjusted, 1)\n            res.append((y[indices[1:]] == y_rep).sum().item())\n\n    return np.sum(res) / len(y) / num_pos\n\ndef cosine_similarity_matrix(x1, x2):\n    x1_norm = x1 / x1.norm(dim=1)[:, None]\n    x2_norm = x2 / x2.norm(dim=1)[:, None]\n    return torch.mm(x1_norm, x2_norm.transpose(0, 1))\n\ndef metric_recall_top_K_for_embs(embs_1, embs_2, true_matches, K=30):\n    similarity_matrix = cosine_similarity_matrix(embs_1, embs_2)\n    K_adjusted = min(len(embs_1), K)\n    top_k = similarity_matrix.topk(k=K_adjusted, dim=1).indices\n    correct_matches = 0\n    for i, indices in enumerate(top_k):\n        if true_matches[i] in indices:\n            correct_matches += 1\n    recall_at_k = correct_matches / len(similarity_matrix)\n    return recall_at_k\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:33.582728Z","iopub.execute_input":"2025-03-21T08:23:33.583026Z","iopub.status.idle":"2025-03-21T08:23:33.601258Z","shell.execute_reply.started":"2025-03-21T08:23:33.582995Z","shell.execute_reply":"2025-03-21T08:23:33.600280Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from ptls.nn import TrxEncoder, RnnSeqEncoder\nfrom ptls.frames.coles import CoLESModule\nfrom functools import partial\nimport torch\nfrom ptls.frames.coles import MultiModalSortTimeSeqEncoderContainer\nfrom ptls.nn.trx_encoder.encoders import IdentityEncoder\nfrom ptls.nn.seq_encoder.rnn_encoder import RnnEncoder\nfrom ptls.nn.seq_encoder.transformer_encoder import TransformerEncoder\nfrom ptls.frames.coles.losses import ContrastiveLoss\nfrom ptls.frames.coles.sampling_strategies import HardNegativePairSelector\n\nhead = ptls.nn.Head(\n    input_size=128,\n    use_norm_encoder=True,\n    hidden_layers_sizes=[128, 128],\n    objective=\"regression\",\n    num_classes=128\n)\n\nloss = ptls.frames.coles.losses.SoftmaxLoss()\n\n# With RNN\nseq_encoders = {\n    'trx': RnnSeqEncoderRegAttn(\n        trx_encoder=TrxEncoder(\n            norm_embeddings=False,\n            embeddings_noise=0.003,\n            linear_projection_size=32,\n            embeddings={\n                'event_type': {\"in\": 58, \"out\": 24},\n                'event_subtype': {\"in\": 59, \"out\": 24},\n                'src_type11': {\"in\": 85, \"out\": 24},\n                'src_type12': {\"in\": 349, \"out\": 24},\n                'dst_type11': {\"in\": 84, \"out\": 24},\n                'dst_type12': {\"in\": 417, \"out\": 24},\n                'src_type22': {\"in\": 90, \"out\": 24},\n                'src_type32': {\"in\": 91, \"out\": 24}\n            },\n            numeric_values={\n                'amount': 'log'\n            }\n        ),\n        type='gru',\n        hidden_size=128\n    ),\n    'dial': RnnSeqEncoder(\n        trx_encoder=TrxEncoder(\n            embeddings_noise=0.003,\n            linear_projection_size=32,\n            custom_embeddings={\n                'embedding': IdentityEncoder(768)\n            }\n        ),\n        type='gru',\n        hidden_size=128\n    )\n}\n\noptimizer_partial = partial(\n    torch.optim.AdamW,\n    lr=0.001,\n    weight_decay=1e-4\n)\n\nlr_scheduler_partial = partial(\n    torch.optim.lr_scheduler.StepLR,\n    step_size=1,\n    gamma=0.9\n)\n\n# pl_module = M3CoLESModule(\n#     validation_metric=ptls.frames.coles.metric.BatchRecallTopK(\n#         K=1,\n#         metric='cosine',\n#     ),\n#     head=head,\n#     seq_encoders=seq_encoders,\n#     loss=loss,\n#     optimizer_partial=optimizer_partial,\n#     lr_scheduler_partial=lr_scheduler_partial\n# )\n\npl_module = M3CoLESModule(\n    validation_metric=ptls.frames.coles.metric.BatchRecallTopK(\n        K=10,\n        metric='cosine',\n    ),\n    head=head,\n    seq_encoders=seq_encoders,\n    loss=loss,\n    optimizer_partial=optimizer_partial,\n    lr_scheduler_partial=lr_scheduler_partial\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:33.602066Z","iopub.execute_input":"2025-03-21T08:23:33.602341Z","iopub.status.idle":"2025-03-21T08:23:33.653817Z","shell.execute_reply.started":"2025-03-21T08:23:33.602321Z","shell.execute_reply":"2025-03-21T08:23:33.653169Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"wandb_logger = WandbLogger(project=\"MBD_My_Code\", log_model=\"all\")\n\ntrainer = pl.Trainer(\n    logger=wandb_logger,\n    max_epochs=25,\n    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    enable_progress_bar=True,\n    gradient_clip_val=0.5,\n    log_every_n_steps=50,\n    limit_val_batches=32\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:33.656848Z","iopub.execute_input":"2025-03-21T08:23:33.657052Z","iopub.status.idle":"2025-03-21T08:23:33.741295Z","shell.execute_reply.started":"2025-03-21T08:23:33.657034Z","shell.execute_reply":"2025-03-21T08:23:33.740477Z"}},"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"trainer.fit(pl_module, data_module)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:23:33.742684Z","iopub.execute_input":"2025-03-21T08:23:33.742902Z","iopub.status.idle":"2025-03-21T08:51:56.922451Z","shell.execute_reply.started":"2025-03-21T08:23:33.742881Z","shell.execute_reply":"2025-03-21T08:51:56.921780Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20250321_082333-p3w1nbdr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bstu/MBD_My_Code/runs/p3w1nbdr' target=\"_blank\">cosmic-snow-41</a></strong> to <a href='https://wandb.ai/bstu/MBD_My_Code' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bstu/MBD_My_Code' target=\"_blank\">https://wandb.ai/bstu/MBD_My_Code</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bstu/MBD_My_Code/runs/p3w1nbdr' target=\"_blank\">https://wandb.ai/bstu/MBD_My_Code/runs/p3w1nbdr</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n<ipython-input-13-35f55d31d8f5>:58: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  features[key] = torch.tensor(tens.tolist())\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"496c171622134c5ca78d6b23989aa884"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=25` reached.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"TARGETS_DATA_PATH = '/kaggle/working/targets/'\n\npreprocessor_target = PysparkDataPreprocessor(\n    col_id=\"client_id\",\n    col_event_time=\"mon\",\n    event_time_transformation=\"dt_to_timestamp\",\n    cols_identity=[\"target_1\", \"target_2\", \"target_3\", \"target_4\"],\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:51:56.923426Z","iopub.execute_input":"2025-03-21T08:51:56.923719Z","iopub.status.idle":"2025-03-21T08:51:56.928543Z","shell.execute_reply.started":"2025-03-21T08:51:56.923690Z","shell.execute_reply":"2025-03-21T08:51:56.927808Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"targets = spark.read.parquet(os.path.join(TARGETS_DATA_PATH , f'fold={fold}'))\nmmt_dataset = spark.read.parquet(os.path.join(MM_DATA_PATH , f'fold={fold}'))\n\ntargets = preprocessor_target.fit_transform(targets).drop(*['event_time' ,'trans_count', 'diff_trans_date'])\nmmt_dataset = mmt_dataset.join(targets, on='client_id', how='left')\nmmt_dataset.write.mode('overwrite').parquet(os.path.join(MMT_DATA_PATH, f'fold={fold}'))\n\ndel mmt_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:52:54.230106Z","iopub.execute_input":"2025-03-21T08:52:54.230448Z","iopub.status.idle":"2025-03-21T08:53:08.515852Z","shell.execute_reply.started":"2025-03-21T08:52:54.230414Z","shell.execute_reply":"2025-03-21T08:53:08.514853Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class MatchingModalities(IterableProcessingDataset):\n    def __init__(\n        self,\n        col_id='client_id',\n        mod1_col_time='trx_event_time',\n        mod2_col_time='dial_event_time',\n        mod1_name='trx',\n        mod2_name='dial'\n    ):\n        super().__init__()\n        self.col_id = col_id\n        self.mod1_col_time = mod1_col_time\n        self.mod2_col_time = mod2_col_time\n        self.mod1_name=mod1_name\n        self.mod2_name=mod2_name\n\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features = features.copy()\n            all_event_time, _ = torch.sort(torch.cat((features[self.mod1_col_time], features[self.mod2_col_time])))\n            mod1_mask = torch.isin(all_event_time, features[self.mod1_col_time])\n            mod2_mask = torch.isin(all_event_time, features[self.mod2_col_time])\n            for key, tens in features.items():\n                if key.startswith(self.mod1_name) and key != self.mod1_col_time:\n                    indices = torch.where(~mod1_mask)[0].tolist()\n                    result = []\n                    for i in range(len(mod1_mask)):\n                        if i in indices:\n                            result.append(0)\n                        if i < len(tens):\n                            result.append(tens[i])\n                    features[key] = torch.tensor(result, dtype=features[key].dtype)\n                    # print(f'mod1_size = {features[key].size()}')\n                elif key.startswith(self.mod2_name) and key != self.mod2_col_time:\n                    indices = torch.where(~mod2_mask)[0].tolist()\n                    # print(indices)\n                    result = []\n                    for i in range(len(mod2_mask)):\n                        if i in indices:\n                            result.append(torch.Tensor([0 for i in range(768)]))\n                        if i < len(tens):\n                            result.append(tens[i])\n                    # print(len(result))\n                    # print(result)\n                    features[key] = torch.stack(result, dim=0)\n                    # print(f'mod2_size = {features[key].size()}')\n            features[self.mod1_col_time] = all_event_time\n            features[self.mod2_col_time] = all_event_time\n            \n            yield features   \n\nclass MMToTorch(IterableProcessingDataset):\n    def __init__(\n        self,\n        col_id='client_id'\n    ):\n        super().__init__()\n        self.col_id='client_id'\n\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features = features.copy()\n            for key, value in features.items():\n                if key != 'client_id':\n                    features[key] = torch.Tensor(value)\n            yield features\n\nclass DialToTorch(IterableProcessingDataset):\n    def __init__(\n        self,\n        embedding_col='dial_embedding'\n    ):\n        super().__init__()\n        self.embedding_col=embedding_col\n\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features = features.copy()\n            features[self.embedding_col] = np.stack(features[self.embedding_col], axis=0).astype(np.int32)\n            features[self.embedding_col] = torch.FloatTensor(features[self.embedding_col])\n            yield features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:53:08.517091Z","iopub.execute_input":"2025-03-21T08:53:08.517419Z","iopub.status.idle":"2025-03-21T08:53:08.530765Z","shell.execute_reply.started":"2025-03-21T08:53:08.517367Z","shell.execute_reply":"2025-03-21T08:53:08.530060Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import copy\n\nclass GetSplit(IterableProcessingDataset):\n    def __init__(\n        self,\n        start_month,\n        end_month,\n        year=2022,\n        col_id='client_id',\n        trx_col_time='trx_event_time',\n        dial_col_time='dial_event_time'\n    ):\n        super().__init__()\n        self.start_month = start_month\n        self.end_month = end_month\n        self._year = year\n        self._col_id = col_id\n        self._trx_col_time = trx_col_time\n        self._dial_col_time = dial_col_time\n        \n    def __iter__(self):\n        for rec in self._src:\n            for month in range(self.start_month, self.end_month+1):\n                features = rec[0] if type(rec) is tuple else rec\n                features = features.copy()\n                # print(f'features event time size = {features[\"trx_event_time\"].size()}')\n                if month == 12:\n                    month_event_time = datetime(self._year + 1, 1, 1).timestamp()\n                else:\n                    month_event_time = datetime(self._year, month + 1, 1).timestamp()\n                    \n                year_event_time = datetime(self._year, 1, 1).timestamp()\n\n                # print(f\"{rec}\")\n                # print(f\"{month_event_time=}\")\n                trx_mask = features[self._trx_col_time] < month_event_time\n                dial_mask = features[self._dial_col_time] < month_event_time\n                \n                for key, tensor in features.items():\n                    if key.startswith('target'):\n                        features[key] = tensor[month - 1].tolist()\n                    if key.startswith('trx'):\n                        features[key] = tensor[trx_mask]\n                        if len(features[key]) == 0:\n                            if key == 'trx_event_time':\n                                features[key] = torch.Tensor([month_event_time]).to(torch.int32)\n                            else:\n                                features[key] = torch.Tensor([0])\n\n                    if key.startswith('dial'):\n                        features[key] = tensor[dial_mask]\n                        if len(features[key]) == 0:\n                            if key == 'dial_event_time':\n                                features[key] = torch.Tensor([month_event_time]).to(torch.int32)\n\n                            else:\n                                features[key] = torch.zeros([1, 768])\n\n                    # elif key != self._col_id:\n                        # print(f'mask_size = {mask.size()}')\n                        # print(f'tensor size = {tensor.size()}')\n                        # print(f'key = {key}')\n                        # if mask.size()[0] > tensor.size()[0]:\n                        #     mask = mask[:tensor.size()[0]]\n                        # if mask.size()[0] < tensor.size()[0]:\n                        #     tensor = tensor[:mask.size()[0]]\n                        # features[key] = tensor[mask] \n                        # print(f'features[key] size = {features[key].size()}')\n                        # print('=====================')\n                    \n                            \n                features[self._col_id] += '_month=' + str(month)\n\n                yield features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:53:08.532195Z","iopub.execute_input":"2025-03-21T08:53:08.532438Z","iopub.status.idle":"2025-03-21T08:53:08.550066Z","shell.execute_reply.started":"2025-03-21T08:53:08.532416Z","shell.execute_reply":"2025-03-21T08:53:08.549448Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"from ptls.frames.coles import MultiModalInferenceIterableDataset\n\ndataset_inf = ParquetDataset(\n    data_files=[\n        os.path.join(MMT_DATA_PATH, f'fold={4}')\n    ],\n    i_filters=[\n        DeleteNan('trx_event_time'),\n        DeleteNan('dial_event_time'),\n        # SeqLenFilter(min_seq_len=1),\n        # ISeqLenLimit(max_seq_len=128),\n        ptls.data_load.iterable_processing.to_torch_tensor.ToTorch(),\n        # MMToTorch('client_id'),\n        DialToTorch(),\n        # MatchingModalities(\n        #     col_id='client_id',\n        #     mod1_col_time='trx_event_time',\n        #     mod2_col_time='dial_event_time'\n        # ),\n        GetSplit(\n            start_month=1,\n            end_month=12,\n            col_id='client_id',\n            trx_col_time='trx_event_time',\n            dial_col_time='dial_event_time'\n        )\n    ],\n    shuffle_files=False\n)\n\n# print(next(iter(dataset_inf)))\n\n\ndataset_inf = MultiModalInferenceIterableDataset(\n        data=dataset_inf,\n        source_features={\n            \"trx\": [\n                \"event_type\",\n                \"event_subtype\",\n                \"src_type11\",\n                \"src_type12\",\n                \"dst_type11\",\n                \"dst_type12\",\n                \"src_type22\",\n                \"src_type32\"\n            ],\n            \"dial\": [\n                \"embedding\"\n            ]\n        },\n        col_id='client_id',\n        col_time='event_time',\n        source_names=['trx', 'dial'],\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:53:08.551128Z","iopub.execute_input":"2025-03-21T08:53:08.551434Z","iopub.status.idle":"2025-03-21T08:53:08.568955Z","shell.execute_reply.started":"2025-03-21T08:53:08.551404Z","shell.execute_reply":"2025-03-21T08:53:08.568192Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# from ptls.custom_layers import StatPooling\n# from ptls.nn.seq_step import LastStepEncoder\nfrom itertools import chain\n\nclass InferenceModuleMultimodal(pl.LightningModule):\n    def __init__(self, model, pandas_output=True, drop_seq_features=True, model_out_name='out', col_id = 'epk_id'):\n        super().__init__()\n\n        self.model = model\n        self.pandas_output = pandas_output\n        self.drop_seq_features = drop_seq_features\n        self.model_out_name = model_out_name\n        self.col_id = col_id\n\n    def forward(self, x: PaddedBatch):\n        x, batch_ids, targets = x\n\n        #TODO: Проверить batch_ids == col_id. Проверил, совпадают\n        out = self.model(x)\n        # print(f\"{_=}\")\n        x_out = {self.col_id : batch_ids, self.model_out_name: out}\n        if self.pandas_output:\n            return self.to_pandas(x_out, targets)\n        return x_out\n\n    def to_pandas(self, x, targets):\n        expand_cols = []\n        scalar_features = {}\n        if self.model_out_name in x:\n            for k, v in x[self.model_out_name].items():\n                x[k] = v\n        del x[self.model_out_name]\n        for k, v in x.items():\n            if type(v) is torch.Tensor:\n                v = v.cpu().numpy()\n            if type(v) is list or len(v.shape) == 1:\n                scalar_features[k] = v\n            elif len(v.shape) == 2:\n                expand_cols.append(k)\n            else:\n                scalar_features[k] = None\n\n        dataframes = [pd.DataFrame(scalar_features)]\n        targets_dataframe = pd.DataFrame([item[0] for item in targets])\n        for col in expand_cols:\n            v = x[col].cpu().numpy()\n            dataframes.append(pd.DataFrame(v, columns=[f'{col}_{i:04d}' for i in range(v.shape[1])]))\n        return pd.concat(dataframes, axis=1).join(targets_dataframe)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:53:08.569809Z","iopub.execute_input":"2025-03-21T08:53:08.570087Z","iopub.status.idle":"2025-03-21T08:53:08.587087Z","shell.execute_reply.started":"2025-03-21T08:53:08.570066Z","shell.execute_reply":"2025-03-21T08:53:08.586464Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def collate_feature_dict(batch):\n    new_x_ = defaultdict(list)\n    for i, x in enumerate(batch):\n        for k, v in x.items():\n            new_x_[k].append(v)\n    \n    seq_col = next(k for k, v in batch[0].items() if FeatureDict.is_seq_feature(k, v))\n    lengths = torch.LongTensor([len(rec[seq_col]) for rec in batch])\n    new_x = {}\n    for k, v in new_x_.items():\n        # print(new_x)\n        if type(v[0]) is torch.Tensor:\n            if k.startswith('target'):\n                new_x[k] = torch.stack(v, dim=0)\n            else:\n                new_x[k] = torch.nn.utils.rnn.pad_sequence(v, batch_first=True)\n        elif type(v[0]) is np.ndarray:\n            new_x[k] = v  # list of arrays[object]\n        else:\n            v = np.array(v)\n            if v.dtype.kind == 'i':\n                new_x[k] = torch.from_numpy(v).long()\n            elif v.dtype.kind == 'f':\n                new_x[k] = torch.from_numpy(v).float()\n            elif v.dtype.kind == 'b':\n                new_x[k] = torch.from_numpy(v).bool()\n            else:\n                new_x[k] = v\n    return PaddedBatch(new_x, lengths)\n\ndef collate_multimodal_feature_dict(batch):\n    res = {}\n    for source, source_batch in batch.items():\n        res[source] = collate_feature_dict(source_batch)\n    return res\n\ndef collate_feature_dict_with_target(batch, col_id='client_id', target_col_names=None):\n    batch_ids = []\n    target_cols = []\n    # print('batch_in')\n    for sample in batch:\n        batch_ids.append(sample[col_id])\n        del sample[col_id]\n        if target_col_names is not None:\n            for target_col in target_col_names:\n                target_cols.append(sample[target_col])\n                del sample[target_col]\n    # print(batch)\n    batch = reduce(lambda x, y: {k: x[k] + y[k] for k in x if k in y}, batch)\n    # print(batch)\n    padded_batch = collate_multimodal_feature_dict(batch)\n    # print(padded_batch['trx'].payload)\n    # print(batch_ids)\n    # print(padded_batch['trx'].payload['event_time'].size())\n    # print(padded_batch['trx'].payload)\n    if target_col_names is not None:\n        return padded_batch, batch_ids, target_cols\n    return padded_batch, batch_ids[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:53:08.587776Z","iopub.execute_input":"2025-03-21T08:53:08.587986Z","iopub.status.idle":"2025-03-21T08:53:08.600538Z","shell.execute_reply.started":"2025-03-21T08:53:08.587966Z","shell.execute_reply":"2025-03-21T08:53:08.599761Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ncollate_fn = partial(\n    collate_feature_dict_with_target,\n    target_col_names=['target']\n)\n\ninference_dl = DataLoader(\n    dataset=dataset_inf,\n    collate_fn=collate_fn,\n    shuffle=False,\n    num_workers=0,\n    batch_size=32\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:53:08.601228Z","iopub.execute_input":"2025-03-21T08:53:08.601477Z","iopub.status.idle":"2025-03-21T08:53:08.618025Z","shell.execute_reply.started":"2025-03-21T08:53:08.601456Z","shell.execute_reply":"2025-03-21T08:53:08.617256Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"inf_module = InferenceModuleMultimodal(\n    model=pl_module,\n    pandas_output=True,\n    col_id='client_id',\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:53:08.619994Z","iopub.execute_input":"2025-03-21T08:53:08.620220Z","iopub.status.idle":"2025-03-21T08:53:08.634148Z","shell.execute_reply.started":"2025-03-21T08:53:08.620199Z","shell.execute_reply":"2025-03-21T08:53:08.633455Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"inf_embeddings = pd.concat(trainer.predict(inf_module, inference_dl))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:53:08.635311Z","iopub.execute_input":"2025-03-21T08:53:08.635622Z","iopub.status.idle":"2025-03-21T08:56:59.031362Z","shell.execute_reply.started":"2025-03-21T08:53:08.635592Z","shell.execute_reply":"2025-03-21T08:56:59.030691Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df8fe218a7fb4602b78814df43bbd4a4"}},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"from catboost import Pool, CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\n\ninf_train, inf_test = train_test_split(inf_embeddings, test_size=0.2)\ninf_train, inf_val = train_test_split(inf_train, test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:56:59.032415Z","iopub.execute_input":"2025-03-21T08:56:59.032711Z","iopub.status.idle":"2025-03-21T08:56:59.964221Z","shell.execute_reply.started":"2025-03-21T08:56:59.032675Z","shell.execute_reply":"2025-03-21T08:56:59.963562Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"y_1_inf_train, y_1_inf_test = inf_train['1'].to_numpy(), inf_test['1'].to_numpy()\ny_2_inf_train, y_2_inf_test = inf_train['2'].to_numpy(), inf_test['2'].to_numpy()\ny_3_inf_train, y_3_inf_test = inf_train['3'].to_numpy(), inf_test['3'].to_numpy()\ny_4_inf_train, y_4_inf_test = inf_train['4'].to_numpy(), inf_test['4'].to_numpy()\n\ny_inf_tests = [\n    y_1_inf_test,\n    y_2_inf_test,\n    y_3_inf_test,\n    y_4_inf_test\n]\n\nX_inf_train, X_inf_test = inf_train.drop(columns=['client_id', '1', '2', '3', '4']).to_numpy(), inf_test.drop(columns=['client_id', '1', '2', '3', '4']).to_numpy()\ninf_val_pairs = {\n    'X_inf': [\n        inf_val.drop(columns=['client_id', '1', '2', '3', '4']).to_numpy(),\n        inf_val.drop(columns=['client_id', '1', '2', '3', '4']).to_numpy(),\n        inf_val.drop(columns=['client_id', '1', '2', '3', '4']).to_numpy(),\n        inf_val.drop(columns=['client_id', '1', '2', '3', '4']).to_numpy()\n    ],\n    'y_inf': [\n        inf_val['1'].to_numpy(),\n        inf_val['2'].to_numpy(),\n        inf_val['3'].to_numpy(),\n        inf_val['4'].to_numpy()\n    ]\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:56:59.965063Z","iopub.execute_input":"2025-03-21T08:56:59.965269Z","iopub.status.idle":"2025-03-21T08:57:00.053145Z","shell.execute_reply.started":"2025-03-21T08:56:59.965251Z","shell.execute_reply":"2025-03-21T08:57:00.052262Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nmodels = [LGBMClassifier(\n    n_estimators=500,\n    boosting_type='gbdt',\n    subsample=0.5,\n    subsample_freq=1,\n    learning_rate=0.02,\n    feature_fraction=0.75,\n    max_depth=6,\n    lambda_l1=1,\n    lambda_l2=1,\n    min_data_in_leaf=50,\n    random_state=42,\n    n_jobs=8,\n    verbose=-1\n) for _ in range(4)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:57:00.053929Z","iopub.execute_input":"2025-03-21T08:57:00.054206Z","iopub.status.idle":"2025-03-21T08:57:02.947619Z","shell.execute_reply.started":"2025-03-21T08:57:00.054173Z","shell.execute_reply":"2025-03-21T08:57:02.946926Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"train_datasets = [\n    (X_inf_train, y_1_inf_train),\n    (X_inf_train, y_2_inf_train),\n    (X_inf_train, y_3_inf_train),\n    (X_inf_train, y_4_inf_train)\n]\nval_datasets = [\n    (inf_val_pairs['X_inf'][0], inf_val_pairs['y_inf'][0]),\n    (inf_val_pairs['X_inf'][1], inf_val_pairs['y_inf'][1]),\n    (inf_val_pairs['X_inf'][2], inf_val_pairs['y_inf'][2]),\n    (inf_val_pairs['X_inf'][3], inf_val_pairs['y_inf'][3])\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:57:02.948454Z","iopub.execute_input":"2025-03-21T08:57:02.949419Z","iopub.status.idle":"2025-03-21T08:57:02.954916Z","shell.execute_reply.started":"2025-03-21T08:57:02.949357Z","shell.execute_reply":"2025-03-21T08:57:02.954136Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"for i in range(len(models)):\n    models[i].fit(train_datasets[i][0], train_datasets[i][1], eval_set=val_datasets[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:57:02.955688Z","iopub.execute_input":"2025-03-21T08:57:02.955872Z","iopub.status.idle":"2025-03-21T08:58:15.403852Z","shell.execute_reply.started":"2025-03-21T08:57:02.955855Z","shell.execute_reply":"2025-03-21T08:58:15.402938Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\npreds = []\nfor i in range(len(models)):\n    preds.append(models[i].predict_proba(X_inf_test))\n    # print(classification_report(y_inf_tests[i], preds[i]))\n    print(f\"ROC-AUC target_{i} = {roc_auc_score(y_inf_tests[i], preds[i][:, 1])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:58:15.404802Z","iopub.execute_input":"2025-03-21T08:58:15.405040Z","iopub.status.idle":"2025-03-21T08:58:16.653980Z","shell.execute_reply.started":"2025-03-21T08:58:15.405018Z","shell.execute_reply":"2025-03-21T08:58:16.653293Z"}},"outputs":[{"name":"stdout","text":"ROC-AUC target_0 = 0.6835130424746901\nROC-AUC target_1 = 0.7069505322479649\nROC-AUC target_2 = 0.5838252484700381\nROC-AUC target_3 = 0.6956062235064757\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"**CoLES + regional attention in trx + LightGBM (Without LN):**\n\nROC-AUC target_0 = 0.6674023787783245\n\nROC-AUC target_1 = 0.8062314626627639\n\nROC-AUC target_2 = 0.5838852723065378\n\nROC-AUC target_3 = 0.6902740886029226\n\n**CoLES + regional attention in trx + LightGBM (With LN):**\n\nROC-AUC target_0 = 0.6835130424746901\n\nROC-AUC target_1 = 0.7069505322479649\n\nROC-AUC target_2 = 0.5838252484700381\n\nROC-AUC target_3 = 0.6956062235064757\n\n\n**CoLES + regional attention in trx + LightGBM (Witр BN):**\n\nROC-AUC target_0 = 0.5941521400656775\n\nROC-AUC target_1 = 0.72975601217508\n\nROC-AUC target_2 = 0.5737223564128565\n\nROC-AUC target_3 = 0.618690245938203","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}