{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import hf_hub_download \n\nhf_hub_download(repo_id=\"ai-lab/MBD-mini\", filename=\"ptls.tar.gz\", repo_type=\"dataset\", local_dir=\"/kaggle/working/\")\nhf_hub_download(repo_id=\"ai-lab/MBD-mini\", filename=\"targets.tar.gz\", repo_type=\"dataset\", local_dir=\"/kaggle/working/\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install lightning","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=\"79f2120f8d4212aceb2c60b3c89a1b6727c19cff\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyspark\n!pip install pytorch-lifestream","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!tar -xf ptls.tar.gz\n!tar -xf targets.tar.gz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom pyspark.sql import types as T\nimport time\nimport datetime\nfrom ptls.data_load.datasets import ParquetDataset, ParquetFiles\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType\nfrom tqdm.notebook import tqdm\nfrom ptls.preprocessing import PysparkDataPreprocessor\nimport pytorch_lightning as pl\nfrom ptls.data_load.datasets import MemoryMapDataset\nfrom ptls.data_load.iterable_processing import SeqLenFilter, FeatureFilter\nfrom ptls.data_load.iterable_processing.iterable_seq_len_limit import ISeqLenLimit\nfrom ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\nfrom ptls.frames.coles import CoLESModule\nfrom ptls.frames import PtlsDataModule\nfrom ptls.frames.coles import ColesDataset\nfrom ptls.frames.coles.split_strategy import SampleSlices\nimport torch\nimport numpy as np\nimport pandas as pd\nimport calendar\nfrom glob import glob\nfrom ptls.data_load.utils import collate_feature_dict\n\nfrom ptls.data_load.iterable_processing_dataset import IterableProcessingDataset\nfrom datetime import datetime\nfrom ptls.data_load.padded_batch import PaddedBatch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"spark_conf = pyspark.SparkConf()\nspark_conf.setMaster(\"local[*]\").setAppName(\"JoinModality\")\nspark_conf.set(\"spark.driver.maxResultSize\", \"16g\")\nspark_conf.set(\"spark.executor.memory\", \"32g\")\nspark_conf.set(\"spark.executor.memoryOverhead\", \"16g\")\nspark_conf.set(\"spark.driver.memory\", \"32g\")\nspark_conf.set(\"spark.driver.memoryOverhead\", \"16g\")\nspark_conf.set(\"spark.cores.max\", \"8\")\nspark_conf.set(\"spark.sql.shuffle.partitions\", \"200\")\nspark_conf.set(\"spark.local.dir\", \"../../spark_local_dir\")\n\n\nspark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sparkContext.getConf().getAll()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/mm_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRX_DATA_PATH = '/kaggle/working/ptls/trx/'\nGEO_DATA_PATH = '/kaggle/working/ptls/geo/'\nDIAL_DATA_PATH = '/kaggle/working/ptls/dialog/'\n\nMM_DATA_PATH = '/kaggle/working/mm_dataset'\nMMT_DATA_PATH = '/kaggle/working/mm_dataset_supervised'\n\nTARGETS_DATA_PATH = '/kaggle/working/targets/'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rename_col(df, prefix, col_id='client_id'):\n    new_column_names = [f\"{prefix}_{col}\" for col in df.columns if col != col_id]\n    old_column_names = [col for col in df.columns if col != col_id]\n    for old_col, new_col in zip(old_column_names, new_column_names):\n        df = df.withColumnRenamed(old_col, new_col)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ptls.preprocessing import PysparkDataPreprocessor\nfrom pyspark.sql.functions import explode, col\n\n\nfor fold in tqdm(range(0, 5)):\n    trx = spark.read.parquet(os.path.join(TRX_DATA_PATH, f'fold={fold}'))\n    dial = spark.read.parquet(os.path.join(DIAL_DATA_PATH, f'fold={fold}'))\n    \n    trx = rename_col(trx, 'trx')\n    dial = rename_col(dial, 'dial')\n    \n    mm_dataset = trx.join(dial, on='client_id', how='outer').drop(*['trx_src_type21', 'trx_src_type31'])\n\n    mm_dataset.write.mode('overwrite').parquet(os.path.join(MM_DATA_PATH, f'fold={fold}'))\n    \n    del trx\n    del dial\n    del mm_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# spark.stop()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom ptls.data_load.iterable_processing_dataset import IterableProcessingDataset\nfrom ptls.data_load import IterableChain\nfrom datetime import datetime\nfrom ptls.data_load.datasets.parquet_dataset import ParquetDataset, ParquetFiles\nfrom ptls.data_load.iterable_processing.feature_filter import FeatureFilter\nfrom ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\nimport torch\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom ptls.data_load.utils import collate_feature_dict\nfrom tqdm import tqdm\n\n\nclass TargetToTorch(IterableProcessingDataset):\n    def __init__(self, col_target):\n        super().__init__()\n        self.col_target = col_target\n\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features[self.col_target] = np.stack(np.array(features[self.col_target]))\n            features[self.col_target] = torch.tensor(features[self.col_target])\n            yield features\n\nclass DeleteNan(IterableProcessingDataset):\n    def __init__(self, col_name):\n        super().__init__()\n        self.col_name = col_name\n    \n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            if features[self.col_name] is not None:\n                yield features\n\n\nclass DialToTorch(IterableProcessingDataset):\n    def __init__(self, col_time, col_embeds):\n        super().__init__()\n        self._year=2022\n        self.col_embeds = col_embeds\n        self.col_time = col_time\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features = features.copy()\n            if features[self.col_time] is None:\n                features[self.col_time] = torch.tensor([0])\n            if features[self.col_embeds] is None:\n                features[self.col_embeds] = torch.zeros(768)\n            \n            for key, tens in features.items():\n                if key == self.col_embeds:\n                    features[key] = torch.tensor(tens.tolist())\n\n            yield features\n\nclass GetSplit(IterableProcessingDataset):\n    def __init__(\n        self,\n        start_month,\n        end_month,\n        year=2022,\n        col_id='client_id',\n        col_time='event_time'\n    ):\n        super().__init__()\n        self.start_month = start_month\n        self.end_month = end_month\n        self._year = year\n        self._col_id = col_id\n        self._col_time = col_time\n        \n    def __iter__(self):\n        for rec in self._src:\n            for month in range(self.start_month, self.end_month+1):\n                features = rec[0] if type(rec) is tuple else rec\n                features = features.copy()\n                \n                if month == 12:\n                    month_event_time = datetime(self._year + 1, 1, 1).timestamp()\n                else:\n                    month_event_time = datetime(self._year, month + 1, 1).timestamp()\n                    \n                year_event_time = datetime(self._year, 1, 1).timestamp()\n                \n                mask = features[self._col_time] < month_event_time\n                \n                for key, tensor in features.items():\n                    if key.startswith('target'):\n                        features[key] = tensor[month - 1].tolist()    \n                    elif key != self._col_id:\n                        features[key] = tensor[mask] \n                            \n                features[self._col_id] += '_month=' + str(month)\n\n                yield features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ptls.data_load.datasets import ParquetDataset\nfrom ptls.data_load.iterable_processing import SeqLenFilter\nfrom ptls.data_load.iterable_processing.iterable_seq_len_limit import ISeqLenLimit\nfrom ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\ntrain = ParquetDataset(\n    data_files=[\n        os.path.join(MM_DATA_PATH, f'fold={0}'),\n        os.path.join(MM_DATA_PATH, f'fold={1}'),\n        os.path.join(MM_DATA_PATH, f'fold={2}')\n    ],\n    i_filters=[\n        DeleteNan('trx_event_time'),\n        DeleteNan('dial_event_time'),\n        SeqLenFilter(min_seq_len=8),\n        ISeqLenLimit(max_seq_len=128),\n        ToTorch(),\n        DialToTorch(col_time='dial_event_time', col_embeds='dial_embedding'),\n        # GetSplit(\n        #     start_month=1,\n        #     end_month=12,\n        #     col_id='client_id'\n        # )\n    ],\n    shuffle_files=True\n)\nvalid = ParquetDataset(\n    data_files=[\n        os.path.join(MM_DATA_PATH, f'fold={3}')\n    ],\n    i_filters=[\n        DeleteNan('trx_event_time'),\n        DeleteNan('dial_event_time'),\n        SeqLenFilter(min_seq_len=8),\n        ISeqLenLimit(max_seq_len=128),\n        ToTorch(),\n        DialToTorch(col_time='dial_event_time', col_embeds='dial_embedding'),\n        # GetSplit(\n        #     start_month=1,\n        #     end_month=12,\n        #     col_id='client_id'\n        # )\n    ],\n    shuffle_files=False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ptls.data_load.feature_dict import FeatureDict\nfrom collections import defaultdict\nfrom functools import reduce\nimport torch.nn.functional as F\n\n\n# def split_and_pad(tensor: torch.Tensor, segment_length):\n#     segments = [tensor[i:i + segment_length] for i in range(0, len(tensor), segment_length)]\n#     padded_segments = [F.pad(segment, (0, segment_length - len(segment)), mode='constant') for segment in segments]\n#     return torch.vstack(padded_segments)\n\n# def get_regional_splits(batch: list[dict], segment_length):\n#     regional_tokens = []\n#     for item in batch:\n#         segmented_data = {key: split_and_pad(tensor, segment_length) for key, tensor in item.items()}\n#         regional_tokens.append(segmented_data)\n#     return regional_tokens\n\nclass MultiModalDiffSplitDataset(FeatureDict, torch.utils.data.Dataset):\n    def __init__(\n        self,\n        data,\n        splitters,\n        source_features,\n        col_id,\n        source_names,\n        col_time='event_time',\n        *args, **kwargs\n    ):\n        \"\"\"\n        Dataset for multimodal learning.\n        Parameters:\n        -----------\n        data:\n            concatinated data with feature dicts.\n        splitter:\n            object from from `ptls.frames.coles.split_strategy`.\n            Used to split original sequence into subsequences which are samples from one client.\n        source_features:\n            list of column names \n        col_id:\n            column name with user_id\n        source_names:\n            column name with name sources\n        col_time:\n            column name with event_time\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        \n        self.data = data\n        self.splitters = splitters\n        self.col_time = col_time\n        self.col_id = col_id\n        self.source_names = source_names\n        self.source_features = source_features\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        feature_arrays = self.data[idx]\n        split_data = self.split_source(feature_arrays)\n        # print(self.get_split(split_data))\n        return self.get_splits(split_data)\n    \n    def __iter__(self):\n        for feature_arrays in self.data:\n            split_data = self.split_source(feature_arrays)\n            yield self.get_splits(split_data)\n            \n    def split_source(self, feature_arrays):\n        res = defaultdict(dict)\n        for feature_name, feature_array in feature_arrays.items():\n            if feature_name == self.col_id:\n                res[self.col_id] = feature_array\n            else:\n                source_name, feature_name_transform = self.get_names(feature_name)\n                res[source_name][feature_name_transform] = feature_array\n        for source in self.source_names:\n            if source not in res:\n                res[source] = {source_feature: torch.tensor([]) for source_feature in self.source_features[source]}\n        # print(f'res = {res}')\n        return res\n    \n    def get_names(self, feature_name):\n        idx_del = feature_name.find('_')\n        return feature_name[:idx_del], feature_name[idx_del + 1:]\n    \n    def get_splits(self, feature_arrays):\n        res = {}\n        for source_name, feature_array in feature_arrays.items():\n            if source_name != self.col_id:\n                local_date = feature_array[self.col_time]\n                if source_name not in self.splitters:\n                    continue\n                indexes = self.splitters[source_name].split(local_date)\n                res[source_name] = [{k: v[ix] for k, v in feature_array.items() if self.is_seq_feature(k, v)} for ix in indexes]\n        return res\n    \n    #Вернуть диалоги с транзакциями (без таргетов)\n    def collate_fn(self, batch, return_dct_labels=False):\n        dict_class_labels = get_dict_class_labels(batch)\n        batch = reduce(lambda x, y: {k: x[k] + y[k] for k in x if k in y}, batch)\n        # Get regional split only for transactions from MBD\n        # batch['reg_trx_tokens'] = get_regional_splits(batch['trx'], segment_length=3)\n        # print(f\"{batch=}\")\n        padded_batch = collate_multimodal_feature_dict(batch)\n        if return_dct_labels:\n            return padded_batch, dict_class_labels\n        return padded_batch, dict_class_labels[list(dict_class_labels.keys())[0]]\n\ndef collate_multimodal_feature_dict(batch):\n    res = {}\n    for source, source_batch in batch.items():\n        res[source] = collate_feature_dict(source_batch)\n    # print(f\"multimodal_feature_dict = {res['trx'].payload['event_time'].size()}\")\n    # print()\n    return res\n\ndef collate_feature_dict(batch):\n    new_x_ = defaultdict(list)\n    for i, x in enumerate(batch):\n        for k, v in x.items():\n            new_x_[k].append(v)\n    \n    seq_col = next(k for k, v in batch[0].items() if FeatureDict.is_seq_feature(k, v))\n    lengths = torch.LongTensor([len(rec[seq_col]) for rec in batch])\n    new_x = {}\n    for k, v in new_x_.items():\n        if type(v[0]) is torch.Tensor:\n            if k.startswith('target'):\n                new_x[k] = torch.stack(v, dim=0)\n            else:\n                new_x[k] = torch.nn.utils.rnn.pad_sequence(v, batch_first=True)\n        elif type(v[0]) is np.ndarray:\n            new_x[k] = v  # list of arrays[object]\n        else:\n            v = np.array(v)\n            if v.dtype.kind == 'i':\n                new_x[k] = torch.from_numpy(v).long()\n            elif v.dtype.kind == 'f':\n                new_x[k] = torch.from_numpy(v).float()\n            elif v.dtype.kind == 'b':\n                new_x[k] = torch.from_numpy(v).bool()\n            else:\n                new_x[k] = v\n    return PaddedBatch(new_x, lengths)\n    \ndef get_dict_class_labels(batch):\n    res = defaultdict(list)\n    for i, samples in enumerate(batch):\n        for source, values in samples.items():\n            for _ in values:\n                res[source].append(i)\n    for source in res:\n        res[source] = torch.LongTensor(res[source])\n    return dict(res)\n\nclass MultiModalDiffSplitIterableDataset(MultiModalDiffSplitDataset, torch.utils.data.IterableDataset):\n    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ptls.frames.coles import MultiModalIterableDataset\nfrom lightning.pytorch.loggers import WandbLogger\nimport ptls\n\ndata_module = PtlsDataModule(\n    train_data=MultiModalDiffSplitIterableDataset(\n        data=train,\n        splitters= {\n            'trx': SampleSlices(\n                split_count=3,\n                cnt_min=16,\n                cnt_max=90\n            ),\n            'dial': SampleSlices(\n                split_count=3,\n                cnt_min=2,\n                cnt_max=10\n            ),\n        },\n        source_features={\n            \"trx\": [\n                \"event_type\",\n                \"event_subtype\",\n                \"src_type11\",\n                \"src_type12\",\n                \"dst_type11\",\n                \"dst_type12\",\n                \"src_type22\",\n                \"src_type32\"\n            ],\n            \"dial\": [\n                \"embedding\"\n            ],\n        },\n        col_id='client_id',\n        col_time='event_time',\n        source_names=['trx', 'dial'],\n    ),\n    valid_data=MultiModalDiffSplitIterableDataset(\n        data=valid,\n        splitters= {\n            'trx': SampleSlices(\n                split_count=2,\n                cnt_min=5,\n                cnt_max=64\n            ),\n            'dial': SampleSlices(\n                split_count=2,\n                cnt_min=2,\n                cnt_max=10\n            ),\n            },\n        source_features={\n            \"trx\": [\n                \"event_type\",\n                \"event_subtype\",\n                \"src_type11\",\n                \"src_type12\",\n                \"dst_type11\",\n                \"dst_type12\",\n                \"src_type22\",\n                \"src_type32\"\n            ],\n            \"dial\": [\n                \"embedding\"\n            ],\n        },\n        col_id='client_id',\n        col_time='event_time',\n        source_names=['trx', 'dial'],\n    ),\n    train_batch_size=64,\n    train_num_workers=0,\n    valid_batch_size=64,\n    valid_num_workers=0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_data_test=MultiModalDiffSplitIterableDataset(\n#         data=train,\n#         splitters= {\n#             'trx': SampleSlices(\n#                 split_count=3,\n#                 cnt_min=16,\n#                 cnt_max=90\n#             ),\n#             'dial': SampleSlices(\n#                 split_count=3,\n#                 cnt_min=2,\n#                 cnt_max=10\n#             ),\n#         },\n#         source_features={\n#             \"trx\": [\n#                 \"event_type\",\n#                 \"event_subtype\",\n#                 \"src_type11\",\n#                 \"src_type12\",\n#                 \"dst_type11\",\n#                 \"dst_type12\",\n#                 \"src_type22\",\n#                 \"src_type32\"\n#             ],\n#             \"dial\": [\n#                 \"embedding\"\n#             ],\n#         },\n#         col_id='client_id',\n#         col_time='event_time',\n#         source_names=['trx', 'dial'],\n#     )\n\n# next(iter(data_module.train_dl(train_data_test)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Add region attention in trx encoder","metadata":{}},{"cell_type":"code","source":"# x_new = torch.rand((192, 87, 32))\n# segment_length = 10\n# pad_length = (segment_length - (x_new.size()[1] % segment_length)) % segment_length\n# padded_x_new = F.pad(x_new, ((0, 0, 0, pad_length, 0, 0)), 'constant', 0)\n# segmented_tensors = torch.stack(torch.split(padded_x_new, segment_length, dim=1), dim=0)\n# segmented_tensors = segmented_tensors.permute(0, 2, 1, 3)\n# segmented_tensors.size()\n\n# a = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n# b = torch.Tensor([[4, 5, 6]])\n# a = torch.cat((a, b), dim=0)\n# a\n# torch.Tensor([])\n\na = torch.rand((128, 64, 32))\nb = torch.rand((128, 64))\n\nprint(b[:, :, None].size())\n\nc = a + b[:, :, None]\nprint(a)\nprint(c)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# from ptls.constant_repository import TORCH_EMB_DTYPE\nfrom ptls.data_load import PaddedBatch\nfrom ptls.nn.seq_encoder.rnn_encoder import RnnEncoder\nfrom ptls.nn.seq_encoder.transformer_encoder import TransformerEncoder\nfrom ptls.nn.seq_encoder.longformer_encoder import LongformerEncoder\nfrom ptls.nn.seq_encoder.custom_encoder import Encoder\nfrom ptls.nn.trx_encoder import TrxEncoder\nfrom ptls.nn.seq_encoder.containers import SeqEncoderContainer\n\nclass RnnSeqEncoderRegAttn(SeqEncoderContainer):\n    def __init__(self,\n                 trx_encoder=None,\n                 input_size=None,\n                 is_reduce_sequence=True,\n                 **seq_encoder_params,\n                 ):\n        super().__init__(\n            trx_encoder=trx_encoder,\n            seq_encoder_cls=RnnEncoder,\n            input_size=input_size,\n            seq_encoder_params=seq_encoder_params,\n            is_reduce_sequence=is_reduce_sequence,\n        )\n        \n        self.reg_seq_encoder = RnnEncoder(\n            input_size=input_size if input_size is not None else trx_encoder.output_size,\n            is_reduce_sequence=is_reduce_sequence,\n            **seq_encoder_params,\n        )\n\n        self.emb_dim = 192\n        self.regional_attention = nn.MultiheadAttention(\n            embed_dim=self.emb_dim,\n            num_heads=8,\n            dropout=0.3,\n            batch_first=True\n        )\n    \n    \n    def forward(self, x, names=None, seq_len=None, h_0=None):\n        # print(f\"x_in = {x.payload['amount'].size()}\")\n        x = self.trx_encoder(x)\n\n        x_new = x.payload\n        \n        segment_length = 5\n        pad_length = (segment_length - (x_new.size()[1] % segment_length)) % segment_length\n        padded_x_new = F.pad(x_new, ((0, 0, 0, pad_length, 0, 0)), 'constant', 0)\n        segmented_tensors = torch.stack(torch.split(padded_x_new, segment_length, dim=1)).to(x_new.device)\n        \n        regional_embeddings = torch.Tensor().to(x.device)\n        for tensor in segmented_tensors:\n            tensor = PaddedBatch(tensor.permute(1, 0, 2), [tensor.size()[0]] * tensor.size()[1])\n            regional_embed = self.reg_seq_encoder(tensor)\n            regional_embeddings = torch.cat((regional_embeddings, regional_embed), 0)\n        \n        regional_embeddings = regional_embeddings[:len(regional_embeddings) - pad_length, :]\n        # layer_norm = nn.LayerNorm([regional_embeddings.size()[0], regional_embeddings.size()[1]])\n        layer_norm.to(x.device)\n        regional_embeddings = layer_norm(regional_embeddings)\n        \n        if regional_embeddings.size()[1] != self.emb_dim:\n            regional_embeddings = F.pad(regional_embeddings, ((0, abs(regional_embeddings.size()[1] - self.emb_dim), 0, 0)), 'constant', 0)\n        x_reg_embed, _ = self.regional_attention(regional_embeddings, regional_embeddings, regional_embeddings)\n        if regional_embeddings.size()[1] != x_new.size()[0]:\n            x_reg_embed = x_reg_embed[:, :-abs(regional_embeddings.size()[1] - x_new.size()[0])]\n        x_reg_embed = x_reg_embed.permute(1, 0)\n        x_reg_embed = x_reg_embed[:, :, None]\n        x_new = x_new + x_reg_embed\n        x_new.to(x.device)\n        x_new = PaddedBatch(x_new, x.seq_lens)\n        # x_new.to(x.device)\n        x = self.seq_encoder(x_new, h_0)\n        # print(f\"rnn_x_size = {x.size()}\")\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ptls.frames.abs_module import ABSModule\nfrom ptls.frames.coles.metric import metric_recall_top_K, outer_cosine_similarity, outer_pairwise_distance\nfrom ptls.frames.coles.losses import ContrastiveLoss\nfrom torch import nn\nfrom ptls.nn.seq_encoder.custom_encoder import MLP\nimport math\n\n\n# class CrossAttention(nn.Module):\n#     def __init__(self, d_in, d_out_kq, d_out_v):\n#         super().__init__()\n#         self.d_out_kq=d_out_kq\n#         self.W_query=nn.Parameter(torch.rand(d_in, d_out_kq))\n#         self.W_key  = nn.Parameter(torch.rand(d_in, d_out_kq))\n#         self.W_value=nn.Parameter(torch.rand(d_in, d_out_v))\n    \n#     def forward(self, x_1, x_2):\n#         queries_1=x_1.matmul(self.W_query)\n#         keys_2=x_2.matmul(self.W_key)\n#         values_2=x_2.matmul(self.W_value)\n        \n#         attn_scores=queries_1.matmul(keys_2.T)\n#         attn_weights=torch.softmax(\n#             attn_scores/self.d_out_kq**0.5, dim=-1\n#         )\n        \n#         context_vec=attn_weights.matmul(values_2)\n#         return context_vec\n\ndef first(iterable, default=None):\n    iterator = iter(iterable)\n    return next(iterator, default)\n\n\n# class PositionalEncoding(nn.Module):\n#     def __init__(self,\n#                  d_model,\n#                  use_start_random_shift=True,\n#                  max_len=5000,\n#                  ):\n#         super().__init__()\n#         self.use_start_random_shift = use_start_random_shift\n#         self.max_len = max_len\n\n#         pe = torch.zeros(max_len, d_model)\n#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n#         pe[:, 0::2] = torch.sin(position * div_term)\n#         pe[:, 1::2] = torch.cos(position * div_term)\n#         pe = pe.unsqueeze(0)\n#         self.register_buffer('pe', pe)\n\n#     def forward(self, x):\n#         T = x.size(0)\n#         if self.training and self.use_start_random_shift:\n#             start_pos = random.randint(0, self.max_len - T)\n#         else:\n#             start_pos = 0\n#         # print(f'{x.size()=}')\n#         # print(f'{self.pe.size()}')\n#         # print(f'{self.pe[:, start_pos:start_pos + T].size()=}')\n#         x = x + self.pe[:, start_pos:start_pos + T]\n#         return x\n\n# Проверить, что на каждой эпохе лежит разный юзер\n# Сделать выбор positive + negative случайно\n# Сделать self-attention на унимодальности\n\nclass M3CoLESModule(ABSModule):\n    \"\"\"\n    Multi-Modal Matching\n    Contrastive Learning for Event Sequences ([CoLES](https://arxiv.org/abs/2002.08232))\n\n    Subsequences are sampled from original sequence.\n    Samples from the same sequence are `positive` examples\n    Samples from the different sequences are `negative` examples\n    Embeddings for all samples are calculated.\n    Paired distances between all embeddings are calculated.\n    The loss function tends to make positive distances smaller and negative ones larger.\n\n    Parameters\n        seq_encoder:\n            Model which calculate embeddings for original raw transaction sequences\n            `seq_encoder` is trained by `CoLESModule` to get better representations of input sequences\n        head:\n            Model which helps to train. Not used during inference\n            Can be normalisation layer which make embedding l2 length equals 1\n            Can be MLP as `projection head` like in SymCLR framework.\n        loss:\n            loss object from `ptls.frames.coles.losses`.\n            There are paired and triplet loss. They are required sampling strategy\n            from `ptls.frames.coles.sampling_strategies`. Sampling strategy takes a relevant pairs or triplets from\n            pairwise distance matrix.\n        validation_metric:\n            Keep None. `ptls.frames.coles.metric.BatchRecallTopK` used by default.\n        optimizer_partial:\n            optimizer init partial. Network parameters are missed.\n        lr_scheduler_partial:\n            scheduler init partial. Optimizer are missed.\n\n    \"\"\"\n    def __init__(self,\n                 seq_encoders=None,\n                 mod_names=None,\n                 head=None,\n                 loss=None,\n                 validation_metric=None,\n                 optimizer_partial=None,\n                 lr_scheduler_partial=None):\n        torch.set_float32_matmul_precision('high')\n        if head is None:\n            head = ptls.nn.Head(use_norm_encoder=True)\n\n        if loss is None:\n            loss = ContrastiveLoss(margin=0.5,\n                                   sampling_strategy=HardNegativePairSelector(neg_count=5))\n\n        if validation_metric is None:\n            validation_metric = BatchRecallTopK(K=4, metric='cosine')\n        \n        for k in seq_encoders.keys():\n            if type(seq_encoders[k]) is str:\n                seq_encoders[k] = seq_encoders[seq_encoders[k]]\n\n        \n        super().__init__(validation_metric,\n                         first(seq_encoders.values()),\n                         loss,\n                         optimizer_partial,\n                         lr_scheduler_partial)\n\n        # cross_mha_MLP = ptls.nn.seq_encoder.\n        self.mha_trx_dial = nn.MultiheadAttention(\n            embed_dim=128,\n            num_heads=8,\n            dropout=0.3,\n            batch_first=True\n        )\n        self.mha_dial_trx = nn.MultiheadAttention(\n            embed_dim=128,\n            num_heads=8,\n            dropout=0.1,\n            batch_first=True\n        )\n\n        ## MLP variation\n        # self.head_trx = MLP(\n        #         n_in=128,\n        #         n_hidden=128,\n        #         n_out=128\n        #     )\n        # self.head_dial = MLP(\n        #         n_in=128,\n        #         n_hidden=128,\n        #         n_out=128\n        #     ) \n\n        #FFN variant\n        self.head_trx = MLP(\n                n_in=128,\n                n_hidden=256,\n                n_out=128\n            )\n        self.head_dial = MLP(\n                n_in=128,\n                n_hidden=256,\n                n_out=128\n            ) \n\n        self.seq_encoders = torch.nn.ModuleDict(seq_encoders)\n        self._head = head   \n        self.y_h_cache = {'train':[], 'valid': []}\n        \n    @property\n    def metric_name(self):\n        return 'recall_top_k'\n\n    @property\n    def is_requires_reduced_sequence(self):\n        return True\n    \n    def forward(self, x):\n        res = {}\n        for mod_name in x.keys():\n            res[mod_name] = self.seq_encoders[mod_name](x[mod_name])\n        return res\n\n    def shared_step(self, x, y):\n        y_h = self(x)\n        \n        if self._head is not None:\n            y_h_head = {k: self._head(y_h_k) for k, y_h_k in y_h.items()}\n            y_h = y_h_head\n        return y_h, y\n    \n    def _one_step(self, batch, _, stage):\n        y_h, y = self.shared_step(*batch)\n        y_h_list = list(y_h.values())\n        loss = self._loss(torch.cat(y_h_list), torch.cat([y, y]))\n        self.log(f'loss/{stage}', loss.detach())\n        \n        x, y = batch\n        for mod_name, mod_x in x.items():\n            self.log(f'seq_len/{stage}/{mod_name}', x[mod_name].seq_lens.float().mean().detach(), prog_bar=True)\n        \n        if stage == \"valid\":\n            n, d = y_h_list[0].shape\n            y_h_concat = torch.zeros((2*n, d), device = y_h_list[0].device)\n            \n            for i in range(2):\n                y_h_concat[range(i,2*n,2)] = y_h_list[i] \n            if len(self.y_h_cache[stage]) <= 380:\n                self.y_h_cache[stage].append((y_h_concat.cpu(), {k: y_h_k.cpu() for k, y_h_k in y_h.items()} , \n                                             {k:x_k.seq_lens.cpu() for k, x_k in x.items()})) \n        return loss\n    \n    def training_step(self, batch, _):\n        return self._one_step(batch, _, \"train\")\n    \n    def validation_step(self, batch, _):\n        return self._one_step(batch, _, \"valid\")\n    \n    def on_validation_epoch_end(self):        \n        #len_intervals = [(0, 10), (10, 20), (20, 30), (30, 40), (40, 60), (60, 80), (80, 120), (120, 160), (160, 240)]\n        self.log_recall_top_K(self.y_h_cache['valid'], len_intervals=None, stage=\"valid\", K=30)\n        self.log_recall_top_K(self.y_h_cache['valid'], len_intervals=None, stage=\"valid\", K=20)\n        self.log_recall_top_K(self.y_h_cache['valid'], len_intervals=None, stage=\"valid\", K=1)\n        \n        \n        del self.y_h_cache[\"valid\"]\n        self.y_h_cache[\"valid\"] = []\n        \n    def log_recall_top_K(self, y_h_cache, len_intervals=None, stage=\"valid\", K=15):\n        y_h = torch.cat([item[0] for item in y_h_cache], dim = 0)\n        y_h_mods = defaultdict(list)\n        seq_lens_dict = defaultdict(list)\n        \n        for item in y_h_cache:\n            for k, emb in item[1].items():\n                y_h_mods[k].append(emb)\n                \n            for k, l in item[2].items():\n                seq_lens_dict[k].append(l)\n        \n        y_h_mods = {k: torch.cat(el, dim=0) for k ,el in y_h_mods.items()}\n        seq_lens_dict = {k: torch.cat(el) for k ,el in seq_lens_dict.items()}\n\n        #n, _ = y_h.shape\n        #y = torch.zeros((n,)).cpu().long()\n        #y[range(0,n,2)] = torch.arange(0, n//2)\n        #y[range(1,n,2)] = torch.arange(0, n//2)\n        #computed_metric = metric_real_recall_top_K(y_h, y, K=100)\n        y_h_bank, y_h_rmb = list(y_h_mods.values())\n        computed_metric_b2r = metric_recall_top_K_for_embs(y_h_bank, y_h_rmb, torch.arange(y_h_rmb.shape[0]), K=K)\n        computed_metric_r2b = metric_recall_top_K_for_embs(y_h_rmb, y_h_bank, torch.arange(y_h_rmb.shape[0]), K=K)\n        \n        if len_intervals != None:\n            for mod, seq_lens in seq_lens_dict.items():\n                for start, end in len_intervals:\n                    mask = ((seq_lens > start) & (seq_lens <= end))\n\n                    if torch.any(mask):\n                        #y_h_filtered = y_h[mask.repeat_interleave(2)]\n                        y_h_bank_filtered = y_h_bank[mask]\n                        y_h_rmb_filtered = y_h_rmb[mask]\n\n                        #y = torch.div(torch.arange(len(y_h_filtered)), 2, rounding_mode='floor')\n                        #recall = metric_real_recall_top_K(y_h_filtered, y, K=100)\n                        recall_r2b = metric_recall_top_K_for_embs(y_h_rmb_filtered, y_h_bank_filtered, torch.arange(y_h_rmb_filtered.shape[0]), K=30)\n                        recall_b2r = metric_recall_top_K_for_embs(y_h_bank_filtered, y_h_rmb_filtered, torch.arange(y_h_rmb_filtered.shape[0]), K=30)\n\n                        #self.log(f\"{mode}/R@100_len_from_{start}_to_{end}\", recall, prog_bar=True)\n                        print(f\"{stage}/{mod}/r2b_R@100_len_from_{start}_to_{end}\", recall_r2b, prog_bar=True)\n                        self.log(f\"{stage}/{mod}/b2r_R@100_len_from_{start}_to_{end}\", recall_b2r, prog_bar=True)\n        \n        #self.log(f\"{mode}/R@100\", computed_metric, prog_bar=True)\n        self.log(f\"{stage}/click2trx_R@{K}\", computed_metric_r2b, prog_bar=True)\n        self.log(f\"{stage}/trx2click_R@{K}\", computed_metric_b2r, prog_bar=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def metric_real_recall_top_K(X, y, K, num_pos=1, metric='cosine'):\n    \"\"\"\n        calculate metric R@K\n        X - tensor with size n x d, where n - number of examples, d - size of embedding vectors\n        y - true labels\n        N - count of closest examples, which we consider for recall calcualtion\n        metric: 'cosine' / 'euclidean'.\n            !!! 'euclidean' - to slow for datasets bigger than 100K rows\n    \"\"\"\n    K_adjusted = min(X.size(0) - 1, K)\n    \n    res = []\n\n    n = X.size(0)\n    d = X.size(1)\n    max_size = 2 ** 32\n    batch_size = max(1, max_size // (n * d))\n\n    with torch.no_grad():\n\n        for i in range(1 + (len(X) - 1) // batch_size):\n\n            id_left = i * batch_size\n            id_right = min((i + 1) * batch_size, len(y))\n            y_batch = y[id_left:id_right]\n            # print(f\"X = {X}\")\n            # print(f\"X[] = {X[id_left:id_right]}\")\n            if metric == 'cosine':\n                pdist = -1 * outer_cosine_similarity(X, X[id_left:id_right])\n            elif metric == 'euclidean':\n                pdist = outer_pairwise_distance(X, X[id_left:id_right])\n            else:\n                raise AttributeError(f'wrong metric \"{metric}\"')\n\n            values, indices = pdist.topk(K_adjusted + 1, 0, largest=False)\n\n            y_rep = y_batch.repeat(K_adjusted, 1)\n            res.append((y[indices[1:]] == y_rep).sum().item())\n\n    return np.sum(res) / len(y) / num_pos\n\ndef cosine_similarity_matrix(x1, x2):\n    x1_norm = x1 / x1.norm(dim=1)[:, None]\n    x2_norm = x2 / x2.norm(dim=1)[:, None]\n    return torch.mm(x1_norm, x2_norm.transpose(0, 1))\n\ndef metric_recall_top_K_for_embs(embs_1, embs_2, true_matches, K=30):\n    similarity_matrix = cosine_similarity_matrix(embs_1, embs_2)\n    K_adjusted = min(len(embs_1), K)\n    top_k = similarity_matrix.topk(k=K_adjusted, dim=1).indices\n    correct_matches = 0\n    for i, indices in enumerate(top_k):\n        if true_matches[i] in indices:\n            correct_matches += 1\n    recall_at_k = correct_matches / len(similarity_matrix)\n    return recall_at_k\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ptls.nn import TrxEncoder, RnnSeqEncoder\nfrom ptls.frames.coles import CoLESModule\nfrom functools import partial\nimport torch\nfrom ptls.frames.coles import MultiModalSortTimeSeqEncoderContainer\nfrom ptls.nn.trx_encoder.encoders import IdentityEncoder\nfrom ptls.nn.seq_encoder.rnn_encoder import RnnEncoder\nfrom ptls.nn.seq_encoder.transformer_encoder import TransformerEncoder\nfrom ptls.frames.coles.losses import ContrastiveLoss\nfrom ptls.frames.coles.sampling_strategies import HardNegativePairSelector\n\nhead = ptls.nn.Head(\n    input_size=128,\n    use_norm_encoder=True,\n    hidden_layers_sizes=[128, 128],\n    objective=\"regression\",\n    num_classes=128\n)\n\nloss = ptls.frames.coles.losses.SoftmaxLoss()\n\n# With RNN\nseq_encoders = {\n    'trx': RnnSeqEncoderRegAttn(\n        trx_encoder=TrxEncoder(\n            norm_embeddings=False,\n            embeddings_noise=0.003,\n            linear_projection_size=32,\n            embeddings={\n                'event_type': {\"in\": 58, \"out\": 24},\n                'event_subtype': {\"in\": 59, \"out\": 24},\n                'src_type11': {\"in\": 85, \"out\": 24},\n                'src_type12': {\"in\": 349, \"out\": 24},\n                'dst_type11': {\"in\": 84, \"out\": 24},\n                'dst_type12': {\"in\": 417, \"out\": 24},\n                'src_type22': {\"in\": 90, \"out\": 24},\n                'src_type32': {\"in\": 91, \"out\": 24}\n            },\n            numeric_values={\n                'amount': 'log'\n            }\n        ),\n        type='gru',\n        hidden_size=128\n    ),\n    'dial': RnnSeqEncoder(\n        trx_encoder=TrxEncoder(\n            embeddings_noise=0.003,\n            linear_projection_size=32,\n            custom_embeddings={\n                'embedding': IdentityEncoder(768)\n            }\n        ),\n        type='gru',\n        hidden_size=128\n    )\n}\n\noptimizer_partial = partial(\n    torch.optim.AdamW,\n    lr=0.001,\n    weight_decay=1e-4\n)\n\nlr_scheduler_partial = partial(\n    torch.optim.lr_scheduler.StepLR,\n    step_size=1,\n    gamma=0.9\n)\n\n# pl_module = M3CoLESModule(\n#     validation_metric=ptls.frames.coles.metric.BatchRecallTopK(\n#         K=1,\n#         metric='cosine',\n#     ),\n#     head=head,\n#     seq_encoders=seq_encoders,\n#     loss=loss,\n#     optimizer_partial=optimizer_partial,\n#     lr_scheduler_partial=lr_scheduler_partial\n# )\n\npl_module = M3CoLESModule(\n    validation_metric=ptls.frames.coles.metric.BatchRecallTopK(\n        K=10,\n        metric='cosine',\n    ),\n    head=head,\n    seq_encoders=seq_encoders,\n    loss=loss,\n    optimizer_partial=optimizer_partial,\n    lr_scheduler_partial=lr_scheduler_partial\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_logger = WandbLogger(project=\"MBD_My_Code\", log_model=\"all\")\n\ntrainer = pl.Trainer(\n    logger=wandb_logger,\n    max_epochs=25,\n    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    enable_progress_bar=True,\n    gradient_clip_val=0.5,\n    log_every_n_steps=50,\n    limit_val_batches=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.fit(pl_module, data_module)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGETS_DATA_PATH = '/kaggle/working/targets/'\n\npreprocessor_target = PysparkDataPreprocessor(\n    col_id=\"client_id\",\n    col_event_time=\"mon\",\n    event_time_transformation=\"dt_to_timestamp\",\n    cols_identity=[\"target_1\", \"target_2\", \"target_3\", \"target_4\"],\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"targets = spark.read.parquet(os.path.join(TARGETS_DATA_PATH , f'fold={fold}'))\nmmt_dataset = spark.read.parquet(os.path.join(MM_DATA_PATH , f'fold={fold}'))\n\ntargets = preprocessor_target.fit_transform(targets).drop(*['event_time' ,'trans_count', 'diff_trans_date'])\nmmt_dataset = mmt_dataset.join(targets, on='client_id', how='left')\nmmt_dataset.write.mode('overwrite').parquet(os.path.join(MMT_DATA_PATH, f'fold={fold}'))\n\ndel mmt_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MatchingModalities(IterableProcessingDataset):\n    def __init__(\n        self,\n        col_id='client_id',\n        mod1_col_time='trx_event_time',\n        mod2_col_time='dial_event_time',\n        mod1_name='trx',\n        mod2_name='dial'\n    ):\n        super().__init__()\n        self.col_id = col_id\n        self.mod1_col_time = mod1_col_time\n        self.mod2_col_time = mod2_col_time\n        self.mod1_name=mod1_name\n        self.mod2_name=mod2_name\n\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features = features.copy()\n            all_event_time, _ = torch.sort(torch.cat((features[self.mod1_col_time], features[self.mod2_col_time])))\n            mod1_mask = torch.isin(all_event_time, features[self.mod1_col_time])\n            mod2_mask = torch.isin(all_event_time, features[self.mod2_col_time])\n            for key, tens in features.items():\n                if key.startswith(self.mod1_name) and key != self.mod1_col_time:\n                    indices = torch.where(~mod1_mask)[0].tolist()\n                    result = []\n                    for i in range(len(mod1_mask)):\n                        if i in indices:\n                            result.append(0)\n                        if i < len(tens):\n                            result.append(tens[i])\n                    features[key] = torch.tensor(result, dtype=features[key].dtype)\n                    # print(f'mod1_size = {features[key].size()}')\n                elif key.startswith(self.mod2_name) and key != self.mod2_col_time:\n                    indices = torch.where(~mod2_mask)[0].tolist()\n                    # print(indices)\n                    result = []\n                    for i in range(len(mod2_mask)):\n                        if i in indices:\n                            result.append(torch.Tensor([0 for i in range(768)]))\n                        if i < len(tens):\n                            result.append(tens[i])\n                    # print(len(result))\n                    # print(result)\n                    features[key] = torch.stack(result, dim=0)\n                    # print(f'mod2_size = {features[key].size()}')\n            features[self.mod1_col_time] = all_event_time\n            features[self.mod2_col_time] = all_event_time\n            \n            yield features   \n\nclass MMToTorch(IterableProcessingDataset):\n    def __init__(\n        self,\n        col_id='client_id'\n    ):\n        super().__init__()\n        self.col_id='client_id'\n\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features = features.copy()\n            for key, value in features.items():\n                if key != 'client_id':\n                    features[key] = torch.Tensor(value)\n            yield features\n\nclass DialToTorch(IterableProcessingDataset):\n    def __init__(\n        self,\n        embedding_col='dial_embedding'\n    ):\n        super().__init__()\n        self.embedding_col=embedding_col\n\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features = features.copy()\n            features[self.embedding_col] = np.stack(features[self.embedding_col], axis=0).astype(np.int32)\n            features[self.embedding_col] = torch.FloatTensor(features[self.embedding_col])\n            yield features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import copy\n\nclass GetSplit(IterableProcessingDataset):\n    def __init__(\n        self,\n        start_month,\n        end_month,\n        year=2022,\n        col_id='client_id',\n        trx_col_time='trx_event_time',\n        dial_col_time='dial_event_time'\n    ):\n        super().__init__()\n        self.start_month = start_month\n        self.end_month = end_month\n        self._year = year\n        self._col_id = col_id\n        self._trx_col_time = trx_col_time\n        self._dial_col_time = dial_col_time\n        \n    def __iter__(self):\n        for rec in self._src:\n            for month in range(self.start_month, self.end_month+1):\n                features = rec[0] if type(rec) is tuple else rec\n                features = features.copy()\n                # print(f'features event time size = {features[\"trx_event_time\"].size()}')\n                if month == 12:\n                    month_event_time = datetime(self._year + 1, 1, 1).timestamp()\n                else:\n                    month_event_time = datetime(self._year, month + 1, 1).timestamp()\n                    \n                year_event_time = datetime(self._year, 1, 1).timestamp()\n\n                # print(f\"{rec}\")\n                # print(f\"{month_event_time=}\")\n                trx_mask = features[self._trx_col_time] < month_event_time\n                dial_mask = features[self._dial_col_time] < month_event_time\n                \n                for key, tensor in features.items():\n                    if key.startswith('target'):\n                        features[key] = tensor[month - 1].tolist()\n                    if key.startswith('trx'):\n                        features[key] = tensor[trx_mask]\n                        if len(features[key]) == 0:\n                            if key == 'trx_event_time':\n                                features[key] = torch.Tensor([month_event_time]).to(torch.int32)\n                            else:\n                                features[key] = torch.Tensor([0])\n\n                    if key.startswith('dial'):\n                        features[key] = tensor[dial_mask]\n                        if len(features[key]) == 0:\n                            if key == 'dial_event_time':\n                                features[key] = torch.Tensor([month_event_time]).to(torch.int32)\n\n                            else:\n                                features[key] = torch.zeros([1, 768])\n\n                    # elif key != self._col_id:\n                        # print(f'mask_size = {mask.size()}')\n                        # print(f'tensor size = {tensor.size()}')\n                        # print(f'key = {key}')\n                        # if mask.size()[0] > tensor.size()[0]:\n                        #     mask = mask[:tensor.size()[0]]\n                        # if mask.size()[0] < tensor.size()[0]:\n                        #     tensor = tensor[:mask.size()[0]]\n                        # features[key] = tensor[mask] \n                        # print(f'features[key] size = {features[key].size()}')\n                        # print('=====================')\n                    \n                            \n                features[self._col_id] += '_month=' + str(month)\n\n                yield features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ptls.frames.coles import MultiModalInferenceIterableDataset\n\ndataset_inf = ParquetDataset(\n    data_files=[\n        os.path.join(MMT_DATA_PATH, f'fold={4}')\n    ],\n    i_filters=[\n        DeleteNan('trx_event_time'),\n        DeleteNan('dial_event_time'),\n        # SeqLenFilter(min_seq_len=1),\n        # ISeqLenLimit(max_seq_len=128),\n        ptls.data_load.iterable_processing.to_torch_tensor.ToTorch(),\n        # MMToTorch('client_id'),\n        DialToTorch(),\n        # MatchingModalities(\n        #     col_id='client_id',\n        #     mod1_col_time='trx_event_time',\n        #     mod2_col_time='dial_event_time'\n        # ),\n        GetSplit(\n            start_month=1,\n            end_month=12,\n            col_id='client_id',\n            trx_col_time='trx_event_time',\n            dial_col_time='dial_event_time'\n        )\n    ],\n    shuffle_files=False\n)\n\n# print(next(iter(dataset_inf)))\n\n\ndataset_inf = MultiModalInferenceIterableDataset(\n        data=dataset_inf,\n        source_features={\n            \"trx\": [\n                \"event_type\",\n                \"event_subtype\",\n                \"src_type11\",\n                \"src_type12\",\n                \"dst_type11\",\n                \"dst_type12\",\n                \"src_type22\",\n                \"src_type32\"\n            ],\n            \"dial\": [\n                \"embedding\"\n            ]\n        },\n        col_id='client_id',\n        col_time='event_time',\n        source_names=['trx', 'dial'],\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from ptls.custom_layers import StatPooling\n# from ptls.nn.seq_step import LastStepEncoder\nfrom itertools import chain\n\nclass InferenceModuleMultimodal(pl.LightningModule):\n    def __init__(self, model, pandas_output=True, drop_seq_features=True, model_out_name='out', col_id = 'epk_id'):\n        super().__init__()\n\n        self.model = model\n        self.pandas_output = pandas_output\n        self.drop_seq_features = drop_seq_features\n        self.model_out_name = model_out_name\n        self.col_id = col_id\n\n    def forward(self, x: PaddedBatch):\n        x, batch_ids, targets = x\n\n        #TODO: Проверить batch_ids == col_id. Проверил, совпадают\n        out = self.model(x)\n        # print(f\"{_=}\")\n        x_out = {self.col_id : batch_ids, self.model_out_name: out}\n        if self.pandas_output:\n            return self.to_pandas(x_out, targets)\n        return x_out\n\n    def to_pandas(self, x, targets):\n        expand_cols = []\n        scalar_features = {}\n        if self.model_out_name in x:\n            for k, v in x[self.model_out_name].items():\n                x[k] = v\n        del x[self.model_out_name]\n        for k, v in x.items():\n            if type(v) is torch.Tensor:\n                v = v.cpu().numpy()\n            if type(v) is list or len(v.shape) == 1:\n                scalar_features[k] = v\n            elif len(v.shape) == 2:\n                expand_cols.append(k)\n            else:\n                scalar_features[k] = None\n\n        dataframes = [pd.DataFrame(scalar_features)]\n        targets_dataframe = pd.DataFrame([item[0] for item in targets])\n        for col in expand_cols:\n            v = x[col].cpu().numpy()\n            dataframes.append(pd.DataFrame(v, columns=[f'{col}_{i:04d}' for i in range(v.shape[1])]))\n        return pd.concat(dataframes, axis=1).join(targets_dataframe)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_feature_dict(batch):\n    new_x_ = defaultdict(list)\n    for i, x in enumerate(batch):\n        for k, v in x.items():\n            new_x_[k].append(v)\n    \n    seq_col = next(k for k, v in batch[0].items() if FeatureDict.is_seq_feature(k, v))\n    lengths = torch.LongTensor([len(rec[seq_col]) for rec in batch])\n    new_x = {}\n    for k, v in new_x_.items():\n        # print(new_x)\n        if type(v[0]) is torch.Tensor:\n            if k.startswith('target'):\n                new_x[k] = torch.stack(v, dim=0)\n            else:\n                new_x[k] = torch.nn.utils.rnn.pad_sequence(v, batch_first=True)\n        elif type(v[0]) is np.ndarray:\n            new_x[k] = v  # list of arrays[object]\n        else:\n            v = np.array(v)\n            if v.dtype.kind == 'i':\n                new_x[k] = torch.from_numpy(v).long()\n            elif v.dtype.kind == 'f':\n                new_x[k] = torch.from_numpy(v).float()\n            elif v.dtype.kind == 'b':\n                new_x[k] = torch.from_numpy(v).bool()\n            else:\n                new_x[k] = v\n    return PaddedBatch(new_x, lengths)\n\ndef collate_multimodal_feature_dict(batch):\n    res = {}\n    for source, source_batch in batch.items():\n        res[source] = collate_feature_dict(source_batch)\n    return res\n\ndef collate_feature_dict_with_target(batch, col_id='client_id', target_col_names=None):\n    batch_ids = []\n    target_cols = []\n    # print('batch_in')\n    for sample in batch:\n        batch_ids.append(sample[col_id])\n        del sample[col_id]\n        if target_col_names is not None:\n            for target_col in target_col_names:\n                target_cols.append(sample[target_col])\n                del sample[target_col]\n    # print(batch)\n    batch = reduce(lambda x, y: {k: x[k] + y[k] for k in x if k in y}, batch)\n    # print(batch)\n    padded_batch = collate_multimodal_feature_dict(batch)\n    # print(padded_batch['trx'].payload)\n    # print(batch_ids)\n    # print(padded_batch['trx'].payload['event_time'].size())\n    # print(padded_batch['trx'].payload)\n    if target_col_names is not None:\n        return padded_batch, batch_ids, target_cols\n    return padded_batch, batch_ids[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ncollate_fn = partial(\n    collate_feature_dict_with_target,\n    target_col_names=['target']\n)\n\ninference_dl = DataLoader(\n    dataset=dataset_inf,\n    collate_fn=collate_fn,\n    shuffle=False,\n    num_workers=0,\n    batch_size=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_module = InferenceModuleMultimodal(\n    model=pl_module,\n    pandas_output=True,\n    col_id='client_id',\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_embeddings = pd.concat(trainer.predict(inf_module, inference_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import Pool, CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\n\ninf_train, inf_test = train_test_split(inf_embeddings, test_size=0.2)\ninf_train, inf_val = train_test_split(inf_train, test_size=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_1_inf_train, y_1_inf_test = inf_train['1'].to_numpy(), inf_test['1'].to_numpy()\ny_2_inf_train, y_2_inf_test = inf_train['2'].to_numpy(), inf_test['2'].to_numpy()\ny_3_inf_train, y_3_inf_test = inf_train['3'].to_numpy(), inf_test['3'].to_numpy()\ny_4_inf_train, y_4_inf_test = inf_train['4'].to_numpy(), inf_test['4'].to_numpy()\n\ny_inf_tests = [\n    y_1_inf_test,\n    y_2_inf_test,\n    y_3_inf_test,\n    y_4_inf_test\n]\n\nX_inf_train, X_inf_test = inf_train.drop(columns=['client_id', '1', '2', '3', '4']).to_numpy(), inf_test.drop(columns=['client_id', '1', '2', '3', '4']).to_numpy()\ninf_val_pairs = {\n    'X_inf': [\n        inf_val.drop(columns=['client_id', '1', '2', '3', '4']).to_numpy(),\n        inf_val.drop(columns=['client_id', '1', '2', '3', '4']).to_numpy(),\n        inf_val.drop(columns=['client_id', '1', '2', '3', '4']).to_numpy(),\n        inf_val.drop(columns=['client_id', '1', '2', '3', '4']).to_numpy()\n    ],\n    'y_inf': [\n        inf_val['1'].to_numpy(),\n        inf_val['2'].to_numpy(),\n        inf_val['3'].to_numpy(),\n        inf_val['4'].to_numpy()\n    ]\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nmodels = [LGBMClassifier(\n    n_estimators=500,\n    boosting_type='gbdt',\n    subsample=0.5,\n    subsample_freq=1,\n    learning_rate=0.02,\n    feature_fraction=0.75,\n    max_depth=6,\n    lambda_l1=1,\n    lambda_l2=1,\n    min_data_in_leaf=50,\n    random_state=42,\n    n_jobs=8,\n    verbose=-1\n) for _ in range(4)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_datasets = [\n    (X_inf_train, y_1_inf_train),\n    (X_inf_train, y_2_inf_train),\n    (X_inf_train, y_3_inf_train),\n    (X_inf_train, y_4_inf_train)\n]\nval_datasets = [\n    (inf_val_pairs['X_inf'][0], inf_val_pairs['y_inf'][0]),\n    (inf_val_pairs['X_inf'][1], inf_val_pairs['y_inf'][1]),\n    (inf_val_pairs['X_inf'][2], inf_val_pairs['y_inf'][2]),\n    (inf_val_pairs['X_inf'][3], inf_val_pairs['y_inf'][3])\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(models)):\n    models[i].fit(train_datasets[i][0], train_datasets[i][1], eval_set=val_datasets[i])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\npreds = []\nfor i in range(len(models)):\n    preds.append(models[i].predict_proba(X_inf_test))\n    # print(classification_report(y_inf_tests[i], preds[i]))\n    print(f\"ROC-AUC target_{i} = {roc_auc_score(y_inf_tests[i], preds[i][:, 1])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CoLES + regional attention in trx (segment_length=10) + LightGBM (Without LN):**\n\nROC-AUC target_0 = 0.6674023787783245\n\nROC-AUC target_1 = 0.8062314626627639\n\nROC-AUC target_2 = 0.5838852723065378\n\nROC-AUC target_3 = 0.6902740886029226\n\n**CoLES + regional attention in trx + LightGBM (With LN):**\n\nROC-AUC target_0 = 0.6835130424746901\n\nROC-AUC target_1 = 0.7069505322479649\n\nROC-AUC target_2 = 0.5838252484700381\n\nROC-AUC target_3 = 0.6956062235064757\n\n\n**CoLES + regional attention in trx + LightGBM (Witр BN):**\n\nROC-AUC target_0 = 0.5941521400656775\n\nROC-AUC target_1 = 0.72975601217508\n\nROC-AUC target_2 = 0.5737223564128565\n\nROC-AUC target_3 = 0.618690245938203\n\n**CoLES + regional attention in trx (segment_length=5) (With LN)**\n\nROC-AUC target_0 = 0.702929348375156\n\nROC-AUC target_1 = 0.5948162619622575\n\nROC-AUC target_2 = 0.576047203889645\n\nROC-AUC target_3 = 0.7450892857142857","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}