{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Commands for downgrading Python version in Kaggle notebook\n\n#adds external APT repository\n!add-apt-repository -y 'ppa:deadsnakes/ppa'\n\n#installs specific python version\n!apt install -y python3.10\n\n#installs distutils package for the specific python version\n!apt-get install -y python3.10-distutils\n\n#Install Python modules using the newly installed Python version\n#-I, Ignores and overwrites the installed packages.\n!sudo /usr/bin/python3.10 -m pip install -Iv <package-name>\n\n#Run Python scripts using the newly installed Python version\n!/usr/bin/python3.10 <python_script.py>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install lightning\n!pip install pyspark\n!pip install pytorch-lifestream","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=\"79f2120f8d4212aceb2c60b3c89a1b6727c19cff\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download \n\nhf_hub_download(repo_id=\"ai-lab/MBD-mini\", filename=\"ptls.tar.gz\", repo_type=\"dataset\", local_dir=\"/kaggle/working/\")\nhf_hub_download(repo_id=\"ai-lab/MBD-mini\", filename=\"targets.tar.gz\", repo_type=\"dataset\", local_dir=\"/kaggle/working/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!tar -xf ptls.tar.gz\n!tar -xf targets.tar.gz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nimport pyspark\nfrom pyspark.sql import SparkSession\n# import pyspark.sql.functions as F\nfrom pyspark.sql import types as T\nimport time\nimport datetime\nfrom ptls.data_load.datasets import ParquetDataset, ParquetFiles\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType\nfrom tqdm.notebook import tqdm\nfrom ptls.preprocessing import PysparkDataPreprocessor\nimport pytorch_lightning as pl\nfrom ptls.data_load.datasets import MemoryMapDataset\nfrom ptls.data_load.iterable_processing import SeqLenFilter, FeatureFilter\nfrom ptls.data_load.iterable_processing.iterable_seq_len_limit import ISeqLenLimit\nfrom ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\nfrom ptls.frames.coles import CoLESModule\nfrom ptls.frames import PtlsDataModule\nfrom ptls.frames.coles import ColesDataset\nfrom ptls.frames.coles.split_strategy import SampleSlices\nimport torch\nimport numpy as np\nimport pandas as pd\nimport calendar\nfrom glob import glob\nfrom ptls.data_load.utils import collate_feature_dict\n\nfrom ptls.data_load.iterable_processing_dataset import IterableProcessingDataset\nfrom datetime import datetime\nfrom ptls.data_load.padded_batch import PaddedBatch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SEED = 0\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"spark_conf = pyspark.SparkConf()\nspark_conf.setMaster(\"local[*]\").setAppName(\"JoinModality\")\nspark_conf.set(\"spark.driver.maxResultSize\", \"16g\")\nspark_conf.set(\"spark.executor.memory\", \"24g\")\nspark_conf.set(\"spark.executor.memoryOverhead\", \"16g\")\nspark_conf.set(\"spark.driver.memory\", \"16g\")\nspark_conf.set(\"spark.driver.memoryOverhead\", \"16g\")\nspark_conf.set(\"spark.cores.max\", \"4\")\nspark_conf.set(\"spark.sql.shuffle.partitions\", \"200\")\nspark_conf.set(\"spark.local.dir\", \"../../spark_local_dir\")\n\n\nspark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sparkContext.getConf().getAll()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/ptls/trx_supervised","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGETS_DATA_PATH = '/kaggle/working/targets/'\nTRX_DATA_PATH = '/kaggle/working/ptls/trx/'\nTRX_SUPERVISED_PATH = '/kaggle/working/ptls/trx_supervised/'\n\npreprocessor_target = PysparkDataPreprocessor(\n    col_id=\"client_id\",\n    col_event_time=\"mon\",\n    event_time_transformation=\"dt_to_timestamp\",\n    cols_identity=[\"target_1\", \"target_2\", \"target_3\", \"target_4\"],\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for fold in range(5):\n    targets = spark.read.parquet(os.path.join(TARGETS_DATA_PATH , f'fold={fold}'))\n    trx = spark.read.parquet(os.path.join(TRX_DATA_PATH , f'fold={fold}'))\n    \n    targets = preprocessor_target.fit_transform(targets).drop(*['event_time' ,'trans_count', 'diff_trans_date'])\n    trx = trx.join(targets, on='client_id', how='left')\n    trx.write.mode('overwrite').parquet(os.path.join(TRX_SUPERVISED_PATH, f'fold={fold}'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"spark.stop()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom ptls.data_load.iterable_processing_dataset import IterableProcessingDataset\nfrom ptls.data_load import IterableChain\nfrom datetime import datetime\nfrom ptls.data_load.datasets.parquet_dataset import ParquetDataset, ParquetFiles\nfrom ptls.data_load.iterable_processing.feature_filter import FeatureFilter\nfrom ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\nimport torch\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom ptls.data_load.utils import collate_feature_dict\nfrom tqdm import tqdm\nimport ptls\nimport torch.nn.functional as F\n\nclass TargetToTorch(IterableProcessingDataset):\n    def __init__(self, col_target):\n        super().__init__()\n        self.col_target = col_target\n\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features[self.col_target] = np.stack(np.array(features[self.col_target]))\n            features[self.col_target] = torch.tensor(features[self.col_target])\n            yield features\n\nclass DeleteNan(IterableProcessingDataset):\n    def __init__(self, col_name):\n        super().__init__()\n        self.col_name = col_name\n    \n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            if features[self.col_name] is not None:\n                yield features\n\n\nclass DialToTorch(IterableProcessingDataset):\n    def __init__(self, col_time, col_embeds):\n        super().__init__()\n        self._year=2022\n        self.col_embeds = col_embeds\n        self.col_time = col_time\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features = features.copy()\n            if features[self.col_time] is None:\n                features[self.col_time] = torch.tensor([0])\n            if features[self.col_embeds] is None:\n                features[self.col_embeds] = torch.zeros(768)\n            \n            for key, tens in features.items():\n                if key == self.col_embeds:\n                    features[key] = torch.tensor(tens.tolist())\n\n            yield features\n\nclass GetSplit(IterableProcessingDataset):\n    def __init__(\n        self,\n        start_month,\n        end_month,\n        year=2022,\n        col_id='client_id',\n        col_time='event_time'\n    ):\n        super().__init__()\n        self.start_month = start_month\n        self.end_month = end_month\n        self._year = year\n        self._col_id = col_id\n        self._col_time = col_time\n        \n    def __iter__(self):\n        for rec in self._src:\n            for month in range(self.start_month, self.end_month+1):\n                features = rec[0] if type(rec) is tuple else rec\n                features = features.copy()\n                \n                if month == 12:\n                    month_event_time = datetime(self._year + 1, 1, 1).timestamp()\n                else:\n                    month_event_time = datetime(self._year, month + 1, 1).timestamp()\n                    \n                year_event_time = datetime(self._year, 1, 1).timestamp()\n                \n                mask = features[self._col_time] < month_event_time\n                for key, tensor in features.items():\n                    if key.startswith('target'):\n                        features[key] = tensor[month - 1].tolist()    \n                    elif key != self._col_id:\n                        features[key] = tensor[mask] \n                            \n                features[self._col_id] += '_month=' + str(month)\n\n                yield features\n\nclass GetPatches(IterableProcessingDataset):\n    def __init__(\n        self,\n        col_id='client_id',\n        patches_size=12\n    ):\n        super().__init__()\n        self._col_id = col_id\n        self._patches_size = patches_size\n\n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            features = features.copy()\n\n            patches_dict = {}\n            for key, tensor in features.items():\n                if key == self._col_id:\n                    patches_dict[key] = tensor\n                else:\n                    patches = list(torch.split(tensor, self._patches_size, dim=0))\n                    if patches[-1].size()[0] != self._patches_size:\n                        pad_size = self._patches_size - patches[-1].size()[0]\n                        padding_tensor = torch.Tensor([0] * pad_size).type(tensor.dtype)\n                        patches[-1] = torch.cat((patches[-1], padding_tensor))\n        \n                    patches = torch.stack(patches)\n                    patches_dict[key] = patches\n\n            yield features, patches_dict\n                \n                \n\n\n# class Get2Patches(IterableProcessingDataset):\n#     def __init__(\n#         self,\n#         col_id='client_id',\n#         small_patches_size=3,\n#         large_patches_size=12\n#     ):\n#         super().__init__()\n#         self._col_id = col_id\n#         self._small_patches_size = small_patches_size\n#         self._large_patches_size = large_patches_size\n\n#     def __iter__(self):\n#         for rec in self._src:\n#             features = rec[0] if type(rec) is tuple else rec\n#             features = features.copy()\n\n#             for key, tensor in features.items():\n#                 small_patches = list(torch.split(tensor, self._small_patches_size, dim=1))\n#                 large_patches = list(torch.split(tensor, self._small_patches_size, dim=1))\n#                 if small_patches[-1].size()[1] != self._small_patches_size:\n#                     pad_size = self._small_patches_size - small_patches[-1].size()[1]\n#                     small_patches[-1] = F.pad(small_patches[-1], (0, 0, pad_size, 0), \"replicate\")\n    \n                \n#                 if large_patches[-1].size()[1] != self._large_patches_size:\n#                     pad_size = self._large_patches_size - large_patches[-1].size()[1]\n#                     large_patches[-1] = F.pad(large_patches[-1], (0, 0, pad_size, 0), \"replicate\")\n    \n#                 small_patches = torch._stack(small_patches)\n#                 large_patches = torch._stack(large_patches)\n            \n            \n    \n#             large_comp_embed = torch.zeros(x_new.size()[0], 128).to(x.device)\n\nclass QuantilfyAmount(IterableProcessingDataset):\n    def __init__(self, col_amt='amount', quantilies=None):\n        super().__init__()\n        self.col_amt = col_amt\n        if quantilies is None:\n            self.quantilies = [0., 267.6, 1198.65, 3667.2, 8639.8, 18325.7, 36713.2, 68950.3, 143969.1, 421719.1]\n        else: \n            self.quantilies = quantilies\n    \n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            amount = features[self.col_amt]\n            am_quant = torch.zeros(len(amount), dtype=torch.int)\n            for i, q in enumerate(self.quantilies):\n                am_quant = torch.where(amount>q, i, am_quant)\n            features[self.col_amt] = am_quant\n            yield features\n            ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = ptls.data_load.datasets.ParquetDataset(\n        data_files=[\n            os.path.join(TRX_SUPERVISED_PATH, 'fold=0'),\n            os.path.join(TRX_SUPERVISED_PATH, 'fold=1'),\n            os.path.join(TRX_SUPERVISED_PATH, 'fold=2'),\n        ],\n    i_filters=[\n        ptls.data_load.iterable_processing.SeqLenFilter(min_seq_len=16),\n        ptls.data_load.iterable_processing.SeqLenFilter(max_seq_len=2048),\n        ptls.data_load.iterable_processing.feature_filter.FeatureFilter(\n            drop_feature_names=[\n                'client_id',\n                'target_1',\n                'target_2',\n                'target_3',\n                'target_4'\n            ]\n        ),\n        # QuantilfyAmount(),\n        ptls.data_load.iterable_processing.CategorySizeClip(\n            category_max_size={\n                'event_type': 58,\n                'event_subtype' :59,\n                'src_type11': 85,\n                'src_type12': 349,\n                'dst_type11': 84,\n                'dst_type12': 417,\n                'src_type22': 90,\n                'src_type32': 91,\n                'amount': 10\n            }\n        ),\n        ptls.data_load.iterable_processing.to_torch_tensor.ToTorch(),\n        \n        \n        # GetPatches()\n    ],\n    shuffle_files=True\n)\n\nvalid = ptls.data_load.datasets.ParquetDataset(\n        data_files=[\n            os.path.join(TRX_DATA_PATH, 'fold=3')\n        ],\n    i_filters=[\n        ptls.data_load.iterable_processing.SeqLenFilter(min_seq_len=16),\n        ptls.data_load.iterable_processing.SeqLenFilter(max_seq_len=2048),\n        ptls.data_load.iterable_processing.feature_filter.FeatureFilter(\n            drop_feature_names=[\n                'client_id',\n                'target_1',\n                'target_2',\n                'target_3',\n                'target_4'\n            ]\n        ),\n        # QuantilfyAmount(),\n        ptls.data_load.iterable_processing.CategorySizeClip(\n            category_max_size={\n                'event_type': 58,\n                'event_subtype' :59,\n                'src_type11': 85,\n                'src_type12': 349,\n                'dst_type11': 84,\n                'dst_type12': 417,\n                'src_type22': 90,\n                'src_type32': 91,\n                'amount': 10\n            }\n        ),\n        ptls.data_load.iterable_processing.to_torch_tensor.ToTorch(),\n        \n        # GetPatches()\n    ]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GPT Baseline","metadata":{}},{"cell_type":"code","source":"from ptls.frames.gpt.gpt_dataset import GptIterableDataset\n\ndata_module = ptls.frames.PtlsDataModule(\n    train_data=GptIterableDataset(\n        data=train,\n        min_len=80,\n        max_len=300\n    ),\n    valid_data=GptIterableDataset(\n        data=valid,\n        min_len=80,\n        max_len=300\n    ),\n    train_batch_size=128,\n    train_num_workers=0,\n    valid_batch_size=64,\n    valid_num_workers=0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Gpt Module**","metadata":{}},{"cell_type":"code","source":"import pytorch_lightning as pl\nimport torch\nfrom torch import nn\nimport warnings\nfrom torchmetrics import MeanMetric\nfrom typing import Tuple, Dict, List, Union\n\nfrom ptls.nn.seq_encoder.abs_seq_encoder import AbsSeqEncoder\nfrom ptls.nn import PBL2Norm\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom ptls.custom_layers import StatPooling, GEGLU\nfrom ptls.nn.seq_step import LastStepEncoder\n\nclass Head(nn.Module):   \n    def __init__(self, input_size, n_classes, hidden_size=64, drop_p=0.1):\n        super().__init__()\n        self.head = nn.Sequential(\n            nn.Linear(input_size, hidden_size, bias=True),\n            nn.GELU(),\n            nn.Dropout(drop_p),\n            nn.Linear(hidden_size, n_classes)\n        )\n    def forward(self, x):\n        x = self.head(x)\n        return x\n\nclass GptPretrainModule(pl.LightningModule):\n    \"\"\"GPT2 Language model\n\n    Original sequence are encoded by `TrxEncoder`.\n    Model `seq_encoder` predicts embedding of next transaction.\n    Heads are used to predict each feature class of future transaction.\n\n    Parameters\n    ----------\n    trx_encoder:\n        Module for transform dict with feature sequences to sequence of transaction representations\n    seq_encoder:\n        Module for sequence processing. Generally this is transformer based encoder. Rnn is also possible\n        Should works without sequence reduce\n    head_hidden_size:\n        Hidden size of heads for feature prediction\n    seed_seq_len:\n         Size of starting sequence without loss \n    total_steps:\n        total_steps expected in OneCycle lr scheduler\n    max_lr:\n        max_lr of OneCycle lr scheduler\n    weight_decay:\n        weight_decay of Adam optimizer\n    pct_start:\n        % of total_steps when lr increase\n    norm_predict:\n        use l2 norm for transformer output or not\n    inference_pooling_strategy:\n        'out' - `seq_encoder` forward (`is_reduce_requence=True`) (B, H)\n        'out_stat' - min, max, mean, std statistics pooled from `seq_encoder` layer (B, H) -> (B, 4H)\n        'trx_stat' - min, max, mean, std statistics pooled from `trx_encoder` layer (B, H) -> (B, 4H)\n        'trx_stat_out' - min, max, mean, std statistics pooled from `trx_encoder` layer + 'out' from `seq_encoder` (B, H) -> (B, 5H)\n    \"\"\"\n\n    def __init__(self,\n                 trx_encoder: torch.nn.Module,\n                 seq_encoder: AbsSeqEncoder,\n                 head_hidden_size: int = 64,\n                 total_steps: int = 64000,\n                 seed_seq_len: int = 16,\n                 max_lr: float = 0.00005,\n                 weight_decay: float = 0.0,\n                 pct_start: float = 0.1,\n                 norm_predict: bool = False,\n                 inference_pooling_strategy: str = 'out_stat'\n                 ):\n\n        super().__init__()\n        self.save_hyperparameters(ignore=['trx_encoder', 'seq_encoder'])\n\n        self.trx_encoder = trx_encoder\n        # assert self.trx_encoder.embeddings, '`embeddings` parameter for `trx_encoder` should contain at least 1 feature!'\n\n        self._seq_encoder = seq_encoder\n        self._seq_encoder.is_reduce_sequence = False\n\n        self.head = nn.ModuleDict()\n        for col_name, noisy_emb in self.trx_encoder.embeddings.items():\n            self.head[col_name] = Head(input_size=self._seq_encoder.embedding_size, hidden_size=head_hidden_size, n_classes=noisy_emb.num_embeddings)\n\n        if self.hparams.norm_predict:\n            self.fn_norm_predict = PBL2Norm()\n\n        self.loss = nn.CrossEntropyLoss(ignore_index=0)\n\n        self.train_gpt_loss = MeanMetric()\n        self.valid_gpt_loss = MeanMetric()\n\n    def forward(self, batch: PaddedBatch):\n        # print(f\"{batch.payload['amount']}\")\n        z_trx = self.trx_encoder(batch) \n        out = self._seq_encoder(z_trx)\n        if self.hparams.norm_predict:\n            out = self.fn_norm_predict(out)\n        return out\n    \n    def loss_gpt(self, predictions, labels, is_train_step):\n        loss = 0\n        for col_name, head in self.head.items():\n            y_pred = head(predictions[:, self.hparams.seed_seq_len:-1, :])\n            y_pred = y_pred.view(-1, y_pred.size(-1))\n\n            y_true = labels[col_name][:, self.hparams.seed_seq_len+1:]\n            y_true = torch.flatten(y_true.long())\n\n            loss += self.loss(y_pred, y_true)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        out = self.forward(batch)  # PB: B, T, H\n        out = out.payload if isinstance(out, PaddedBatch) else out\n        labels = batch.payload\n\n        loss_gpt = self.loss_gpt(out, labels, is_train_step=True)\n        self.train_gpt_loss(loss_gpt)\n        self.log(f'gpt/loss', loss_gpt, sync_dist=True)\n        return loss_gpt\n\n    def validation_step(self, batch, batch_idx):\n        out = self.forward(batch)  # PB: B, T, H\n        out = out.payload if isinstance(out, PaddedBatch) else out\n        labels = batch.payload\n\n        loss_gpt = self.loss_gpt(out, labels, is_train_step=False)\n        self.valid_gpt_loss(loss_gpt)\n\n    def on_training_epoch_end(self):\n        self.log(f'gpt/train_gpt_loss', self.train_gpt_loss, prog_bar=False, sync_dist=True, rank_zero_only=True)\n        # self.train_gpt_loss reset not required here\n\n    def on_validation_epoch_end(self):\n        self.log(f'gpt/valid_gpt_loss', self.valid_gpt_loss, prog_bar=True, sync_dist=True, rank_zero_only=True)\n        # self.valid_gpt_loss reset not required here\n\n    def configure_optimizers(self):\n        optim = torch.optim.NAdam(self.parameters(),\n                                 lr=self.hparams.max_lr,\n                                 weight_decay=self.hparams.weight_decay,\n                                 )\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer=optim,\n            max_lr=self.hparams.max_lr,\n            total_steps=self.hparams.total_steps,\n            pct_start=self.hparams.pct_start,\n            anneal_strategy='cos',\n            cycle_momentum=False,\n            div_factor=25.0,\n            final_div_factor=10000.0,\n            three_phase=False,\n        )\n        scheduler = {'scheduler': scheduler, 'interval': 'step'}\n        return [optim], [scheduler]\n    \n    @property\n    def seq_encoder(self):\n        return GPTInferenceModule(pretrained_model=self)\n\nclass GPTInferenceModule(torch.nn.Module):\n    def __init__(self, pretrained_model):\n        super().__init__()\n        self.model = pretrained_model\n        self.model.is_reduce_sequence = False\n\n        self.stat_pooler = StatPooling()\n        self.last_step = LastStepEncoder()\n\n    def forward(self, batch):\n        z_trx = self.model.trx_encoder(batch)\n        out = self.model._seq_encoder(z_trx)\n        out = out if isinstance(out, PaddedBatch) else PaddedBatch(out, batch.seq_lens)\n        if self.model.hparams.inference_pooling_strategy=='trx_stat_out':\n            stats = self.stat_pooler(z_trx)\n            out = self.last_step(out)\n            out = torch.cat([stats, out], dim=1)\n        elif self.model.hparams.inference_pooling_strategy=='trx_stat':\n            out = self.stat_pooler(z_trx)\n        elif self.model.hparams.inference_pooling_strategy=='out_stat':\n            out = self.stat_pooler(out)\n        elif self.model.hparams.inference_pooling_strategy=='out':\n            out = self.last_step(out)\n        else:\n            raise\n        if self.model.hparams.norm_predict:\n            out = out / (out.pow(2).sum(dim=-1, keepdim=True) + 1e-9).pow(0.5)\n        return out\n\nclass CorrGptPretrainModule(GptPretrainModule):\n    def __init__(self, feature_encoder, seq_encoder, trx_encoder, *args, **kwargs):\n        trx_encoder.numeric_values = {}\n        super().__init__(trx_encoder=trx_encoder, seq_encoder=seq_encoder, *args, **kwargs)\n        self.save_hyperparameters(ignore=['trx_encoder', 'seq_encoder', 'feature_encoder'])\n        self.feature_encoder = feature_encoder\n\n    def forward(self, batch):\n        # print(f\"{batch.payload['event_time'].size()=}\")\n        z_trx = self.trx_encoder(batch)\n        # print(z_trx.payload.size())\n        payload = z_trx.payload.view(z_trx.payload.shape[:-1] + (-1, 24))\n        payload = self.feature_encoder(payload)\n        encoded_trx = PaddedBatch(payload=payload, length=z_trx.seq_lens)\n        out = self._seq_encoder(encoded_trx)\n        # print(out.payload.size())\n        if self.hparams.norm_predict:\n            out = self.fn_norm_predict(out)\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ptls.nn import TabFormerFeatureEncoder\nfrom ptls.nn import TransformerEncoder\nfrom ptls.nn import TrxEncoder\n\nfeature_encoder = TabFormerFeatureEncoder(\n    n_cols=9,\n    emb_dim=24\n)\n\nseq_encoder = TransformerEncoder(\n    n_heads=2,\n    n_layers=2,\n    input_size=216,\n    use_positional_encoding=True\n)\n\ntrx_encoder = TrxEncoder(\n            norm_embeddings=False,\n            embeddings_noise=0.0,\n            embeddings={\n                'event_type': {\"in\": 58, \"out\": 24},\n                'event_subtype': {\"in\": 59, \"out\": 24},\n                'src_type11': {\"in\": 85, \"out\": 24},\n                'src_type12': {\"in\": 349, \"out\": 24},\n                'dst_type11': {\"in\": 84, \"out\": 24},\n                'dst_type12': {\"in\": 417, \"out\": 24},\n                'src_type22': {\"in\": 90, \"out\": 24},\n                'src_type32': {\"in\": 91, \"out\": 24},\n                'amount': {\"in\": 11, \"out\": 24}\n            },\n            # numeric_values={'amount': 'log'}\n        )\n\npl_module = CorrGptPretrainModule(\n    # total_steps=20000,\n    feature_encoder=feature_encoder,\n    trx_encoder=trx_encoder,\n    seq_encoder=seq_encoder\n)\n# pl_module = GptPretrainModule(\n#     total_steps=20000,\n#     trx_encoder=trx_encoder,\n#     seq_encoder=seq_encoder\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightning.pytorch.loggers import WandbLogger\n\nwandb_logger = WandbLogger(project=\"MBD_My_Code\", log_model=\"all\")\n\ntrainer = pl.Trainer(\n    logger=wandb_logger,\n    max_epochs=20,\n    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    enable_progress_bar=True,\n    gradient_clip_val=0.5,\n    log_every_n_steps=50,\n    limit_val_batches=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.fit(pl_module, data_module)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_dataset = ptls.data_load.datasets.ParquetDataset(\n        data_files=[\n            os.path.join(TRX_SUPERVISED_PATH, 'fold=4')\n        ],\n    i_filters=[\n        ptls.data_load.iterable_processing.SeqLenFilter(min_seq_len=16),\n        ptls.data_load.iterable_processing.SeqLenFilter(max_seq_len=2048),\n        ptls.data_load.iterable_processing.feature_filter.FeatureFilter(\n            keep_feature_names=[\n                'client_id',\n                'target_1',\n                'target_2',\n                'target_3',\n                'target_4'\n            ]\n        ),\n        QuantilfyAmount(),\n        ptls.data_load.iterable_processing.CategorySizeClip(\n            category_max_size={\n                'event_type': 58,\n                'event_subtype' :59,\n                'src_type11': 85,\n                'src_type12': 349,\n                'dst_type11': 84,\n                'dst_type12': 417,\n                'src_type22': 90,\n                'src_type32': 91,\n                'amount': 10\n            }\n        ),\n        GetSplit(\n            start_month=1,\n            end_month=12,\n            col_id='client_id'\n        ),\n        ptls.data_load.iterable_processing.to_torch_tensor.ToTorch(),\n    ],\n    shuffle_files=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_dl = DataLoader(\n    dataset=inference_dataset,\n    shuffle=False,\n    num_workers=0,\n    batch_size=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport numpy as np\nfrom itertools import chain\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom datetime import datetime\nfrom ptls.custom_layers import StatPooling\nfrom ptls.nn.seq_step import LastStepEncoder\n\n\nclass InferenceModuleMultimodal(pl.LightningModule):\n    def __init__(\n        self,\n        model,\n        pandas_output=True,\n        col_id='client_id',\n        target_col_names=None,\n        model_out_name='emb',\n        model_type='notab'\n    ):\n        super().__init__()\n\n        self.model = model\n        self.pandas_output = pandas_output\n        self.target_col_names = target_col_names\n        self.col_id = col_id\n        self.model_out_name = model_out_name\n        self.model_type = model_type\n\n        self.stat_pooler = StatPooling()\n        self.last_step = LastStepEncoder()\n\n    def forward(self, x):\n        x_len = len(x)\n        if x_len == 3:\n            x, batch_ids, target_cols = x\n        else: \n            x, batch_ids = x\n        if 'seq_encoder' in dir(self.model):\n            out = self.model.seq_encoder(x)\n        else:\n            out = self.model(x)\n            \n        if x_len == 3:\n            target_cols = torch.Tensor(target_cols)\n            \n            x_out = {\n                self.col_id: batch_ids,\n                self.model_out_name: out\n            }\n            if len(target_cols.size()) > 1:\n                for idx, target_col in enumerate(self.target_col_names):\n                    x_out[target_col] = target_cols[:, idx]\n            else: \n                x_out[self.target_col_names[0]] = target_cols[::4]\n        else:\n            x_out = {\n                self.col_id: batch_ids,\n                self.model_out_name: out\n            }\n\n        if self.pandas_output:\n            return self.to_pandas(x_out)\n        return x_out\n\n    @staticmethod\n    def to_pandas(x):\n        expand_cols = []\n        scalar_features = {}\n\n        for k, v in x.items():\n            if type(v) is torch.Tensor:\n                v = v.cpu().numpy()\n\n            if type(v) is list or len(v.shape) == 1:\n                scalar_features[k] = v\n            elif len(v.shape) == 2:\n                expand_cols.append(k)\n            else:\n                scalar_features[k] = None\n        # print(scalar_features)\n        dataframes = [pd.DataFrame(scalar_features)]\n        for col in expand_cols:\n            v = x[col].cpu().numpy()\n            dataframes.append(pd.DataFrame(v, columns=[f'{col}_{i:04d}' for i in range(v.shape[1])]))\n\n        return pd.concat(dataframes, axis=1)\n\ndef collate_feature_dict_with_target(batch, col_id='client_id', target_col_names=None):\n    batch_ids = []\n    target_cols = []\n    for sample in batch:\n        batch_ids.append(sample[col_id])\n        del sample[col_id]\n        \n        if target_col_names is not None:\n            sample_targets = []\n            for target_col in target_col_names:\n                sample_targets.append(sample[target_col])\n                del sample[target_col]\n            target_cols.append(sample_targets)\n                \n            \n    padded_batch = collate_feature_dict(batch)\n    if target_col_names is not None:\n        return padded_batch, batch_ids, target_cols\n    return padded_batch, batch_ids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_col_names = [\n    'target_1',\n    'target_2',\n    'target_3',\n    'target_4'\n]\n\ncollate_fn = partial(\n    collate_feature_dict_with_target,\n    target_col_names=target_col_names\n)\n\ninference_dl = DataLoader(\n    dataset=inference_dataset,\n    collate_fn=collate_fn,\n    shuffle=False,\n    num_workers=0,\n    batch_size=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_module = InferenceModuleMultimodal(\n    model=pl_module,\n    pandas_output=True,\n    col_id='client_id',\n    target_col_names=target_col_names\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_embeddings = pd.concat(trainer.predict(inf_module, inference_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndwns_train, dwns_test = train_test_split(inf_embeddings, test_size=0.2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"targets_train = np.array(\n    [\n        dwns_train['target_1'].to_numpy(),\n        dwns_train['target_2'].to_numpy(),\n        dwns_train['target_3'].to_numpy(),\n        dwns_train['target_4'].to_numpy()\n    ]\n).T\ntargets_test = np.array(\n    [\n        dwns_test['target_1'].to_numpy(),\n        dwns_test['target_2'].to_numpy(),\n        dwns_test['target_3'].to_numpy(),\n        dwns_test['target_4'].to_numpy()\n    ]\n).T\n\ndwns_train = dwns_train.drop(columns=[\n    'client_id',\n    'target_1',\n    'target_2',\n    'target_3',\n    'target_4'\n]).to_numpy()\n\ndwns_test = dwns_test.drop(columns=[\n    'client_id',\n    'target_1',\n    'target_2',\n    'target_3',\n    'target_4'\n]).to_numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nmodels = [LGBMClassifier(\n    n_estimators=500,\n    boosting_type='gbdt',\n    subsample=0.5,\n    subsample_freq=1,\n    learning_rate=0.02,\n    feature_fraction=0.75,\n    max_depth=6,\n    lambda_l1=1,\n    lambda_l2=1,\n    min_data_in_leaf=50,\n    random_state=42,\n    n_jobs=8,\n    verbose=-1\n) for _ in range(4)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for target_id in range(4):\n    models[target_id].fit(dwns_train, targets_train[:, target_id])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\nfor i in range(len(models)):\n    preds = models[i].predict_proba(dwns_test)\n    print(f\"ROC-AUC target_{i} = {roc_auc_score(targets_test[:, i], preds[:, 1])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GPT Module with SWIN enc","metadata":{}},{"cell_type":"code","source":"from ptls.frames.gpt.gpt_dataset import GptIterableDataset\n\ndata_module = ptls.frames.PtlsDataModule(\n    train_data=GptIterableDataset(\n        data=train,\n        min_len=80,\n        max_len=300\n    ),\n    valid_data=GptIterableDataset(\n        data=valid,\n        min_len=80,\n        max_len=300\n    ),\n    train_batch_size=128,\n    train_num_workers=0,\n    valid_batch_size=64,\n    valid_num_workers=0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\na = torch.rand(64, 853, 216)\nB, L, C = a.shape\ntorch.arange(L)[None, :] + torch.ones((B, L))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nimport numpy as np\n\n\n#------------------------------------------------------------------------------------------------------------\n# Based on https://github.com/yukara-ikemiya/Swin-Transformer-1d/tree/main and adapted to pytorch-lifestream\n#------------------------------------------------------------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nfrom ptls.data_load.padded_batch import PaddedBatch\n\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n    # copied from timm/models/layers/drop.py\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:\n        random_tensor.div_(keep_prob)\n    return x * random_tensor\n\n\nclass DropPath(nn.Module):\n    # copied from timm/models/layers/drop.py\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None, scale_by_keep=True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, L, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, C)\n    \"\"\"\n    B, L, C = x.shape\n    x = x.view(B, L // window_size, window_size, C)\n    windows = x.contiguous().view(-1, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, L):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, C)\n        window_size (int): Window size\n        L (int): Length of data\n\n    Returns:\n        x: (B, L, C)\n    \"\"\"\n    B = int(windows.shape[0] / (L / window_size))\n    x = windows.view(B, L // window_size, window_size, -1)\n    x = x.contiguous().view(B, L, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (int): The width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n    def __init__(self, dim: int, window_size: int, num_heads: int,\n                 qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(2 * window_size - 1, num_heads))  # 2*window_size - 1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_w = torch.arange(self.window_size)\n        relative_coords = coords_w[:, None] - coords_w[None, :]  # W, W\n        relative_coords[:, :] += self.window_size - 1  # shift to start from 0\n\n        # relative_position_index | example\n        # [2, 1, 0]\n        # [3, 2, 1]\n        # [4, 3, 2]\n        self.register_buffer(\"relative_position_index\", relative_coords)  # (W, W): range of 0 -- 2*(W-1)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        torch.nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask_add, mask_mult):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, W, C)\n            mask: (0/-inf) mask with shape of (num_windows, W, W) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size, self.window_size, -1)  # W, W, nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, W, W\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        nW = mask_add.shape[1]\n        attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask_add\n        attn = attn.view(-1, self.num_heads, N, N)\n        attn = self.softmax(attn)\n        attn = attn * mask_mult\n        \n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        \n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=self.window_size, num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        attn_mask = None\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def forward(self, x):\n        seq_lens = x.seq_lens\n        x = x.payload\n        \n        B, L, C = x.shape\n\n        # define seq_len_mask\n        mask = torch.arange(L, device=x.device)[None, :] + torch.ones((B, L), device=x.device)\n        mask[mask > seq_lens[:, None]] = 0.\n        mask[mask > 0.] = 1.\n        mask = mask[:, :, None]\n\n        # make new max seq_len `L` divisible by `self.window_size` by adding 'zero' samples\n        num_samples_to_add = self.window_size - (L % self.window_size)\n        \n        if num_samples_to_add < self.window_size:\n            additional_samples = torch.zeros((B, num_samples_to_add, C), device=x.device)\n            x = torch.cat((x, additional_samples), dim=1)\n            mask_additional_samples = torch.zeros((B, num_samples_to_add, mask.shape[2]), device=mask.device)\n            mask = torch.cat((mask, mask_additional_samples), dim=1)\n            L += num_samples_to_add\n\n        # mask out padding transactions\n        x = x * mask\n        \n        assert L >= self.window_size, f'input length ({L}) must be >= window size ({self.window_size})'\n        assert L % self.window_size == 0, f'input length ({L}) must be divisible by window size ({self.window_size})'\n\n        shortcut = x\n        x = self.norm1(x)\n\n        # shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=-self.shift_size, dims=1) # cyclic shift as in orig. 2D SWIN-transformer\n            mask = torch.roll(mask, shifts=-self.shift_size, dims=1) # cyclic shift of the mask\n        else:\n            shifted_x = x\n        \n        # partition\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, C\n        mask = window_partition(mask, self.window_size) # nW*B, window_size, 1\n        \n        # calculate attn_mask\n        attn_mask = (mask @ mask.transpose(-2, -1)) # nW*B, window_size, window_size\n        \n        attn_mask_real = attn_mask.clone().detach()\n        attn_mask_real = attn_mask_real.view(attn_mask_real.shape[0], self.window_size, self.window_size).unsqueeze(1).expand(-1, self.num_heads, -1, -1) # B*nW, nH, window_size, window_size\n        \n        attn_mask[attn_mask == 0.] = -torch.inf\n        attn_mask[attn_mask == 1.] = 0.\n        attn_mask[:, torch.arange(attn_mask.shape[-1]), torch.arange(attn_mask.shape[-1])] = 0.\n        attn_mask = attn_mask.view(B, attn_mask.shape[0] // B, self.window_size, self.window_size).unsqueeze(2).expand(-1, -1, self.num_heads, -1, -1) # B, nW, nH, window_size, window_size\n        \n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask_add=attn_mask, mask_mult=attn_mask_real)  # nW*B, window_size, C\n        \n        # merge windows\n        shifted_x = window_reverse(attn_windows, self.window_size, L)  # (B, L, C)\n\n        # reverse zero-padding shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=self.shift_size, dims=1) # cyclic shift as in orig. 2D SWIN-transformer\n        else:\n            x = shifted_x\n\n        x = shortcut + self.drop_path(x)\n\n        # FFN\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        \n        return PaddedBatch(x, seq_lens)\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n\nclass SwinTransformerV2Layer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        pretrained_window_size (int): Local window size in pre-training.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        depth: int,\n        num_heads: int,\n        window_size: int,\n        mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n        drop_path=0., norm_layer=nn.LayerNorm, use_checkpoint=False,\n        pretrained_window_size=0\n    ):\n\n        super().__init__()\n        self.dim = dim\n        self.depth = depth\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer)\n            for i in range(depth)])\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, depth={self.depth}, num_heads={self.num_heads}, window_size={self.window_size}\"\n\n    @property\n    def embedding_size(self):\n        return self.dim\n    \n    def flops(self):\n        flops = 0\n        for blk in self.blocks:\n            flops += blk.flops()\n        return flops\n\n    def _init_respostnorm(self):\n        for blk in self.blocks:\n            nn.init.constant_(blk.norm1.bias, 0)\n            nn.init.constant_(blk.norm1.weight, 0)\n            nn.init.constant_(blk.norm2.bias, 0)\n            nn.init.constant_(blk.norm2.weight, 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\nclass GPTInferenceModule(torch.nn.Module):\n    def __init__(self, pretrained_model):\n        super().__init__()\n        self.model = pretrained_model\n        self.model.is_reduce_sequence = False\n\n        self.stat_pooler = StatPooling()\n        self.last_step = LastStepEncoder()\n\n    def forward(self, batch):\n        z_trx = self.model.trx_encoder(batch)\n        out = self.model._seq_encoder(z_trx)\n        out = out if isinstance(out, PaddedBatch) else PaddedBatch(out, batch.seq_lens)\n        if self.model.hparams.inference_pooling_strategy=='trx_stat_out':\n            stats = self.stat_pooler(z_trx)\n            out = self.last_step(out)\n            out = torch.cat([stats, out], dim=1)\n        elif self.model.hparams.inference_pooling_strategy=='trx_stat':\n            out = self.stat_pooler(z_trx)\n        elif self.model.hparams.inference_pooling_strategy=='out_stat':\n            out = self.stat_pooler(out)\n        elif self.model.hparams.inference_pooling_strategy=='out':\n            out = self.last_step(out)\n        else:\n            raise\n        if self.model.hparams.norm_predict:\n            out = out / (out.pow(2).sum(dim=-1, keepdim=True) + 1e-9).pow(0.5)\n        return out\n\nclass SWINPretrainModule(GptPretrainModule):\n    def __init__(self, feature_encoder, seq_encoder, trx_encoder, *args, **kwargs):\n        trx_encoder.numeric_values = {}\n        super().__init__(trx_encoder=trx_encoder, seq_encoder=seq_encoder, *args, **kwargs)\n        self.save_hyperparameters(ignore=['trx_encoder', 'seq_encoder', 'feature_encoder'])\n        self.feature_encoder = feature_encoder\n        self.window_size = self._seq_encoder.window_size\n\n    def forward(self, batch):\n        # print(f\"{batch.payload['event_time'].size()=}\")\n        \n        \n        z_trx = self.trx_encoder(batch)\n        # print(z_trx.seq_lens)\n        # print(f'{z_trx.payload.size()=}')\n        # print(z_trx.payload.size())\n        payload = z_trx.payload.view(z_trx.payload.shape[:-1] + (-1, 24))\n        payload = self.feature_encoder(payload)\n        # print(f\"{payload.size()=}\")\n        feature_embed = PaddedBatch(payload, z_trx.seq_lens)\n        # encoded_trx = PaddedBatch(payload=payload, length=z_trx.seq_lens)\n\n        # pad_size = math.ceil(feature_embed.size()[1] / self.window_size) * self.window_size - feature_embed.size()[1]\n        # feature_embed = F.pad(feature_embed, (0, 0, 0, pad_size, 0, 0), 'constant', 0)\n        \n        out = self._seq_encoder(feature_embed)\n        out = PaddedBatch(out.payload[:, :payload.shape[1], :], z_trx.seq_lens)\n        # print(f'{out.size()=}')\n        # print(f'{payload.size()=}')\n        # if pad_size > 0:\n        #     out = out[:, :-pad_size, :]\n        # out = PaddedBatch(out, z_trx.seq_lens)\n        # print(f\"{out.seq_lens=}\")\n        # print(f\"{out.payload.size()}\")\n        if self.hparams.norm_predict:\n            out = self.fn_norm_predict(out)\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ptls.nn import TabFormerFeatureEncoder\nfrom ptls.nn import TransformerEncoder\nfrom ptls.nn import TrxEncoder\n\nfeature_encoder = TabFormerFeatureEncoder(\n    n_cols=9,\n    emb_dim=24\n)\n\nseq_encoder = SwinTransformerV2Layer(\n    num_heads=4,\n    depth=4,\n    dim=216,\n    window_size=12\n)\n\ntrx_encoder = TrxEncoder(\n            norm_embeddings=False,\n            embeddings_noise=0.0,\n            embeddings={\n                'event_type': {\"in\": 58, \"out\": 24},\n                'event_subtype': {\"in\": 59, \"out\": 24},\n                'src_type11': {\"in\": 85, \"out\": 24},\n                'src_type12': {\"in\": 349, \"out\": 24},\n                'dst_type11': {\"in\": 84, \"out\": 24},\n                'dst_type12': {\"in\": 417, \"out\": 24},\n                'src_type22': {\"in\": 90, \"out\": 24},\n                'src_type32': {\"in\": 91, \"out\": 24},\n                'amount': {\"in\": 11, \"out\": 24}\n            },\n            # numeric_values={'amount': 'log'}\n        )\n\npl_module = SWINPretrainModule(\n    # total_steps=20000,\n    feature_encoder=feature_encoder,\n    trx_encoder=trx_encoder,\n    seq_encoder=seq_encoder\n)\n# pl_module = GptPretrainModule(\n#     total_steps=20000,\n#     trx_encoder=trx_encoder,\n#     seq_encoder=seq_encoder\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightning.pytorch.loggers import WandbLogger\n\nwandb_logger = WandbLogger(project=\"MBD_My_Code\", log_model=\"all\")\n\ntrainer = pl.Trainer(\n    logger=wandb_logger,\n    max_epochs=20,\n    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    enable_progress_bar=True,\n    gradient_clip_val=0.5,\n    log_every_n_steps=50,\n    limit_val_batches=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.fit(pl_module, data_module)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_dataset = ptls.data_load.datasets.ParquetDataset(\n        data_files=[\n            os.path.join(TRX_SUPERVISED_PATH, 'fold=4')\n        ],\n    i_filters=[\n        ptls.data_load.iterable_processing.SeqLenFilter(min_seq_len=16),\n        ptls.data_load.iterable_processing.SeqLenFilter(max_seq_len=2048),\n        ptls.data_load.iterable_processing.feature_filter.FeatureFilter(\n            keep_feature_names=[\n                'client_id',\n                'target_1',\n                'target_2',\n                'target_3',\n                'target_4'\n            ]\n        ),\n        QuantilfyAmount(),\n        ptls.data_load.iterable_processing.CategorySizeClip(\n            category_max_size={\n                'event_type': 58,\n                'event_subtype' :59,\n                'src_type11': 85,\n                'src_type12': 349,\n                'dst_type11': 84,\n                'dst_type12': 417,\n                'src_type22': 90,\n                'src_type32': 91,\n                'amount': 10\n            }\n        ),\n        GetSplit(\n            start_month=1,\n            end_month=12,\n            col_id='client_id'\n        ),\n        ptls.data_load.iterable_processing.to_torch_tensor.ToTorch(),\n    ],\n    shuffle_files=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_dl = DataLoader(\n    dataset=inference_dataset,\n    shuffle=False,\n    num_workers=0,\n    batch_size=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_col_names = [\n    'target_1',\n    'target_2',\n    'target_3',\n    'target_4'\n]\n\ncollate_fn = partial(\n    collate_feature_dict_with_target,\n    target_col_names=target_col_names\n)\n\ninference_dl = DataLoader(\n    dataset=inference_dataset,\n    collate_fn=collate_fn,\n    shuffle=False,\n    num_workers=0,\n    batch_size=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport numpy as np\nfrom itertools import chain\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom datetime import datetime\nfrom ptls.custom_layers import StatPooling\nfrom ptls.nn.seq_step import LastStepEncoder\n\n\nclass InferenceModuleMultimodal(pl.LightningModule):\n    def __init__(\n        self,\n        model,\n        pandas_output=False,\n        col_id='client_id',\n        target_col_names=None,\n        model_out_name='emb',\n        model_type='notab'\n    ):\n        super().__init__()\n\n        self.model = model\n        self.pandas_output = pandas_output\n        self.target_col_names = target_col_names\n        self.col_id = col_id\n        self.model_out_name = model_out_name\n        self.model_type = model_type\n\n        self.stat_pooler = StatPooling()\n        self.last_step = LastStepEncoder()\n\n    def forward(self, x):\n        x_len = len(x)\n        if x_len == 3:\n            x, batch_ids, target_cols = x\n        else: \n            x, batch_ids = x\n        # if 'seq_encoder' in dir(self.model):\n        #     out = self.model.seq_encoder(x)\n        # else:\n        out = self.model(x).payload[:, 0, :]\n            \n        if x_len == 3:\n            target_cols = torch.tensor(target_cols)\n            x_out = {\n                self.col_id: batch_ids,\n                self.model_out_name: out\n            }\n            if len(target_cols.size()) > 1:\n                for idx, target_col in enumerate(self.target_col_names):\n                    x_out[target_col] = target_cols[:, idx]\n            else: \n                x_out[self.target_col_names[0]] = target_cols[::4]\n        else:\n            x_out = {\n                self.col_id: batch_ids,\n                self.model_out_name: out\n            }\n\n        if self.pandas_output:\n            return self.to_pandas(x_out)\n        return x_out\n\n    @staticmethod\n    def to_pandas(x):\n        expand_cols = []\n        scalar_features = {}\n\n        for k, v in x.items():\n            if type(v) is torch.Tensor:\n                v = v.cpu().numpy()\n\n            if type(v) is list or len(v.shape) == 1:\n                scalar_features[k] = v\n            elif len(v.shape) == 2:\n                expand_cols.append(k)\n            else:\n                scalar_features[k] = None\n\n        dataframes = [pd.DataFrame(scalar_features)]\n        for col in expand_cols:\n            v = x[col].cpu().numpy()\n            dataframes.append(pd.DataFrame(v, columns=[f'{col}_{i:04d}' for i in range(v.shape[1])]))\n\n        return pd.concat(dataframes, axis=1)\n\ndef collate_feature_dict_with_target(batch, col_id='client_id', target_col_names=None):\n    batch_ids = []\n    target_cols = []\n    for sample in batch:\n        batch_ids.append(sample[col_id])\n        del sample[col_id]\n        \n        if target_col_names is not None:\n            sample_targets = []\n            for target_col in target_col_names:\n                sample_targets.append(sample[target_col])\n                del sample[target_col]\n            target_cols.append(sample_targets)\n                \n            \n    padded_batch = collate_feature_dict(batch)\n    if target_col_names is not None:\n        return padded_batch, batch_ids, target_cols\n    return padded_batch, batch_ids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_module = InferenceModuleMultimodal(\n    model=pl_module,\n    pandas_output=True,\n    col_id='client_id',\n    target_col_names=target_col_names\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_embeddings = pd.concat(trainer.predict(inf_module, inference_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_embeddings[:3]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndwns_train, dwns_test = train_test_split(inf_embeddings, test_size=0.2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"targets_train = np.array(\n    [\n        dwns_train['target_1'].to_numpy(),\n        dwns_train['target_2'].to_numpy(),\n        dwns_train['target_3'].to_numpy(),\n        dwns_train['target_4'].to_numpy()\n    ]\n).T\ntargets_test = np.array(\n    [\n        dwns_test['target_1'].to_numpy(),\n        dwns_test['target_2'].to_numpy(),\n        dwns_test['target_3'].to_numpy(),\n        dwns_test['target_4'].to_numpy()\n    ]\n).T\n\ndwns_train = dwns_train.drop(columns=[\n    'client_id',\n    'target_1',\n    'target_2',\n    'target_3',\n    'target_4'\n]).to_numpy()\n\ndwns_test = dwns_test.drop(columns=[\n    'client_id',\n    'target_1',\n    'target_2',\n    'target_3',\n    'target_4'\n]).to_numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nmodels = [LGBMClassifier(\n    n_estimators=500,\n    boosting_type='gbdt',\n    subsample=0.5,\n    subsample_freq=1,\n    learning_rate=0.02,\n    feature_fraction=0.75,\n    max_depth=6,\n    lambda_l1=1,\n    lambda_l2=1,\n    min_data_in_leaf=50,\n    random_state=42,\n    n_jobs=8,\n    verbose=-1\n) for _ in range(4)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for target_id in range(4):\n    models[target_id].fit(dwns_train, targets_train[:, target_id])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\nfor i in range(len(models)):\n    preds = models[i].predict_proba(dwns_test)\n    print(f\"ROC-AUC target_{i} = {roc_auc_score(targets_test[:, i], preds[:, 1])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GPT with unitregular","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nimport numpy as np\n\n\n#------------------------------------------------------------------------------------------------------------\n# Based on https://github.com/yukara-ikemiya/Swin-Transformer-1d/tree/main and adapted to pytorch-lifestream\n#------------------------------------------------------------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nfrom ptls.data_load.padded_batch import PaddedBatch\n\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n    # copied from timm/models/layers/drop.py\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:\n        random_tensor.div_(keep_prob)\n    return x * random_tensor\n\n\nclass DropPath(nn.Module):\n    # copied from timm/models/layers/drop.py\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None, scale_by_keep=True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, L, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, C)\n    \"\"\"\n    B, L, C = x.shape\n    x = x.view(B, L // window_size, window_size, C)\n    windows = x.contiguous().view(-1, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, L):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, C)\n        window_size (int): Window size\n        L (int): Length of data\n\n    Returns:\n        x: (B, L, C)\n    \"\"\"\n    B = int(windows.shape[0] / (L / window_size))\n    x = windows.view(B, L // window_size, window_size, -1)\n    x = x.contiguous().view(B, L, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (int): The width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n    def __init__(self, dim: int, window_size: int, num_heads: int,\n                 qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(2 * window_size - 1, num_heads))  # 2*window_size - 1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_w = torch.arange(self.window_size)\n        relative_coords = coords_w[:, None] - coords_w[None, :]  # W, W\n        relative_coords[:, :] += self.window_size - 1  # shift to start from 0\n\n        # relative_position_index | example\n        # [2, 1, 0]\n        # [3, 2, 1]\n        # [4, 3, 2]\n        self.register_buffer(\"relative_position_index\", relative_coords)  # (W, W): range of 0 -- 2*(W-1)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        torch.nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask_add, mask_mult):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, W, C)\n            mask: (0/-inf) mask with shape of (num_windows, W, W) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size, self.window_size, -1)  # W, W, nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, W, W\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        nW = mask_add.shape[1]\n        attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask_add\n        attn = attn.view(-1, self.num_heads, N, N)\n        attn = self.softmax(attn)\n        attn = attn * mask_mult\n        \n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        \n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=self.window_size, num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        attn_mask = None\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def forward(self, x):\n        seq_lens = x.seq_lens\n        x = x.payload\n        \n        B, L, C = x.shape\n\n        # define seq_len_mask\n        mask = torch.arange(L, device=x.device)[None, :] + torch.ones((B, L), device=x.device)\n        mask[mask > seq_lens[:, None]] = 0.\n        mask[mask > 0.] = 1.\n        mask = mask[:, :, None]\n\n        # make new max seq_len `L` divisible by `self.window_size` by adding 'zero' samples\n        num_samples_to_add = self.window_size - (L % self.window_size)\n        \n        if num_samples_to_add < self.window_size:\n            additional_samples = torch.zeros((B, num_samples_to_add, C), device=x.device)\n            x = torch.cat((x, additional_samples), dim=1)\n            mask_additional_samples = torch.zeros((B, num_samples_to_add, mask.shape[2]), device=mask.device)\n            mask = torch.cat((mask, mask_additional_samples), dim=1)\n            L += num_samples_to_add\n\n        # mask out padding transactions\n        x = x * mask\n        \n        assert L >= self.window_size, f'input length ({L}) must be >= window size ({self.window_size})'\n        assert L % self.window_size == 0, f'input length ({L}) must be divisible by window size ({self.window_size})'\n\n        shortcut = x\n        x = self.norm1(x)\n\n        # shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=-self.shift_size, dims=1) # cyclic shift as in orig. 2D SWIN-transformer\n            mask = torch.roll(mask, shifts=-self.shift_size, dims=1) # cyclic shift of the mask\n        else:\n            shifted_x = x\n        \n        # partition\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, C\n        mask = window_partition(mask, self.window_size) # nW*B, window_size, 1\n        \n        # calculate attn_mask\n        attn_mask = (mask @ mask.transpose(-2, -1)) # nW*B, window_size, window_size\n        \n        attn_mask_real = attn_mask.clone().detach()\n        attn_mask_real = attn_mask_real.view(attn_mask_real.shape[0], self.window_size, self.window_size).unsqueeze(1).expand(-1, self.num_heads, -1, -1) # B*nW, nH, window_size, window_size\n        \n        attn_mask[attn_mask == 0.] = -torch.inf\n        attn_mask[attn_mask == 1.] = 0.\n        attn_mask[:, torch.arange(attn_mask.shape[-1]), torch.arange(attn_mask.shape[-1])] = 0.\n        attn_mask = attn_mask.view(B, attn_mask.shape[0] // B, self.window_size, self.window_size).unsqueeze(2).expand(-1, -1, self.num_heads, -1, -1) # B, nW, nH, window_size, window_size\n        \n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask_add=attn_mask, mask_mult=attn_mask_real)  # nW*B, window_size, C\n        \n        # merge windows\n        shifted_x = window_reverse(attn_windows, self.window_size, L)  # (B, L, C)\n\n        # reverse zero-padding shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=self.shift_size, dims=1) # cyclic shift as in orig. 2D SWIN-transformer\n        else:\n            x = shifted_x\n\n        x = shortcut + self.drop_path(x)\n\n        # FFN\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        \n        return PaddedBatch(x, seq_lens)\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n\nclass SwinTransformerV2Layer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        pretrained_window_size (int): Local window size in pre-training.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        depth: int,\n        num_heads: int,\n        window_size: int,\n        mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n        drop_path=0., norm_layer=nn.LayerNorm, use_checkpoint=False,\n        pretrained_window_size=0\n    ):\n\n        super().__init__()\n        self.dim = dim\n        self.depth = depth\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer)\n            for i in range(depth)])\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, depth={self.depth}, num_heads={self.num_heads}, window_size={self.window_size}\"\n\n    @property\n    def embedding_size(self):\n        return self.dim\n    \n    def flops(self):\n        flops = 0\n        for blk in self.blocks:\n            flops += blk.flops()\n        return flops\n\n    def _init_respostnorm(self):\n        for blk in self.blocks:\n            nn.init.constant_(blk.norm1.bias, 0)\n            nn.init.constant_(blk.norm1.weight, 0)\n            nn.init.constant_(blk.norm2.bias, 0)\n            nn.init.constant_(blk.norm2.weight, 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\nclass GPTInferenceModule(torch.nn.Module):\n    def __init__(self, pretrained_model):\n        super().__init__()\n        self.model = pretrained_model\n        self.model.is_reduce_sequence = False\n\n        self.stat_pooler = StatPooling()\n        self.last_step = LastStepEncoder()\n\n    def forward(self, batch):\n        z_trx = self.model.trx_encoder(batch)\n        out = self.model._seq_encoder(z_trx)\n        out = out if isinstance(out, PaddedBatch) else PaddedBatch(out, batch.seq_lens)\n        if self.model.hparams.inference_pooling_strategy=='trx_stat_out':\n            stats = self.stat_pooler(z_trx)\n            out = self.last_step(out)\n            out = torch.cat([stats, out], dim=1)\n        elif self.model.hparams.inference_pooling_strategy=='trx_stat':\n            out = self.stat_pooler(z_trx)\n        elif self.model.hparams.inference_pooling_strategy=='out_stat':\n            out = self.stat_pooler(out)\n        elif self.model.hparams.inference_pooling_strategy=='out':\n            out = self.last_step(out)\n        else:\n            raise\n        if self.model.hparams.norm_predict:\n            out = out / (out.pow(2).sum(dim=-1, keepdim=True) + 1e-9).pow(0.5)\n        return out\n\nclass SWINPretrainModule(GptPretrainModule):\n    def __init__(self, feature_encoder, seq_encoder, trx_encoder, *args, **kwargs):\n        trx_encoder.numeric_values = {}\n        super().__init__(trx_encoder=trx_encoder, seq_encoder=seq_encoder, *args, **kwargs)\n        self.save_hyperparameters(ignore=['trx_encoder', 'seq_encoder', 'feature_encoder'])\n        self.feature_encoder = feature_encoder\n        self.window_size = self._seq_encoder.window_size\n\n    def forward(self, batch):\n        # print(f\"{batch.payload['event_time'].size()=}\")\n        \n        \n        z_trx = self.trx_encoder(batch)\n        # print(z_trx.seq_lens)\n        # print(f'{z_trx.payload.size()=}')\n        # print(z_trx.payload.size())\n        payload = z_trx.payload.view(z_trx.payload.shape[:-1] + (-1, 24))\n        payload = self.feature_encoder(payload)\n        # print(f\"{payload.size()=}\")\n        feature_embed = PaddedBatch(payload, z_trx.seq_lens)\n        # encoded_trx = PaddedBatch(payload=payload, length=z_trx.seq_lens)\n\n        # pad_size = math.ceil(feature_embed.size()[1] / self.window_size) * self.window_size - feature_embed.size()[1]\n        # feature_embed = F.pad(feature_embed, (0, 0, 0, pad_size, 0, 0), 'constant', 0)\n        \n        out = self._seq_encoder(feature_embed)\n        out = PaddedBatch(out.payload[:, :payload.shape[1], :], z_trx.seq_lens)\n        # print(f'{out.size()=}')\n        # print(f'{payload.size()=}')\n        # if pad_size > 0:\n        #     out = out[:, :-pad_size, :]\n        # out = PaddedBatch(out, z_trx.seq_lens)\n        # print(f\"{out.seq_lens=}\")\n        # print(f\"{out.payload.size()}\")\n        if self.hparams.norm_predict:\n            out = self.fn_norm_predict(out)\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CoLES with SWIN enc","metadata":{}},{"cell_type":"code","source":"data_module = ptls.frames.PtlsDataModule(\n    train_data=ptls.frames.coles.ColesIterableDataset(\n        splitter=ptls.frames.coles.split_strategy.SampleSlices(\n            split_count=5,\n            cnt_min=16,\n            cnt_max=400\n        ),\n        data=train\n    ),\n    valid_data=ptls.frames.coles.ColesIterableDataset(\n        splitter=ptls.frames.coles.split_strategy.SampleSlices(\n            split_count=5,\n            cnt_min=16,\n            cnt_max=400\n        ),\n        data=valid\n    ),\n    train_batch_size=64,\n    train_num_workers=0,\n    valid_batch_size=32,\n    valid_num_workers=0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nimport numpy as np\n\n\n#------------------------------------------------------------------------------------------------------------\n# Based on https://github.com/yukara-ikemiya/Swin-Transformer-1d/tree/main and adapted to pytorch-lifestream\n#------------------------------------------------------------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nfrom ptls.data_load.padded_batch import PaddedBatch\n\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n    # copied from timm/models/layers/drop.py\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:\n        random_tensor.div_(keep_prob)\n    return x * random_tensor\n\n\nclass DropPath(nn.Module):\n    # copied from timm/models/layers/drop.py\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None, scale_by_keep=True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, L, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, C)\n    \"\"\"\n    B, L, C = x.shape\n    x = x.view(B, L // window_size, window_size, C)\n    windows = x.contiguous().view(-1, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, L):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, C)\n        window_size (int): Window size\n        L (int): Length of data\n\n    Returns:\n        x: (B, L, C)\n    \"\"\"\n    B = int(windows.shape[0] / (L / window_size))\n    x = windows.view(B, L // window_size, window_size, -1)\n    x = x.contiguous().view(B, L, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (int): The width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n    def __init__(self, dim: int, window_size: int, num_heads: int,\n                 qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(2 * window_size - 1, num_heads))  # 2*window_size - 1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_w = torch.arange(self.window_size)\n        relative_coords = coords_w[:, None] - coords_w[None, :]  # W, W\n        relative_coords[:, :] += self.window_size - 1  # shift to start from 0\n\n        # relative_position_index | example\n        # [2, 1, 0]\n        # [3, 2, 1]\n        # [4, 3, 2]\n        self.register_buffer(\"relative_position_index\", relative_coords)  # (W, W): range of 0 -- 2*(W-1)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        torch.nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask_add, mask_mult):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, W, C)\n            mask: (0/-inf) mask with shape of (num_windows, W, W) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size, self.window_size, -1)  # W, W, nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, W, W\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        nW = mask_add.shape[1]\n        attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask_add\n        attn = attn.view(-1, self.num_heads, N, N)\n        attn = self.softmax(attn)\n        attn = attn * mask_mult\n        \n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        \n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=self.window_size, num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        attn_mask = None\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n    def forward(self, x):\n        seq_lens = x.seq_lens\n        x = x.payload\n        \n        B, L, C = x.shape\n\n        # define seq_len_mask\n        mask = torch.arange(L, device=x.device)[None, :] + torch.ones((B, L), device=x.device)\n        mask[mask > seq_lens[:, None]] = 0.\n        mask[mask > 0.] = 1.\n        mask = mask[:, :, None]\n\n        # make new max seq_len `L` divisible by `self.window_size` by adding 'zero' samples\n        num_samples_to_add = self.window_size - (L % self.window_size)\n        \n        if num_samples_to_add < self.window_size:\n            additional_samples = torch.zeros((B, num_samples_to_add, C), device=x.device)\n            x = torch.cat((x, additional_samples), dim=1)\n            mask_additional_samples = torch.zeros((B, num_samples_to_add, mask.shape[2]), device=mask.device)\n            mask = torch.cat((mask, mask_additional_samples), dim=1)\n            L += num_samples_to_add\n\n        # mask out padding transactions\n        x = x * mask\n        \n        assert L >= self.window_size, f'input length ({L}) must be >= window size ({self.window_size})'\n        assert L % self.window_size == 0, f'input length ({L}) must be divisible by window size ({self.window_size})'\n\n        shortcut = x\n        x = self.norm1(x)\n\n        # shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=-self.shift_size, dims=1) # cyclic shift as in orig. 2D SWIN-transformer\n            mask = torch.roll(mask, shifts=-self.shift_size, dims=1) # cyclic shift of the mask\n        else:\n            shifted_x = x\n        \n        # partition\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, C\n        mask = window_partition(mask, self.window_size) # nW*B, window_size, 1\n        \n        # calculate attn_mask\n        attn_mask = (mask @ mask.transpose(-2, -1)) # nW*B, window_size, window_size\n        \n        attn_mask_real = attn_mask.clone().detach()\n        attn_mask_real = attn_mask_real.view(attn_mask_real.shape[0], self.window_size, self.window_size).unsqueeze(1).expand(-1, self.num_heads, -1, -1) # B*nW, nH, window_size, window_size\n        \n        attn_mask[attn_mask == 0.] = -torch.inf\n        attn_mask[attn_mask == 1.] = 0.\n        attn_mask[:, torch.arange(attn_mask.shape[-1]), torch.arange(attn_mask.shape[-1])] = 0.\n        attn_mask = attn_mask.view(B, attn_mask.shape[0] // B, self.window_size, self.window_size).unsqueeze(2).expand(-1, -1, self.num_heads, -1, -1) # B, nW, nH, window_size, window_size\n        \n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask_add=attn_mask, mask_mult=attn_mask_real)  # nW*B, window_size, C\n        \n        # merge windows\n        shifted_x = window_reverse(attn_windows, self.window_size, L)  # (B, L, C)\n\n        # reverse zero-padding shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=self.shift_size, dims=1) # cyclic shift as in orig. 2D SWIN-transformer\n        else:\n            x = shifted_x\n\n        x = shortcut + self.drop_path(x)\n\n        # FFN\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        \n        return PaddedBatch(x, seq_lens)\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n\nclass SwinTransformerV2Layer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        pretrained_window_size (int): Local window size in pre-training.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        depth: int,\n        num_heads: int,\n        window_size: int,\n        mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n        drop_path=0., norm_layer=nn.LayerNorm, use_checkpoint=False,\n        pretrained_window_size=0\n    ):\n\n        super().__init__()\n        self.dim = dim\n        self.depth = depth\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer)\n            for i in range(depth)])\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, depth={self.depth}, num_heads={self.num_heads}, window_size={self.window_size}\"\n\n    @property\n    def embedding_size(self):\n        return self.dim\n    \n    def flops(self):\n        flops = 0\n        for blk in self.blocks:\n            flops += blk.flops()\n        return flops\n\n    def _init_respostnorm(self):\n        for blk in self.blocks:\n            nn.init.constant_(blk.norm1.bias, 0)\n            nn.init.constant_(blk.norm1.weight, 0)\n            nn.init.constant_(blk.norm2.bias, 0)\n            nn.init.constant_(blk.norm2.weight, 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\n\nimport torch\n\n# from ptls.constant_repository import TORCH_EMB_DTYPE\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom ptls.nn.trx_encoder.batch_norm import RBatchNorm, RBatchNormWithLens\nfrom ptls.nn.trx_encoder.noisy_embedding import NoisyEmbedding\nfrom ptls.nn.trx_encoder.trx_encoder_base import TrxEncoderBase\n\nTORCH_EMB_DTYPE = torch.float32\n\nclass TrxEncoderSWIN(TrxEncoderBase):\n    \"\"\"Network layer which makes representation for single transactions\n\n     Input is `PaddedBatch` with ptls-format dictionary, with feature arrays of shape (B, T)\n     Output is `PaddedBatch` with transaction embeddings of shape (B, T, H)\n     where:\n        B - batch size, sequence count in batch\n        T - sequence length\n        H - hidden size, representation dimension\n\n    `ptls.nn.trx_encoder.noisy_embedding.NoisyEmbedding` implementation are used for categorical features.\n\n    Parameters\n        embeddings:\n            dict with categorical feature names.\n            Values must be like this `{'in': dictionary_size, 'out': embedding_size}`\n            These features will be encoded with lookup embedding table of shape (dictionary_size, embedding_size)\n            Values can be a `torch.nn.Embedding` implementation\n        numeric_values:\n            dict with numerical feature names.\n            Values must be a string with scaler_name.\n            Possible values are: 'identity', 'sigmoid', 'log', 'year'.\n            These features will be scaled with selected scaler.\n            Values can be `ptls.nn.trx_encoder.scalers.BaseScaler` implementatoin\n\n            One field can have many scalers. In this case key become alias and col name should be in scaler.\n            Check `TrxEncoderBase.numeric_values` for more details\n\n        embeddings_noise (float):\n            Noise level for embedding. `0` meens without noise\n        emb_dropout (float):\n            Probability of an element of embedding to be zeroed\n        spatial_dropout (bool):\n            Whether to dropout full dimension of embedding in the whole sequence\n\n        use_batch_norm:\n            True - All numerical values will be normalized after scaling\n            False - No normalizing for numerical values\n        use_batch_norm_with_lens:\n            True - Respect seq_lens during batch_norm. Padding zeroes will be ignored\n            False - Batch norm ower all time axis. Padding zeroes will included.\n\n        orthogonal_init:\n            if True then `torch.nn.init.orthogonal` applied\n        linear_projection_size:\n            Linear layer at the end will be added for non-zero value\n\n        out_of_index:\n            How to process a categorical indexes which are greater than dictionary size.\n            'clip' - values will be collapsed to maximum index. This works well for frequency encoded categories.\n                We join infrequent categories to one.\n            'assert' - raise an error of invalid index appear.\n\n        norm_embeddings: keep default value for this parameter\n        clip_replace_value: Not useed. keep default value for this parameter\n        positions: Not used. Keep default value for this parameter\n\n    Examples:\n        >>> B, T = 5, 20\n        >>> trx_encoder = TrxEncoder(\n        >>>     embeddings={'mcc_code': {'in': 100, 'out': 5}},\n        >>>     numeric_values={'amount': 'log'},\n        >>> )\n        >>> x = PaddedBatch(\n        >>>     payload={\n        >>>         'mcc_code': torch.randint(0, 99, (B, T)),\n        >>>         'amount': torch.randn(B, T),\n        >>>     },\n        >>>     length=torch.randint(10, 20, (B,)),\n        >>> )\n        >>> z = trx_encoder(x)\n        >>> assert z.payload.shape == (5, 20, 6)  # B, T, H\n    \"\"\"\n    def __init__(self,\n                 embeddings=None,\n                 numeric_values=None,\n                 custom_embeddings=None,\n                 embeddings_noise: float = 0,\n                 norm_embeddings=None,\n                 use_batch_norm=False,\n                 use_batch_norm_with_lens=False,\n                 clip_replace_value=None,\n                 positions=None,\n                 emb_dropout=0,\n                 spatial_dropout=False,\n                 orthogonal_init=False,\n                 linear_projection_size=0,\n                 out_of_index: str = 'clip',\n                 ):\n        if clip_replace_value is not None:\n            warnings.warn('`clip_replace_value` attribute is deprecated. Always \"clip to max\" used. '\n                          'Use `out_of_index=\"assert\"` to avoid categorical values clip', DeprecationWarning)\n\n        if positions is not None:\n            warnings.warn('`positions` is deprecated. positions is not used', UserWarning)\n\n        if embeddings is None:\n            embeddings = {}\n        if custom_embeddings is None:\n            custom_embeddings = {}\n\n        noisy_embeddings = {}\n        for emb_name, emb_props in embeddings.items():\n            if emb_props.get('disabled', False):\n                continue\n            if emb_props['in'] == 0 or emb_props['out'] == 0:\n                continue\n            noisy_embeddings[emb_name] = NoisyEmbedding(\n                num_embeddings=emb_props['in'],\n                embedding_dim=emb_props['out'],\n                padding_idx=0,\n                max_norm=1 if norm_embeddings else None,\n                noise_scale=embeddings_noise,\n                dropout=emb_dropout,\n                spatial_dropout=spatial_dropout,\n            )\n\n        super().__init__(\n            embeddings=noisy_embeddings,\n            numeric_values=numeric_values,\n            custom_embeddings=custom_embeddings,\n            out_of_index=out_of_index,\n        )\n        self.swin_encoder = SwinTransformerV2Layer(\n            num_heads=5,\n            depth=3,\n            dim=225,\n            window_size=15\n        )\n        custom_embedding_size = self.custom_embedding_size\n        if use_batch_norm and custom_embedding_size > 0:\n            # :TODO: Should we use Batch norm with not-numerical custom embeddings?\n            if use_batch_norm_with_lens:\n                self.custom_embedding_batch_norm = RBatchNormWithLens(custom_embedding_size)\n            else:\n                self.custom_embedding_batch_norm = RBatchNorm(custom_embedding_size)\n        else:\n            self.custom_embedding_batch_norm = None\n\n        if linear_projection_size > 0:\n            self.linear_projection_head = torch.nn.Linear(super().output_size, linear_projection_size)\n        else:\n            self.linear_projection_head = None\n\n        if orthogonal_init:\n            for n, p in self.named_parameters():\n                if n.startswith('embeddings.') and n.endswith('.weight'):\n                    torch.nn.init.orthogonal_(p.data[1:])\n                if n == 'linear_projection_head.weight':\n                    torch.nn.init.orthogonal_(p.data)\n\n    def forward(self, x: PaddedBatch, names=None, seq_len=None):\n        if isinstance(x, PaddedBatch) is False:\n            pre_x = dict()\n            for i, field_name in enumerate(names):\n                pre_x[field_name] = x[i]\n            x = PaddedBatch(pre_x, seq_len)\n\n        processed_embeddings = [self.get_category_embeddings(x, field_name)\n                                for field_name in self.embeddings.keys()]\n        processed_custom_embeddings = [self.get_custom_embeddings(x, field_name)\n                                       for field_name in self.custom_embeddings.keys()]\n        if len(processed_custom_embeddings):\n            processed_custom_embeddings = torch.cat(processed_custom_embeddings, dim=2)\n            if self.custom_embedding_batch_norm is not None:\n                processed_custom_embeddings = PaddedBatch(processed_custom_embeddings, x.seq_lens)\n                processed_custom_embeddings = self.custom_embedding_batch_norm(processed_custom_embeddings)\n                processed_custom_embeddings = processed_custom_embeddings.payload\n            processed_embeddings.append(processed_custom_embeddings)\n\n        out = torch.cat(processed_embeddings, dim=2)\n        out = out.type(TORCH_EMB_DTYPE)\n        out = self.linear_projection_head(out) if self.linear_projection_head is not None else out\n        # print(out.size())\n        out = PaddedBatch(out, x.seq_lens)\n        out = self.swin_encoder(out)\n        return out\n\n    @property\n    def output_size(self):\n        \"\"\"Returns hidden size of output representation\n        \"\"\"\n        if self.linear_projection_head is not None:\n            return self.linear_projection_head.out_features\n        return super().output_size\n\nfrom ptls.nn.seq_encoder.rnn_encoder import RnnEncoder\nfrom ptls.nn.seq_encoder.containers import SeqEncoderContainer\n\n\nclass SwinTransformerBackbone(nn.Module):\n    \"\"\" Swin Transformer Backbone (4 stages as in orig. 2D impl.).\n\n    Args:\n        dim (int): Number of input channels.\n        depths (list[int]): Numbers of blocks in stages.\n        num_heads (int): Number of attention heads in W-MSA layers.\n        start_window_size (int): Local window size of stage 1.\n        window_size_mult (int): the number by which the `window_size` is being multiplied when moving to another stage\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        depths: list[int],\n        num_heads: int,\n        start_window_size: int,\n        window_size_mult: int = 1,\n        mlp_ratio=4.,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.,\n        attn_drop=0.,\n        drop_path=0.,\n        norm_layer=nn.LayerNorm\n    ):\n        super().__init__()\n        self.dim = dim\n        self.depths = depths\n        self.num_heads = num_heads\n        self.window_sizes = [start_window_size]\n        \n        for i in range(len(self.depths) - 1):\n            self.window_sizes += [self.window_sizes[-1] * window_size_mult]\n\n        # build model\n        self.backbone = nn.ModuleList([\n            SwinTransformerLayer(dim=self.dim,\n                                 depth=self.depths[i],\n                                 num_heads=self.num_heads,\n                                 window_size=self.window_sizes[i],\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias,\n                                 qk_scale=qk_scale,\n                                 drop=drop,\n                                 attn_drop=attn_drop,\n                                 drop_path=drop_path,\n                                 norm_layer=norm_layer)\n            for i in range(len(self.depths))])\n\n    def forward(self, x):\n        for layer in self.backbone:\n            x = layer(x)\n        return x\n\nclass SWIN_RNN_SeqEncoder(SeqEncoderContainer):\n    \"\"\"SeqEncoderContainer with SWIN transformer layer for features hierarchic fusion and RnnEncoder for feature aggregation.\n    \n    Parameters\n        trx_encoder:\n            TrxEncoder object\n        input_size:\n            input_size parameter for RnnEncoder\n            If None: input_size = trx_encoder.output_size\n            Set input_size explicitly or use None if your trx_encoder object has output_size attribute\n        is_reduce_sequence:\n            False - returns PaddedBatch with all transactions embeddings\n            True - returns one embedding for sequence based on CLS token\n        swin_depths: Numbers of blocks in stages (SWIN backbone).\n        swin_num_heads: Number of attention heads in W-MSA layers (SWIN backbone).\n        swin_start_window_size (int): Local window size of stage 1 (SWIN backbone).\n        swin_window_size_mult (int): the number by which the `window_size` is being multiplied when moving to another stage (SWIN backbone).\n        swin_drop: Dropout rate (SWIN backbone). Default: 0.0\n        swin_attn_drop: Attention dropout rate (SWIN backbone). Default: 0.0\n        swin_drop_path: Stochastic depth rate (SWIN backbone). Default: 0.0\n        **rnn_seq_encoder_params:\n            RnnEncoder params\n\n    \"\"\"\n    def __init__(self,\n                 trx_encoder=None,\n                 input_size=None,\n                 is_reduce_sequence=True,\n                 swin_depths=[],\n                 swin_num_heads=4,\n                 swin_start_window_size=4,\n                 swin_window_size_mult=1,\n                 swin_drop=0.,\n                 swin_attn_drop=0.,\n                 swin_drop_path=0.,\n                 **rnn_seq_encoder_params\n                 ):\n        super().__init__(\n            trx_encoder=trx_encoder,\n            seq_encoder_cls=RnnEncoder,\n            input_size=input_size,\n            seq_encoder_params=rnn_seq_encoder_params,\n            is_reduce_sequence=is_reduce_sequence,\n        )\n        self.swin_fusion = SwinTransformerBackbone(\n                               dim=trx_encoder.output_size,\n                               depths=swin_depths,\n                               num_heads=swin_num_heads,\n                               start_window_size=swin_start_window_size,\n                               window_size_mult=swin_window_size_mult,\n                               drop=swin_drop,\n                               attn_drop=swin_attn_drop,\n                               drop_path=swin_drop_path\n                              )\n\n    def forward(self, x, names=None, seq_len=None, h_0=None):\n        x = self.trx_encoder(x)\n        x = self.swin_fusion(x)\n        x = self.seq_encoder(x, h_0)\n        return x\n\n# class SWINTrxEncoder(nn.Module):\n#     def __init__(self, trx_encoder):\n#         super().__init__()\n#         self.trx_encoder = trx_encoder\n#         self.swin_encoder = nn.ModuleList([SwinTransformerV2Layer(\n#             num_heads=4,\n#             depth=4,\n#             dim=216,\n#             window_size=12 // i\n#         ) for i in range (1, 4)])\n\n#     def forward(self, x):\n#         x = self.trx_encoder(x)\n#         for layer in self.swin_encoder:\n#             x = layer(x)\n#         return x\n\n#     @property\n#     def output_size(self):\n#         return self.trx_encoder.output_size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer_partial = partial(\n    torch.optim.AdamW,\n    lr=1e-3,\n    weight_decay=1e-4\n)\n\nlr_scheduler_partial = partial(\n    torch.optim.lr_scheduler.StepLR,\n    step_size=2,\n    gamma=0.9025\n)\n\ntrx_encoder = ptls.nn.TrxEncoder(\n                norm_embeddings=False,\n                embeddings_noise=0.003,\n                embeddings={\n                    'event_type': {\"in\": 58, \"out\": 28},\n                    'event_subtype': {\"in\": 59, \"out\": 28},\n                    'src_type11': {\"in\": 85, \"out\": 28},\n                    'src_type12': {\"in\": 349, \"out\": 28},\n                    'dst_type11': {\"in\": 84, \"out\": 28},\n                    'dst_type12': {\"in\": 417, \"out\": 28},\n                    'src_type22': {\"in\": 90, \"out\": 28},\n                    'src_type32': {\"in\": 91, \"out\": 28},\n                },\n                numeric_values={\n                    'amount': 'log'\n                }\n            )\nseq_encoder = ptls.nn.RnnSeqEncoder(\n    trx_encoder=trx_encoder,\n    type='gru',\n    hidden_size=256\n)\n\npl_module = ptls.frames.coles.CoLESModule(\n    # validation_metric=ptls.frames.coles.metric.BatchRecallTopK(\n    #     K=4,\n    #     metric='cosine'\n    # ),\n    seq_encoder=seq_encoder,\n    optimizer_partial=optimizer_partial,\n    lr_scheduler_partial=lr_scheduler_partial\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightning.pytorch.loggers import WandbLogger\n\nwandb_logger = WandbLogger(project=\"MBD_My_Code\", log_model=\"all\")\n\ntrainer = pl.Trainer(\n    logger=wandb_logger,\n    max_epochs=20,\n    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    enable_progress_bar=True,\n    gradient_clip_val=0.3,\n    log_every_n_steps=50,\n    limit_val_batches=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.fit(pl_module, data_module)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ptls.data_load.iterable_processing.iterable_seq_len_limit import ISeqLenLimit\nfrom ptls.data_load.iterable_processing.feature_filter import FeatureFilter\nfrom ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\n\ninference_dataset = ptls.data_load.datasets.ParquetDataset(\n    data_files=[os.path.join(TRX_SUPERVISED_PATH, 'fold=4')],\n    i_filters=[\n        ISeqLenLimit(max_seq_len=4096),\n        ToTorch(),\n        FeatureFilter(\n            keep_feature_names=[\n                'client_id',\n                'target_1',\n                'target_2',\n                'target_3',\n                'target_4'\n            ]\n        ),\n        GetSplit(\n            start_month=1,\n            end_month=12,\n            col_id='client_id'\n        )\n    ]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_dl = DataLoader(\n    dataset=inference_dataset,\n    shuffle=False,\n    num_workers=0,\n    batch_size=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport numpy as np\nfrom itertools import chain\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom datetime import datetime\nfrom ptls.custom_layers import StatPooling\nfrom ptls.nn.seq_step import LastStepEncoder\n\n\nclass InferenceModuleMultimodal(pl.LightningModule):\n    def __init__(\n        self,\n        model,\n        pandas_output=True,\n        col_id='client_id',\n        target_col_names=None,\n        model_out_name='emb',\n        model_type='notab'\n    ):\n        super().__init__()\n\n        self.model = model\n        self.pandas_output = pandas_output\n        self.target_col_names = target_col_names\n        self.col_id = col_id\n        self.model_out_name = model_out_name\n        self.model_type = model_type\n\n        self.stat_pooler = StatPooling()\n        self.last_step = LastStepEncoder()\n\n        self.batch_cntr = 0\n\n    def forward(self, x):\n        x_len = len(x)\n        if x_len == 3:\n            x, batch_ids, target_cols = x\n        else: \n            x, batch_ids = x\n        if 'seq_encoder' in dir(self.model):\n            out = self.model.seq_encoder(x)\n        else:\n            out = self.model(x)\n        if x_len == 3:\n            target_cols = torch.tensor(target_cols)\n            x_out = {\n                self.col_id: batch_ids,\n                self.model_out_name: out\n            }\n            if len(target_cols.size()) > 1:\n                for idx, target_col in enumerate(self.target_col_names):\n                    x_out[target_col] = target_cols[:, idx]\n            else: \n                x_out[self.target_col_names[0]] = target_cols[::4]\n        else:\n            x_out = {\n                self.col_id: batch_ids,\n                self.model_out_name: out\n            }\n\n        if self.pandas_output:\n            return self.to_pandas(x_out)\n        return x_out\n\n    @staticmethod\n    def to_pandas(x):\n        expand_cols = []\n        scalar_features = {}\n\n        for k, v in x.items():\n            if type(v) is torch.Tensor:\n                v = v.cpu().numpy()\n\n            if type(v) is list or len(v.shape) == 1:\n                scalar_features[k] = v\n            elif len(v.shape) == 2:\n                expand_cols.append(k)\n            else:\n                scalar_features[k] = None\n\n        dataframes = [pd.DataFrame(scalar_features)]\n        for col in expand_cols:\n            v = x[col].cpu().numpy()\n            dataframes.append(pd.DataFrame(v, columns=[f'{col}_{i:04d}' for i in range(v.shape[1])]))\n\n        return pd.concat(dataframes, axis=1)\n\ndef collate_feature_dict_with_target(batch, col_id='client_id', target_col_names=None):\n    batch_ids = []\n    target_cols = []\n    for sample in batch:\n        batch_ids.append(sample[col_id])\n        del sample[col_id]\n        \n        if target_col_names is not None:\n            sample_targets = []\n            for target_col in target_col_names:\n                sample_targets.append(sample[target_col])\n                del sample[target_col]\n            target_cols.append(sample_targets)\n                \n            \n    padded_batch = collate_feature_dict(batch)\n    if target_col_names is not None:\n        return padded_batch, batch_ids, target_cols\n    return padded_batch, batch_ids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Subset\n\ntarget_col_names = [\n    'target_1',\n    'target_2',\n    'target_3',\n    'target_4'\n]\n\ncollate_fn = partial(\n    collate_feature_dict_with_target,\n    target_col_names=target_col_names\n)\n\nindices = range(3000)\n\ninference_dl = DataLoader(\n    # dataset=Subset(inference_dataset, indices),\n    dataset=inference_dataset,\n    collate_fn=collate_fn,\n    shuffle=False,\n    num_workers=0,\n    batch_size=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_module = InferenceModuleMultimodal(\n    model=pl_module,\n    pandas_output=True,\n    col_id='client_id',\n    target_col_names=target_col_names\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_embeddings = pd.concat(trainer.predict(inf_module, inference_dl))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndwns_train, dwns_test = train_test_split(inf_embeddings, test_size=0.2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"targets_train = np.array(\n    [\n        dwns_train['target_1'].to_numpy(),\n        dwns_train['target_2'].to_numpy(),\n        dwns_train['target_3'].to_numpy(),\n        dwns_train['target_4'].to_numpy()\n    ]\n).T\ntargets_test = np.array(\n    [\n        dwns_test['target_1'].to_numpy(),\n        dwns_test['target_2'].to_numpy(),\n        dwns_test['target_3'].to_numpy(),\n        dwns_test['target_4'].to_numpy()\n    ]\n).T\n\ndwns_train = dwns_train.drop(columns=[\n    'client_id',\n    'target_1',\n    'target_2',\n    'target_3',\n    'target_4'\n]).to_numpy()\n\ndwns_test = dwns_test.drop(columns=[\n    'client_id',\n    'target_1',\n    'target_2',\n    'target_3',\n    'target_4'\n]).to_numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nmodels = [LGBMClassifier(\n    n_estimators=500,\n    boosting_type='gbdt',\n    subsample=0.5,\n    subsample_freq=1,\n    learning_rate=0.02,\n    feature_fraction=0.75,\n    max_depth=6,\n    lambda_l1=1,\n    lambda_l2=1,\n    min_data_in_leaf=50,\n    random_state=42,\n    n_jobs=8,\n    verbose=-1\n) for _ in range(4)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for target_id in range(4):\n    models[target_id].fit(dwns_train, targets_train[:, target_id])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\nfor i in range(len(models)):\n    preds = models[i].predict_proba(dwns_test)\n    print(f\"ROC-AUC target_{i} = {roc_auc_score(targets_test[:, i], preds[:, 1])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Патчи(до лучгих времен)","metadata":{}},{"cell_type":"markdown","source":"**Check data distribution from week**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nfor idx, item in enumerate(iter(train)):\n    if idx == 30:\n        break\n    dates = pd.to_datetime(item['event_time'].numpy(), unit='s')\n    \n    days_of_week = dates.dayofweek.tolist()\n    \n    days_counts = pd.Series(days_of_week).value_counts().sort_index()\n    \n    days_counts = days_counts.reindex(range(7), fill_value=0)\n    \n    weekday_names = [\n        'Понедельник', 'Вторник', 'Среда',\n        'Четверг', 'Пятница', 'Суббота', 'Воскресенье'\n    ]\n    \n    plt.figure(figsize=(10, 6))\n    plt.bar(weekday_names, days_counts, color='skyblue')\n    plt.title('Распределение меток по дням недели', fontsize=14)\n    plt.xlabel('День недели', fontsize=12)\n    plt.ylabel('Количество меток', fontsize=12)\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.show()\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a = np.arange(5)\na = np.append(a, 7)\na","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a = np.arange(15)\nprint(len(a))\nsplit_ids = np.arange(0, 15, 5)\nsplit_ids = np.append(split_ids, 16)\n[a[split_ids[i]:split_ids[i + 1]] for i in range(len(split_ids) - 1)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from functools import reduce\nfrom operator import iadd\nfrom ptls.data_load.utils import collate_feature_dict\nimport joblib\nfrom joblib import Parallel, delayed\nfrom ptls.data_load.feature_dict import FeatureDict\n\n\nclass PatchesSplitter:\n    def __init__(self, patch_size):\n        self.patch_size = patch_size\n\n    def split(self, dates):\n        date_len = dates.shape[0]\n        date_range = np.arange(date_len)\n\n        split_ids = np.arange(0, date_len, self.patch_size)\n        if len(split_ids) == 0 or split_ids[-1] < date_len:\n           split_ids = np.append(split_ids, date_len)\n        return [date_range[split_ids[i]:split_ids[i + 1]] for i in range(len(split_ids) - 1)]\n\nclass PatchedDataset(FeatureDict, torch.utils.data.Dataset):\n    def __init__(self,\n                 data,\n                 splitter,\n                 col_time='event_time',\n                 n_jobs=1):\n        self.data = data\n        self.col_time = col_time\n        self.splitter = splitter\n        self.n_jobs = n_jobs\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx: int):\n        feature_arrays = self.data[idx]\n        return self.get_splits(feature_arrays)\n\n    def __iter__(self):\n        for feature_arrays in self.data:\n            yield feature_arrays, self.get_splits(feature_arrays)\n\n    def _create_split_subset(self, idx, feature_arrays):\n        return {k: v[idx] for k, v in feature_arrays.items() if self.is_seq_feature(k, v)}\n    \n    def get_splits(self, feature_arrays):\n        local_date = feature_arrays[self.col_time]\n        indexes = self.splitter.split(local_date)\n        with joblib.parallel_backend(backend='threading', n_jobs=self.n_jobs):\n            parallel = Parallel()\n            result_dict = parallel(delayed(self._create_split_subset)(idx, feature_arrays) for idx in indexes)\n\n        return result_dict\n\n    @staticmethod\n    def collate_fn(batch):\n        full, patched = [item[0] for item in batch], [item[1] for item in batch]\n        \n        patched_class_labels = torch.LongTensor(reduce(iadd, list(map(lambda x: [x[0] for _ in x[1]], enumerate(patched)))))\n        patched_padded_batch = collate_feature_dict(reduce(iadd, patched))\n\n        for i in range(len(full)):\n            full[i] = [full[i]]\n        full_class_labels = torch.LongTensor(reduce(iadd, list(map(lambda x: [x[0] for _ in x[1]], enumerate(full)))))\n        full_padded_batch = collate_feature_dict(reduce(iadd, full))\n        return full_padded_batch, full_class_labels, patched_padded_batch, patched_class_labels\n\nclass PatchedIterableDataset(PatchedDataset, torch.utils.data.IterableDataset):\n    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_module = ptls.frames.PtlsDataModule(\n    train_data=PatchedIterableDataset(\n        splitter=PatchesSplitter(\n            patch_size=80\n        ),\n        data=train\n    ),\n    valid_data=PatchedIterableDataset(\n        splitter=PatchesSplitter(\n            patch_size=80\n        ),\n        data=valid\n    ),\n    train_batch_size=3,\n    train_num_workers=0,\n    valid_batch_size=32,\n    valid_num_workers=0\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"next(iter(data_module.train_dl(train_data=PatchedIterableDataset(\n        splitter=PatchesSplitter(\n            patch_size=12\n        ),\n        data=train\n    ))))[3].size()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nfrom ptls.frames.coles.losses import ContrastiveLoss\nfrom ptls.frames.coles.metric import BatchRecallTopK\nfrom ptls.frames.coles.sampling_strategies import HardNegativePairSelector\nfrom ptls.nn.head import Head\n\nclass PBLinear(torch.nn.Linear):\n    def forward(self, x: PaddedBatch):\n        return PaddedBatch(super().forward(x.payload), x.seq_lens)\n\n\n# class RSA(nn.Module):\n#     def __init__(self, dim, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n#         super().__init__()\n#         self.dim = dim\n#         self.num_heads = num_heads\n#         head_dim = dim // num_heads\n#         self.scale = qk_scale or head_dim ** -0.5\n\n#         self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n#         self.attn_drop = nn.Dropout(attn_drop)\n#         self.proj = nn.Linear(dim, dim)\n#         self.proj_drop = nn.Dropout(proj_drop)\n\n#         self.softmax = nn.Softmax(dim=-1)\n        \n#     def forward(self, x):\n#         B_, N, C = x.shape\n#         qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n#         q, k, v = qkv[0], qkv[1], qkv[2]\n\n#         q = q * self.scale\n#         attn = (q @ k.transpose(-2, -1))\n\n#         attn = self.softmax(attn)\n\n#         attn = self.attn_drop(attn)\n\n#         x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n#         x = self.proj(x)\n#         x = self.proj_drop(x)\n#         return x\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self,\n                 d_model,\n                 use_start_random_shift=True,\n                 max_len=5000,\n                 ):\n        super().__init__()\n        self.use_start_random_shift = use_start_random_shift\n        self.max_len = max_len\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        T = x.size(1)\n        if self.training and self.use_start_random_shift:\n            start_pos = random.randint(0, self.max_len - T)\n        else:\n            start_pos = 0\n        x = x + self.pe[:, start_pos:start_pos + T]\n        return x\n\nclass RegionalModule(pl.LightningModule):\n    def __init__(self,\n                num_layers,\n                trx_encoder,\n                head=None,\n                loss=None,\n                validation_metric=None,\n                optimizer_partial=None,\n                lr_scheduler_partial=None):\n        super().__init__()\n        self.save_hyperparameters()\n        dim = 256\n        self.trx_encoder = trx_encoder\n        self.num_layers = num_layers\n\n        self.optimizer_partial = optimizer_partial\n        self.lr_scheduler_partial = lr_scheduler_partial\n        \n        self.pb_linear_full = PBLinear(trx_encoder.output_size, 256)\n        self.pb_linear_patched = PBLinear(trx_encoder.output_size, 256)\n\n        self.enc_layer_patched = torch.nn.TransformerEncoderLayer(\n            d_model=256,\n            nhead=4,\n            dim_feedforward=256,\n            dropout=0.2,\n            batch_first=True\n        )\n\n        self.enc_layer_full = torch.nn.TransformerEncoderLayer(\n            d_model=256,\n            nhead=4,\n            dim_feedforward=256,\n            dropout=0.2,\n            batch_first=True\n        )\n        \n        # self.rsa = RSA(\n            \n        # )\n        self.pe = PositionalEncoding(\n                use_start_random_shift=True,\n                max_len=5000,\n                d_model=256,\n            )\n\n        self.cross_attn = nn.MultiheadAttention(\n            embed_dim=256,\n            num_heads=8,\n            dropout=0.1,\n            batch_first=True\n        )\n        \n        self.enc_norm = nn.LayerNorm(256)\n        self.loss = ContrastiveLoss(margin=0.5,\n                                   sampling_strategy=HardNegativePairSelector(neg_count=3))\n        self.validation_metric = BatchRecallTopK(K=4, metric='cosine')\n        self.head = Head(True)\n\n    def forward(self, batch):\n        x_full, full_labels, x_patches, patches_labels = batch\n\n        print(x_patches.payload['event_time'].size())\n        \n        patches_embeds = self.trx_encoder(x_patches)\n        patches_embeds = self.pb_linear_patched(patches_embeds)\n        patches_embeds = self.pe(patches_embeds.payload)\n\n        \n\n        full_embeds = self.trx_encoder(x_full)\n        full_embeds = self.pb_linear_full(full_embeds)\n        full_embeds = self.pe(full_embeds.payload)\n        full_embeds = full_embeds[patches_labels]\n\n        print(f\"{full_embeds.size()=}\")\n\n        # print(full_embeds.size())\n        # print(patches_embeds.size())\n        # print(full_embeds[patches_labels].size())\n\n        for _ in range(self.num_layers):\n            patches_embeds = self.enc_layer_patched(patches_embeds)\n            patches_embeds = self.enc_norm(patches_embeds)\n            \n            full_embeds = self.enc_layer_full(full_embeds)\n            full_embeds = self.enc_norm(full_embeds)\n\n        print(f\"{patches_embeds.size()=}\")\n        print(f\"{full_embeds.size()=}\")\n\n        out, _ = self.cross_attn(full_embeds, patches_embeds, patches_embeds)\n        print(out.size())\n        counts = torch.bincount(patches_labels)\n        splits = torch.split(patches_embeds, counts.tolist(), dim=0)\n        splits = [s.reshape(1, -1, 256) for s in splits]\n        outs = []\n        for i in range(len(splits)):\n            out, _ = self.cross_attn(full_embeds[i].reshape(1, -1, 256), splits[i], splits[i])\n            outs.append(out.mean(dim=1)[0])\n        outs = torch.stack(outs)\n\n        print(outs.size())\n\n        return out\n\n    def shared_step(self, batch):\n        y_h = self.head(self(batch)) if self.head is not None else self(batch)\n        y = batch[-1]\n        print(f\"{y.size()=}\")\n        return y_h, y\n    \n    def training_step(self, batch, _):\n        _, _, y_h, y = self.shared_step(batch)\n        loss = self.loss(y_h, y)\n        self.log('loss', loss)\n        return loss\n\n    def validation_step(self, batch, _):\n        y_h, y = self.shared_step(batch)\n        print(y_h.size(), y.size())\n        self.validation_metric(y_h, y)\n\n    def on_validation_epoch_end(self):\n        self.log(f'valid/{self.metric_name}', self.validation_metric.compute(), prog_bar=True)\n        self.validation_metric.reset()\n\n    def configure_optimizers(self):\n        optimizer = self.optimizer_partial(self.parameters())\n        scheduler = self.lr_scheduler_partial(optimizer)\n        \n        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            scheduler = {\n                'scheduler': scheduler,\n                'monitor': self.metric_name,\n            }\n        return [optimizer], [scheduler]\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nimport random\n\ntrx_encoder = ptls.nn.TrxEncoder(\n            norm_embeddings=False,\n            embeddings_noise=0.003,\n            embeddings={\n                'event_type': {\"in\": 58, \"out\": 24},\n                'event_subtype': {\"in\": 59, \"out\": 24},\n                'src_type11': {\"in\": 85, \"out\": 24},\n                'src_type12': {\"in\": 349, \"out\": 24},\n                'dst_type11': {\"in\": 84, \"out\": 24},\n                'dst_type12': {\"in\": 417, \"out\": 24},\n                'src_type22': {\"in\": 90, \"out\": 24},\n                'src_type32': {\"in\": 91, \"out\": 24}\n            },\n            numeric_values={\n                'amount': 'log'\n            }\n        )\n\noptimizer_partial = partial(\n    torch.optim.AdamW,\n    lr=0.001,\n    weight_decay=1e-4\n)\n\nlr_scheduler_partial = partial(\n    torch.optim.lr_scheduler.StepLR,\n    step_size=1,\n    gamma=0.9\n)\n\nmodule = RegionalModule(\n    3,\n    trx_encoder=trx_encoder,\n    optimizer_partial=optimizer_partial,\n    lr_scheduler_partial=lr_scheduler_partial\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# batch = next(iter(data_module.train_dl(train_data=PatchedIterableDataset(\n#         splitter=PatchesSplitter(\n#             patch_size=12\n#         ),\n#         data=train\n#     ))))\n\n# module.forward(batch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightning.pytorch.loggers import WandbLogger\n\nwandb_logger = WandbLogger(project=\"MBD_My_Code\", log_model=\"all\")\n\ntrainer = pl.Trainer(\n    logger=wandb_logger,\n    max_epochs=50,\n    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    enable_progress_bar=True,\n    gradient_clip_val=0.5,\n    log_every_n_steps=50,\n    limit_val_batches=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.fit(module, data_module)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results\n\n**GPT Baseline (RPE)**\n\nROC-AUC target_0 = 0.721350441667294\n\nROC-AUC target_1 = 0.8153566994158479\n\nROC-AUC target_2 = 0.7543318213719505\n\nROC-AUC target_3 = 0.7678734355960962\n\n**GPT + SWIN encoder (mean agg)**\n\nROC-AUC target_0 = 0.6516282035967174\n\nROC-AUC target_1 = 0.6540494938132733\n\nROC-AUC target_2 = 0.5948775519632066\n\nROC-AUC target_3 = 0.745110173028166\n\n**CoLES + SWIN + mean**\n\nROC-AUC target_0 = 0.7036866820699578\n\nROC-AUC target_1 = 0.6407223242752476\n\nROC-AUC target_2 = 0.6655641307690068\n\nROC-AUC target_3 = 0.7021158878362178\n\n\n**CoLES + SWIN + RNN**\n\n\nROC-AUC target_0 = 0.6988762517846345\n\nROC-AUC target_1 = 0.8023436653648343\n\nROC-AUC target_2 = 0.6582610542989475\n\nROC-AUC target_3 = 0.7862126496756947","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}