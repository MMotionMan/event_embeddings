{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Dzhambo/MBD.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:51:52.411064Z","iopub.execute_input":"2025-01-16T23:51:52.411641Z","iopub.status.idle":"2025-01-16T23:51:54.141732Z","shell.execute_reply.started":"2025-01-16T23:51:52.411594Z","shell.execute_reply":"2025-01-16T23:51:54.140630Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'MBD'...\nremote: Enumerating objects: 390, done.\u001b[K\nremote: Counting objects: 100% (390/390), done.\u001b[K\nremote: Compressing objects: 100% (196/196), done.\u001b[K\nremote: Total 390 (delta 232), reused 313 (delta 178), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (390/390), 143.82 KiB | 6.25 MiB/s, done.\nResolving deltas: 100% (232/232), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Install dependencies**","metadata":{}},{"cell_type":"code","source":"!pip install -r /kaggle/working/MBD/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:51:54.143501Z","iopub.execute_input":"2025-01-16T23:51:54.143817Z","iopub.status.idle":"2025-01-16T23:52:54.095393Z","shell.execute_reply.started":"2025-01-16T23:51:54.143789Z","shell.execute_reply":"2025-01-16T23:52:54.094554Z"}},"outputs":[{"name":"stdout","text":"Collecting pytorch-lifestream==0.6.0 (from -r /kaggle/working/MBD/requirements.txt (line 1))\n  Downloading pytorch-lifestream-0.6.0.tar.gz (163 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.4/163.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tensorboard>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/MBD/requirements.txt (line 2)) (2.16.2)\nCollecting pyspark==3.3.3 (from -r /kaggle/working/MBD/requirements.txt (line 3))\n  Downloading pyspark-3.3.3.tar.gz (281.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting hydra-core==1.2.0 (from -r /kaggle/working/MBD/requirements.txt (line 4))\n  Downloading hydra_core-1.2.0-py3-none-any.whl.metadata (4.0 kB)\nCollecting pytorch_lightning==2.3.3 (from -r /kaggle/working/MBD/requirements.txt (line 5))\n  Downloading pytorch_lightning-2.3.3-py3-none-any.whl.metadata (21 kB)\nCollecting lightgbm==3.3.2 (from -r /kaggle/working/MBD/requirements.txt (line 6))\n  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl.metadata (15 kB)\nCollecting xgboost==1.6.2 (from -r /kaggle/working/MBD/requirements.txt (line 7))\n  Downloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/MBD/requirements.txt (line 8)) (3.7.5)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/MBD/requirements.txt (line 9)) (0.12.2)\nCollecting duckdb (from pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1))\n  Downloading duckdb-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (762 bytes)\nRequirement already satisfied: numpy>=1.21.5 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (1.26.4)\nCollecting omegaconf (from pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1))\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: pandas>=1.3.5 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (2.2.3)\nRequirement already satisfied: pyarrow>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (17.0.0)\nRequirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (1.2.2)\nRequirement already satisfied: torch>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (2.4.0)\nRequirement already satisfied: torchmetrics>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (1.6.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (4.46.3)\nCollecting py4j==0.10.9.5 (from pyspark==3.3.3->-r /kaggle/working/MBD/requirements.txt (line 3))\n  Downloading py4j-0.10.9.5-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting antlr4-python3-runtime==4.9.* (from hydra-core==1.2.0->-r /kaggle/working/MBD/requirements.txt (line 4))\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from hydra-core==1.2.0->-r /kaggle/working/MBD/requirements.txt (line 4)) (21.3)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning==2.3.3->-r /kaggle/working/MBD/requirements.txt (line 5)) (4.66.4)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning==2.3.3->-r /kaggle/working/MBD/requirements.txt (line 5)) (6.0.2)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r /kaggle/working/MBD/requirements.txt (line 5)) (2024.6.0)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning==2.3.3->-r /kaggle/working/MBD/requirements.txt (line 5)) (4.12.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning==2.3.3->-r /kaggle/working/MBD/requirements.txt (line 5)) (0.11.9)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.2->-r /kaggle/working/MBD/requirements.txt (line 6)) (0.43.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.2->-r /kaggle/working/MBD/requirements.txt (line 6)) (1.14.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.1.0->-r /kaggle/working/MBD/requirements.txt (line 2)) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.1.0->-r /kaggle/working/MBD/requirements.txt (line 2)) (1.62.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.1.0->-r /kaggle/working/MBD/requirements.txt (line 2)) (3.6)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.1.0->-r /kaggle/working/MBD/requirements.txt (line 2)) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.1.0->-r /kaggle/working/MBD/requirements.txt (line 2)) (70.0.0)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.1.0->-r /kaggle/working/MBD/requirements.txt (line 2)) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.1.0->-r /kaggle/working/MBD/requirements.txt (line 2)) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.1.0->-r /kaggle/working/MBD/requirements.txt (line 2)) (3.1.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r /kaggle/working/MBD/requirements.txt (line 8)) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r /kaggle/working/MBD/requirements.txt (line 8)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r /kaggle/working/MBD/requirements.txt (line 8)) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r /kaggle/working/MBD/requirements.txt (line 8)) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r /kaggle/working/MBD/requirements.txt (line 8)) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r /kaggle/working/MBD/requirements.txt (line 8)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r /kaggle/working/MBD/requirements.txt (line 8)) (2.9.0.post0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r /kaggle/working/MBD/requirements.txt (line 5)) (3.9.5)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (2024.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.2->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.2->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (3.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.1.0->-r /kaggle/working/MBD/requirements.txt (line 2)) (2.1.5)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (0.26.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (0.4.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r /kaggle/working/MBD/requirements.txt (line 5)) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r /kaggle/working/MBD/requirements.txt (line 5)) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r /kaggle/working/MBD/requirements.txt (line 5)) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r /kaggle/working/MBD/requirements.txt (line 5)) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r /kaggle/working/MBD/requirements.txt (line 5)) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning==2.3.3->-r /kaggle/working/MBD/requirements.txt (line 5)) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12.0->pytorch-lifestream==0.6.0->-r /kaggle/working/MBD/requirements.txt (line 1)) (1.3.0)\nDownloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.1/151.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytorch_lightning-2.3.3-py3-none-any.whl (812 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl (255.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.9/255.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading duckdb-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pytorch-lifestream, pyspark, antlr4-python3-runtime\n  Building wheel for pytorch-lifestream (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorch-lifestream: filename=pytorch_lifestream-0.6.0-py3-none-any.whl size=274640 sha256=26556d970cf37c78d1cdde06a8e683b609e3fd75b71823b731914fc08977c7a2\n  Stored in directory: /root/.cache/pip/wheels/90/76/b4/0a944bc7c5a69201e4d757cc54886971117a2a581740e7f11d\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.3-py2.py3-none-any.whl size=281891018 sha256=a70d84b3153d4f9bd4fe70c20648ba247d87c026c564b95aa09c52cacd66900a\n  Stored in directory: /root/.cache/pip/wheels/42/b9/63/fe694d7c8358d6714826c98530dab187e6837a7758b775c51f\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=b481fb91894214c62c08b02737de887ec8168339383865346040e277e4df501d\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\nSuccessfully built pytorch-lifestream pyspark antlr4-python3-runtime\nInstalling collected packages: py4j, antlr4-python3-runtime, pyspark, omegaconf, duckdb, xgboost, hydra-core, lightgbm, pytorch_lightning, pytorch-lifestream\n  Attempting uninstall: py4j\n    Found existing installation: py4j 0.10.9.7\n    Uninstalling py4j-0.10.9.7:\n      Successfully uninstalled py4j-0.10.9.7\n  Attempting uninstall: xgboost\n    Found existing installation: xgboost 2.0.3\n    Uninstalling xgboost-2.0.3:\n      Successfully uninstalled xgboost-2.0.3\n  Attempting uninstall: lightgbm\n    Found existing installation: lightgbm 4.2.0\n    Uninstalling lightgbm-4.2.0:\n      Successfully uninstalled lightgbm-4.2.0\n  Attempting uninstall: pytorch_lightning\n    Found existing installation: pytorch-lightning 2.4.0\n    Uninstalling pytorch-lightning-2.4.0:\n      Successfully uninstalled pytorch-lightning-2.4.0\nSuccessfully installed antlr4-python3-runtime-4.9.3 duckdb-1.1.3 hydra-core-1.2.0 lightgbm-3.3.2 omegaconf-2.3.0 py4j-0.10.9.5 pyspark-3.3.3 pytorch-lifestream-0.6.0 pytorch_lightning-2.3.3 xgboost-1.6.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"**Download Dataset**","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download \n \nhf_hub_download(repo_id=\"ai-lab/MBD-mini\", filename=\"ptls.tar.gz\", repo_type=\"dataset\", local_dir=\"/kaggle/working/\")\nhf_hub_download(repo_id=\"ai-lab/MBD-mini\", filename=\"targets.tar.gz\", repo_type=\"dataset\", local_dir=\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:52:54.097494Z","iopub.execute_input":"2025-01-16T23:52:54.098170Z","iopub.status.idle":"2025-01-16T23:53:28.178760Z","shell.execute_reply.started":"2025-01-16T23:52:54.098126Z","shell.execute_reply":"2025-01-16T23:53:28.177881Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"ptls.tar.gz:   0%|          | 0.00/1.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f19a19a1232847c4b9508dd46d8827ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"targets.tar.gz:   0%|          | 0.00/7.93M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4f33fd02825415db40a145c16b7ee70"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/targets.tar.gz'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!tar -xf ptls.tar.gz\n!tar -xf targets.tar.gz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:54:17.995216Z","iopub.execute_input":"2025-01-16T23:54:17.995906Z","iopub.status.idle":"2025-01-16T23:54:36.685555Z","shell.execute_reply.started":"2025-01-16T23:54:17.995872Z","shell.execute_reply":"2025-01-16T23:54:36.684291Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom pyspark.sql import types as T\n\nspark_conf = pyspark.SparkConf()\nspark_conf.setMaster(\"local[*]\").setAppName(\"JoinModality\")\nspark_conf.set(\"spark.driver.maxResultSize\", \"16g\")\nspark_conf.set(\"spark.executor.memory\", \"32g\")\nspark_conf.set(\"spark.executor.memoryOverhead\", \"16g\")\nspark_conf.set(\"spark.driver.memory\", \"32g\")\nspark_conf.set(\"spark.driver.memoryOverhead\", \"16g\")\nspark_conf.set(\"spark.cores.max\", \"24\")\nspark_conf.set(\"spark.sql.shuffle.partitions\", \"200\")\nspark_conf.set(\"spark.local.dir\", \"../../spark_local_dir\")\n\n\nspark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\nspark.sparkContext.getConf().getAll()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:54:36.687692Z","iopub.execute_input":"2025-01-16T23:54:36.688072Z","iopub.status.idle":"2025-01-16T23:54:41.935546Z","shell.execute_reply.started":"2025-01-16T23:54:36.688031Z","shell.execute_reply":"2025-01-16T23:54:41.933516Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"25/01/16 23:54:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/01/16 23:54:40 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[('spark.driver.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n ('spark.app.startTime', '1737071680172'),\n ('spark.app.id', 'local-1737071681384'),\n ('spark.executor.memoryOverhead', '16g'),\n ('spark.driver.memory', '32g'),\n ('spark.driver.maxResultSize', '16g'),\n ('spark.app.name', 'JoinModality'),\n ('spark.local.dir', '../../spark_local_dir'),\n ('spark.executor.id', 'driver'),\n ('spark.driver.host', '493aff43ccbd'),\n ('spark.executor.memory', '32g'),\n ('spark.app.submitTime', '1737071679938'),\n ('spark.rdd.compress', 'True'),\n ('spark.driver.memoryOverhead', '16g'),\n ('spark.executor.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.sql.shuffle.partitions', '200'),\n ('spark.master', 'local[*]'),\n ('spark.submit.pyFiles', ''),\n ('spark.submit.deployMode', 'client'),\n ('spark.driver.port', '35189'),\n ('spark.cores.max', '24'),\n ('spark.ui.showConsoleProgress', 'true')]"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"spark","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:54:41.937175Z","iopub.execute_input":"2025-01-16T23:54:41.938041Z","iopub.status.idle":"2025-01-16T23:54:42.879900Z","shell.execute_reply.started":"2025-01-16T23:54:41.937996Z","shell.execute_reply":"2025-01-16T23:54:42.878853Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<pyspark.sql.session.SparkSession at 0x7b8617c8f1f0>","text/html":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://493aff43ccbd:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>JoinModality</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"TRX_DATA_PATH = '/kaggle/working/ptls/trx/'\nGEO_DATA_PATH = '/kaggle/working/ptls/geo/'\nDIAL_DATA_PATH = '/kaggle/working/ptls/dialog/'\n\nMM_DATA_PATH = '/kaggle/working/MBD/scenario_mbd/data/mm_dataset/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:54:42.882090Z","iopub.execute_input":"2025-01-16T23:54:42.882480Z","iopub.status.idle":"2025-01-16T23:54:42.887601Z","shell.execute_reply.started":"2025-01-16T23:54:42.882432Z","shell.execute_reply":"2025-01-16T23:54:42.886654Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def rename_col(df, prefix, col_id='client_id'):\n    new_column_names = [f\"{prefix}_{col}\" for col in df.columns if col != col_id]\n    old_column_names = [col for col in df.columns if col != col_id]\n    for old_col, new_col in zip(old_column_names, new_column_names):\n        df = df.withColumnRenamed(old_col, new_col)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:54:42.888508Z","iopub.execute_input":"2025-01-16T23:54:42.888850Z","iopub.status.idle":"2025-01-16T23:54:42.899159Z","shell.execute_reply.started":"2025-01-16T23:54:42.888812Z","shell.execute_reply":"2025-01-16T23:54:42.898014Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nfor fold in tqdm(range(0, 5)):\n    trx = spark.read.parquet(os.path.join(TRX_DATA_PATH, f'fold={fold}'))\n    geo = spark.read.parquet(os.path.join(GEO_DATA_PATH, f'fold={fold}'))\n    dial = spark.read.parquet(os.path.join(DIAL_DATA_PATH, f'fold={fold}'))\n    \n    trx = rename_col(trx, 'trx')\n    geo = rename_col(geo, 'geo')\n    dial = rename_col(dial, 'dial')\n    \n    mm_dataset = trx.join(geo, on='client_id', how='outer').join(dial, on='client_id', how='outer')\n    mm_dataset.write.mode('overwrite').parquet(os.path.join(MM_DATA_PATH, f'fold={fold}'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:54:42.900285Z","iopub.execute_input":"2025-01-16T23:54:42.900621Z","iopub.status.idle":"2025-01-16T23:56:40.986907Z","shell.execute_reply.started":"2025-01-16T23:54:42.900568Z","shell.execute_reply":"2025-01-16T23:56:40.986070Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f6ab2742d644221bd366278b3389f5c"}},"metadata":{}},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# spark.read.parquet(os.path.join(MM_DATA_PATH, f'fold=1'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T04:14:57.972950Z","iopub.execute_input":"2024-12-27T04:14:57.973340Z","iopub.status.idle":"2024-12-27T04:14:57.978190Z","shell.execute_reply.started":"2024-12-27T04:14:57.973298Z","shell.execute_reply":"2024-12-27T04:14:57.977441Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"**Targets**","metadata":{}},{"cell_type":"code","source":"from ptls.preprocessing import PysparkDataPreprocessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:59:49.913380Z","iopub.execute_input":"2025-01-16T23:59:49.913773Z","iopub.status.idle":"2025-01-16T23:59:53.663909Z","shell.execute_reply.started":"2025-01-16T23:59:49.913742Z","shell.execute_reply":"2025-01-16T23:59:53.663073Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"TARGETS_DATA_PATH = '/kaggle/working/targets'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:59:53.665349Z","iopub.execute_input":"2025-01-16T23:59:53.665815Z","iopub.status.idle":"2025-01-16T23:59:53.669959Z","shell.execute_reply.started":"2025-01-16T23:59:53.665784Z","shell.execute_reply":"2025-01-16T23:59:53.669045Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"preprocessor_target = PysparkDataPreprocessor(\n    col_id=\"client_id\",\n    col_event_time=\"mon\",\n    event_time_transformation=\"dt_to_timestamp\",\n    cols_identity=[\"target_1\", \"target_2\", \"target_3\", \"target_4\"],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:59:53.671065Z","iopub.execute_input":"2025-01-16T23:59:53.671316Z","iopub.status.idle":"2025-01-16T23:59:53.691879Z","shell.execute_reply.started":"2025-01-16T23:59:53.671292Z","shell.execute_reply":"2025-01-16T23:59:53.691040Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"MMT_DATA_PATH = '/kaggle/working/MBD/scenario_mbd/data/mm_supervised/'\n\nfor fold in range(5):\n    targets = spark.read.parquet(os.path.join(TARGETS_DATA_PATH , f'fold={fold}'))\n    mm_dataset = spark.read.parquet(os.path.join(MM_DATA_PATH , f'fold={fold}'))\n    \n    targets = preprocessor_target.fit_transform(targets)\n    mm_dataset = mm_dataset.join(targets, on='client_id', how='left').drop(*['trans_count', 'diff_trans_date'])\n    mm_dataset.write.parquet(os.path.join(MMT_DATA_PATH, f'fold={fold}'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T00:00:25.526736Z","iopub.execute_input":"2025-01-17T00:00:25.527086Z","iopub.status.idle":"2025-01-17T00:02:01.902819Z","shell.execute_reply.started":"2025-01-17T00:00:25.527057Z","shell.execute_reply":"2025-01-17T00:02:01.901887Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"spark.stop()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T23:06:10.589390Z","iopub.execute_input":"2025-01-16T23:06:10.589768Z","iopub.status.idle":"2025-01-16T23:06:11.628378Z","shell.execute_reply.started":"2025-01-16T23:06:10.589735Z","shell.execute_reply":"2025-01-16T23:06:11.627683Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"!pip list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %load /kaggle/working/MBD/scenario_mbd/conf/trx_coles.yaml\nnew_config = \"\"\"fold: 3\ndefaults:\n  - _self_\n  - dataset_unsupervised/parquet_trx\n  - inference/inference_trx\n  - downstream/downstream\n\nseed_everything: 42\nlogger_name: trx_coles_${fold}\nmodel_path: scenario_mbd/models/${logger_name}.p\nembed_file_name: ${logger_name}\n\ndata_module:\n  _target_: ptls.frames.PtlsDataModule\n  train_data:\n    _target_: ptls.frames.coles.ColesIterableDataset\n    splitter:\n      _target_: ptls.frames.coles.split_strategy.SampleSlices\n      split_count: 5\n      cnt_min: 32\n      cnt_max: 180\n    data: ${dataset_unsupervised.train}\n  valid_data:\n    _target_: ptls.frames.coles.ColesIterableDataset\n    splitter:\n      _target_: ptls.frames.coles.split_strategy.SampleSlices\n      split_count: 5\n      cnt_min: 32\n      cnt_max: 180\n    data: ${dataset_unsupervised.valid}\n  train_batch_size: 512\n  train_num_workers: 16\n  valid_batch_size: 128\n  valid_num_workers: 16\n  \ntrainer:\n  precision: \"bf16\"\n  accelerator: \"gpu\"\n  devices: 1\n  max_epochs: 15\n  limit_val_batches: 5000\n  gradient_clip_val: 0.5\n  deterministic: true\n  checkpoints_every_n_val_epochs: 1\n  fast_dev_run: false\n  \npl_module:\n  _target_: ptls.frames.coles.CoLESModule\n  validation_metric:\n    _target_: ptls.frames.coles.metric.BatchRecallTopK\n    K: 4\n    metric: cosine\n  seq_encoder:\n    _target_: ptls.nn.RnnSeqEncoder\n    trx_encoder:\n      _target_: ptls.nn.TrxEncoder\n      norm_embeddings: false\n      embeddings_noise: 0.003\n      embeddings:\n        event_type:\n          in: 58\n          out: 24\n        event_subtype:\n          in: 59\n          out: 24\n        src_type11:\n          in: 85\n          out: 24\n        src_type12:\n          in: 349\n          out: 24\n        dst_type11:\n          in: 84\n          out: 24\n        dst_type12:\n          in: 417\n          out: 12\n        src_type22:\n          in: 90\n          out: 24\n        src_type32:\n          in: 91\n          out: 24\n      numeric_values:\n        amount: log\n    type: gru\n    hidden_size: 256\n  optimizer_partial:\n    _partial_: true\n    _target_: torch.optim.AdamW\n    lr: 0.001\n  lr_scheduler_partial:\n    _partial_: true\n    _target_: torch.optim.lr_scheduler.StepLR\n    step_size: 3\n    gamma: 0.9025\"\"\"\n\nwith open('/kaggle/working/MBD/scenario_mbd/conf/trx_coles.yaml', mode='w') as file:\n    file.write(new_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T04:17:33.700650Z","iopub.execute_input":"2024-12-27T04:17:33.701247Z","iopub.status.idle":"2024-12-27T04:17:33.707569Z","shell.execute_reply.started":"2024-12-27T04:17:33.701211Z","shell.execute_reply":"2024-12-27T04:17:33.706637Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# %load /kaggle/working/MBD/scenario_mbd/conf/dataset_unsupervised/parquet_trx.yaml\nnew_config = \"\"\"train:\n  _target_: ptls.data_load.datasets.ParquetDataset\n  data_files: \n    - scenario_mbd/data/trx/fold=0\n    - scenario_mbd/data/trx/fold=1\n    - scenario_mbd/data/trx/fold=2\n    - scenario_mbd/data/trx/fold=3\n    - scenario_mbd/data/trx/fold=4\n  i_filters:\n    - _target_: ptls.data_load.iterable_processing.SeqLenFilter\n      min_seq_len: 32\n    - _target_: ptls.data_load.iterable_processing.iterable_seq_len_limit.ISeqLenLimit\n      max_seq_len: 4096\n    - _target_: ptls.data_load.iterable_processing.feature_filter.FeatureFilter\n      drop_feature_names: \n        - client_id\n        - target_1\n        - target_2\n        - target_3\n        - target_4\n    - _target_: ptls.data_load.iterable_processing.to_torch_tensor.ToTorch\n  shuffle_files: true\nvalid:\n  _target_: ptls.data_load.datasets.ParquetDataset\n  data_files: \n    - scenario_mbd/data/trx/fold=${fold}\n  i_filters:\n    - _target_: ptls.data_load.iterable_processing.SeqLenFilter\n      min_seq_len: 32\n    - _target_: ptls.data_load.iterable_processing.iterable_seq_len_limit.ISeqLenLimit\n      max_seq_len: 4096\n    - _target_: ptls.data_load.iterable_processing.feature_filter.FeatureFilter\n      drop_feature_names: \n        - client_id\n        - target_1\n        - target_2\n        - target_3\n        - target_4\n    - _target_: ptls.data_load.iterable_processing.to_torch_tensor.ToTorch\"\"\"\n\nwith open('/kaggle/working/MBD/scenario_mbd/conf/dataset_unsupervised/parquet_trx.yaml', mode='w') as file:\n    file.write(new_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T04:17:42.609335Z","iopub.execute_input":"2024-12-27T04:17:42.609740Z","iopub.status.idle":"2024-12-27T04:17:42.615610Z","shell.execute_reply.started":"2024-12-27T04:17:42.609709Z","shell.execute_reply":"2024-12-27T04:17:42.614653Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# %load /kaggle/working/MBD/scenario_mbd/conf/inference/inference_trx.yaml\nnew_config = \"\"\"num_workers: 16\nbatch_size: 256\ngpus: 1\nuse_save_model: True\nmultimodal: False\ndataset_train:\n     _target_: ptls.data_load.datasets.ParquetDataset\n     data_files: \n        - scenario_mbd/data/trx/fold=0\n        - scenario_mbd/data/trx/fold=1\n        - scenario_mbd/data/trx/fold=2\n        - scenario_mbd/data/trx/fold=3\n        - scenario_mbd/data/trx/fold=4\n     i_filters:\n      - _target_: ptls.data_load.iterable_processing.iterable_seq_len_limit.ISeqLenLimit\n        max_seq_len: 4096\n      - _target_: ptls.data_load.iterable_processing.feature_filter.FeatureFilter\n        keep_feature_names: \n          - client_id\n          - target_1\n          - target_2\n          - target_3\n          - target_4\n      - _target_: ptls.data_load.iterable_processing.to_torch_tensor.ToTorch\n      - _target_: modules.processing.GetSplit\n        start_month: 1 \n        end_month: 12\n        col_id: client_id\ndataset_test:\n    _target_: ptls.data_load.datasets.ParquetDataset\n    data_files: \n        - scenario_mbd/data/trx/fold=${fold}\n    i_filters:\n        - _target_: ptls.data_load.iterable_processing.iterable_seq_len_limit.ISeqLenLimit\n          max_seq_len: 4096\n        - _target_: ptls.data_load.iterable_processing.feature_filter.FeatureFilter\n          keep_feature_names: \n            - client_id\n            - target_1\n            - target_2\n            - target_3\n            - target_4\n        - _target_: ptls.data_load.iterable_processing.to_torch_tensor.ToTorch\n        - _target_: modules.processing.GetSplit\n          start_month: 1 \n          end_month: 12\n          col_id: client_id\noutput:\n  path: scenario_mbd/data/${embed_file_name}\n  format: parquet\n\ncol_id: client_id\ntarget_col_names:\n  - target_1\n  - target_2\n  - target_3\n  - target_4\"\"\"\n\n\nwith open('/kaggle/working/MBD/scenario_mbd/conf/inference/inference_trx.yaml', mode='w') as file:\n    file.write(new_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T04:17:53.069552Z","iopub.execute_input":"2024-12-27T04:17:53.069932Z","iopub.status.idle":"2024-12-27T04:17:53.076148Z","shell.execute_reply.started":"2024-12-27T04:17:53.069898Z","shell.execute_reply":"2024-12-27T04:17:53.075233Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"!ls /kaggle/working/MBD/scenario_mbd/data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T04:17:55.453495Z","iopub.execute_input":"2024-12-27T04:17:55.453829Z","iopub.status.idle":"2024-12-27T04:17:56.471298Z","shell.execute_reply.started":"2024-12-27T04:17:55.453803Z","shell.execute_reply":"2024-12-27T04:17:56.470302Z"}},"outputs":[{"name":"stdout","text":"mm_dataset  mmt_dataset\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!mv /kaggle/working/ptls/trx /kaggle/working/MBD/scenario_mbd/data\n!mv /kaggle/working/ptls/geo /kaggle/working/MBD/scenario_mbd/data\n!mv /kaggle/working/ptls/dialog /kaggle/working/MBD/scenario_mbd/data\n# !mv /kaggle/working/MBD/scenario_mbd/data/trx /kaggle/working/MBD/scenario_mbd/data/mm_dataset ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T04:18:05.702254Z","iopub.execute_input":"2024-12-27T04:18:05.703175Z","iopub.status.idle":"2024-12-27T04:18:08.718317Z","shell.execute_reply.started":"2024-12-27T04:18:05.703127Z","shell.execute_reply":"2024-12-27T04:18:08.716615Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# !export FOLD=3\n%cd /kaggle/working/MBD\n!python ptls_train_inf_down_module.py \\\n        fold=3 \\\n        \"dataset_unsupervised.train.data_files.3='/kaggle/working/MBD/scenario_mbd/data/trx/fold=4'\"\\\n        \"inference.dataset_train.data_files.3='/kaggle/working/MBD/scenario_mbd/data/trx/fold=4'\" \\\n            --config-dir scenario_mbd/conf --config-name trx_coles","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mv /kaggle/working/MBD/scenario_mbd/data/mmt_dataset /kaggle/working/MBD/scenario_mbd/data/mm_supervised","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T08:59:59.112442Z","iopub.execute_input":"2024-12-13T08:59:59.113271Z","iopub.status.idle":"2024-12-13T09:00:00.114926Z","shell.execute_reply.started":"2024-12-13T08:59:59.113230Z","shell.execute_reply":"2024-12-13T09:00:00.113774Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"**trx agg**","metadata":{}},{"cell_type":"code","source":"# %load /kaggle/working/MBD/scenario_mbd/conf/trx_agg.yaml\n\"\"\"fold: 4\ndefaults:\n  - _self_\n  - dataset_unsupervised/parquet_trx\n  - inference/inference_agg_trx\n  - downstream/downstream\n\n\nseed_everything: 42\nlogger_name: trx_agg_${fold}\nmodel_path: scenario_mbd/models/${logger_name}.p\nembed_file_name: ${logger_name}\n\npl_module:\n    seq_encoder:\n    _target_: ptls.nn.AggFeatureSeqEncoder\n    embeddings:\n        event_type:\n          in: 58\n          out: 24\n        event_subtype:\n          in: 59\n          out: 24\n        src_type11:\n          in: 85\n          out: 24\n        src_type12:\n          in: 349\n          out: 24\n        dst_type11:\n          in: 84\n          out: 24\n        dst_type12:\n          in: 417\n          out: 12\n        src_type22:\n          in: 90\n          out: 24\n        src_type32:\n          in: 91\n          out: 24\n    numeric_values:\n      amount: log\n    was_logified: false\n    log_scale_factor: 1\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T05:03:57.428032Z","iopub.execute_input":"2024-12-27T05:03:57.428430Z","iopub.status.idle":"2024-12-27T05:03:57.435032Z","shell.execute_reply.started":"2024-12-27T05:03:57.428392Z","shell.execute_reply":"2024-12-27T05:03:57.434051Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"new_config = \"\"\"num_workers: 0\nbatch_size: 32\ngpus: 1\nuse_save_model: True\nmultimodal: False\ndataset_train:\n     _target_: ptls.data_load.datasets.ParquetDataset\n     data_files: \n          - trx_train_prepr.parquet\n     i_filters:\n     - _target_: ptls.data_load.iterable_processing.iterable_seq_len_limit.ISeqLenLimit\n       max_seq_len: 128\n     - _target_: ptls.data_load.iterable_processing.feature_filter.FeatureFilter\n       keep_feature_names: \n        - client_id\n        - target_1\n        - target_2\n        - target_3\n        - target_4\n     - _target_: ptls.data_load.iterable_processing.to_torch_tensor.ToTorch\n     - _target_: modules.processing.GetSplit\n       start_month: 1 \n       end_month: 12\n       col_id: client_id\ndataset_test:\n    _target_: ptls.data_load.datasets.ParquetDataset\n    data_files: \n          - trx_train_prepr.parquet\n    i_filters:\n        - _target_: ptls.data_load.iterable_processing.iterable_seq_len_limit.ISeqLenLimit\n          max_seq_len: 128\n        - _target_: ptls.data_load.iterable_processing.feature_filter.FeatureFilter\n          keep_feature_names: \n            - client_id\n            - target_1\n            - target_2\n            - target_3\n            - target_4\n        - _target_: ptls.data_load.iterable_processing.to_torch_tensor.ToTorch\noutput:\n  path: ${embed_file_name}\n  format: parquet\"\"\"\nwith open('/kaggle/working/MBD/scenario_mbd/conf/inference/inference_agg_trx.yaml', mode='w') as file:\n    file.write(new_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T05:04:12.049011Z","iopub.execute_input":"2024-12-27T05:04:12.049392Z","iopub.status.idle":"2024-12-27T05:04:12.055418Z","shell.execute_reply.started":"2024-12-27T05:04:12.049358Z","shell.execute_reply":"2024-12-27T05:04:12.054495Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"!python ptls_train_inf_down_module.py \\\n    fold=3 \\\n    \"dataset_unsupervised.train.data_files.2='/kaggle/working/MBD/scenario_mbd/data/mm_dataset/fold=1'\"\\\n    \"inference.dataset_train.data_files.2='/kaggle/working/MBD/scenario_mbd/data/mm_dataset/fold=1'\" \\\n      --config-dir scenario_mbd/conf --config-name trx_agg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T05:16:51.312333Z","iopub.execute_input":"2024-12-27T05:16:51.312747Z","iopub.status.idle":"2024-12-27T05:17:03.564775Z","shell.execute_reply.started":"2024-12-27T05:16:51.312713Z","shell.execute_reply":"2024-12-27T05:17:03.563673Z"}},"outputs":[{"name":"stdout","text":"Error merging override inference.dataset_train.data_files.2='/kaggle/working/MBD/scenario_mbd/data/mm_dataset/fold=1'\nlist index out of range\n    full_key: inference.dataset_train.data_files[2]\n    object_type=list\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n","output_type":"stream"}],"execution_count":62},{"cell_type":"markdown","source":"**Mutimodal**","metadata":{}},{"cell_type":"code","source":"# %load /kaggle/working/MBD/scenario_mbd/conf/dataset_unsupervised/multimodal_parquet_supervised.yaml\nnew_config = \"\"\"train:\n  _target_: ptls.data_load.datasets.ParquetDataset\n  data_files:\n    - scenario_mbd/data/mm_supervised/fold=0\n    - scenario_mbd/data/mm_supervised/fold=1\n    - scenario_mbd/data/mm_supervised/fold=2\n    - scenario_mbd/data/mm_supervised/fold=3\n    - scenario_mbd/data/mm_supervised/fold=4\n  i_filters:\n    - _target_: ptls.data_load.iterable_processing.SeqLenFilter\n      min_seq_len: 32\n    - _target_: ptls.data_load.iterable_processing.iterable_seq_len_limit.ISeqLenLimit\n      max_seq_len: 4096\n    - _target_: ptls.data_load.iterable_processing.to_torch_tensor.ToTorch\n    - _target_: modules.processing.TargetToTorch\n      col_target: target_1\n    - _target_: modules.processing.GetSplit\n      start_month: 1\n      end_month: 12\n      col_id: client_id\n  shuffle_files: true\nvalid:\n  _target_: ptls.data_load.datasets.ParquetDataset\n  data_files:\n    - scenario_mbd/data/mm_supervised/fold=${fold}\n  i_filters:\n    - _target_: ptls.data_load.iterable_processing.SeqLenFilter\n      min_seq_len: 32\n    - _target_: ptls.data_load.iterable_processing.iterable_seq_len_limit.ISeqLenLimit\n      max_seq_len: 4096\n    - _target_: ptls.data_load.iterable_processing.to_torch_tensor.ToTorch\n    - _target_: modules.processing.TargetToTorch\n      col_target: target_1\n    - _target_: modules.processing.GetSplit\n      start_month: 1\n      end_month: 12\n      col_id: client_id\"\"\"\n\nwith open('/kaggle/working/MBD/scenario_mbd/conf/dataset_unsupervised/multimodal_parquet_supervised.yaml', mode='w') as file:\n    file.write(new_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T00:03:02.626715Z","iopub.execute_input":"2025-01-17T00:03:02.627117Z","iopub.status.idle":"2025-01-17T00:03:02.633363Z","shell.execute_reply.started":"2025-01-17T00:03:02.627084Z","shell.execute_reply":"2025-01-17T00:03:02.632564Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# %load /kaggle/working/MBD/scenario_mbd/conf/matching_conf_trxdial.yaml\nnew_config = \"\"\"fold: 4\n\ndefaults:\n  - _self_\n  - dataset_unsupervised/multimodal_parquet_supervised\n\nlogger_name: matching_trxdial_${fold}\n\ninference_run: false\ndownstream_run: false\n\ntrainer:\n    max_epochs: 50\n    accelerator: gpu\n    enable_progress_bar: True\n    gradient_clip_val: 0.5\n    log_every_n_steps: 50\n    limit_val_batches: 512\n\ndata_module:\n  _target_: ptls.frames.PtlsDataModule\n  train_data:\n    _target_: modules.matching.MultiModalDiffSplitIterableDataset\n    splitters:\n        trx:\n          _target_: ptls.frames.coles.split_strategy.SampleSlices\n          split_count: 2\n          cnt_min: 32\n          cnt_max: 180\n        dial:\n          _target_: ptls.frames.coles.split_strategy.SampleSlices\n          split_count: 2\n          cnt_min: 2\n          cnt_max: 10\n    data: ${dataset_unsupervised.train}\n    source_features:\n        trx:\n          - trx_event_type\n          - trx_event_subtype\n          - trx_src_type11\n          - trx_src_type12\n          - trx_dst_type11\n          - trx_dst_type12\n          - trx_src_type22\n          - trx_src_type32\n          - trx_event_time\n        dial:\n          - dial_embedding\n          - dial_event_time\n    col_id: client_id\n    col_time: event_time\n    source_names:\n        - trx\n        - dial\n  valid_data:\n    _target_: modules.matching.MultiModalDiffSplitIterableDataset\n    splitters:\n        trx:\n          _target_: ptls.frames.coles.split_strategy.NoSplit\n        dial:\n          _target_: ptls.frames.coles.split_strategy.NoSplit\n    data: ${dataset_unsupervised.valid}\n    source_features:\n        trx:\n          - trx_event_type\n          - trx_event_subtype\n          - trx_src_type11\n          - trx_src_type12\n          - trx_dst_type11\n          - trx_dst_type12\n          - trx_src_type22\n          - trx_src_type32\n          - trx_event_time\n        dial:\n          - dial_embedding\n          - dial_event_time\n    col_id: user_id\n    col_time: event_time\n    source_names:\n        - trx\n        - dial\n  train_batch_size: 256\n  train_num_workers: 255\n  valid_batch_size: 256\n  valid_num_workers: 255\n\npl_module:\n  _target_: modules.matching.M3CoLESModule\n  validation_metric:\n    _target_: ptls.frames.coles.metric.BatchRecallTopK\n    K: 1\n    metric: cosine\n  head:\n    _target_: ptls.nn.Head\n    input_size: 128\n    use_norm_encoder: True\n    hidden_layers_sizes: \n      - 128\n      - 128\n    objective: \"regression\"\n    num_classes: 128\n  seq_encoders:\n      trx:\n        _target_: ptls.nn.RnnSeqEncoder\n        trx_encoder:\n          _target_: ptls.nn.TrxEncoder\n          norm_embeddings: false\n          embeddings_noise: 0.003\n          linear_projection_size: 32\n          embeddings:\n            event_type:\n              in: 58\n              out: 24\n            event_subtype:\n              in: 59\n              out: 24\n            src_type11:\n              in: 85\n              out: 24\n            src_type12:\n              in: 349\n              out: 24\n            dst_type11:\n              in: 84\n              out: 24\n            dst_type12:\n              in: 417\n              out: 12\n            src_type22:\n              in: 90\n              out: 24\n            src_type32:\n              in: 91\n              out: 24\n          numeric_values:\n            amount: log\n        type: gru\n        hidden_size: 128\n      dial: \n        _target_: ptls.nn.RnnSeqEncoder\n        trx_encoder:\n          _target_: ptls.nn.TrxEncoder\n          embeddings_noise: 0.003\n          linear_projection_size: 32\n          custom_embeddings:\n            embedding: \n              _target_: ptls.nn.trx_encoder.encoders.IdentityEncoder\n              output_size: 768\n        type: gru\n        hidden_size: 128\n  loss:\n    _target_: ptls.frames.coles.losses.SoftmaxLoss\n  optimizer_partial:\n    _partial_: true\n    _target_: torch.optim.AdamW\n    lr: 0.001\n    weight_decay: 1e-4\n  lr_scheduler_partial:\n    _partial_: true\n    _target_: torch.optim.lr_scheduler.StepLR\n    step_size: 1\n    gamma: 0.9\"\"\"\n\nwith open('/kaggle/working/MBD/scenario_mbd/conf/matching_conf_trxdial.yaml', mode='w') as file:\n    file.write(new_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T00:02:22.873194Z","iopub.execute_input":"2025-01-17T00:02:22.873790Z","iopub.status.idle":"2025-01-17T00:02:22.880385Z","shell.execute_reply.started":"2025-01-17T00:02:22.873756Z","shell.execute_reply":"2025-01-17T00:02:22.879508Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"spark.read.parquet(os.path.join(MMT_DATA_PATH, f'fold={1}'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T00:34:55.602868Z","iopub.execute_input":"2025-01-17T00:34:55.603837Z","iopub.status.idle":"2025-01-17T00:34:55.672587Z","shell.execute_reply.started":"2025-01-17T00:34:55.603782Z","shell.execute_reply":"2025-01-17T00:34:55.671431Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"DataFrame[client_id: string, trx_event_time: array<bigint>, trx_amount: array<float>, trx_event_type: array<int>, trx_event_subtype: array<int>, trx_currency: array<int>, trx_src_type11: array<int>, trx_src_type12: array<int>, trx_dst_type11: array<int>, trx_dst_type12: array<int>, trx_src_type21: array<int>, trx_src_type22: array<int>, trx_src_type31: array<int>, trx_src_type32: array<int>, geo_event_time: array<bigint>, geo_geohash_4: array<int>, geo_geohash_5: array<int>, geo_geohash_6: array<int>, dial_event_time: array<bigint>, dial_embedding: array<array<float>>, event_time: array<bigint>, target_1: array<int>, target_2: array<int>, target_3: array<int>, target_4: array<int>]"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"spark.read.parquet(os.path.join(MMT_DATA_PATH, f'fold={1}')).select('event_time').show(2, truncate=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T00:34:38.876594Z","iopub.execute_input":"2025-01-17T00:34:38.877674Z","iopub.status.idle":"2025-01-17T00:34:39.006030Z","shell.execute_reply.started":"2025-01-17T00:34:38.877610Z","shell.execute_reply":"2025-01-17T00:34:39.005085Z"}},"outputs":[{"name":"stdout","text":"+------------------------------------------------------------------------------------------------------------------------------------------------+\n|event_time                                                                                                                                      |\n+------------------------------------------------------------------------------------------------------------------------------------------------+\n|[1646006400, 1648684800, 1651276800, 1653955200, 1656547200, 1659225600, 1661904000, 1664496000, 1667174400, 1669766400, 1672444800, 1675123200]|\n|[1646006400, 1648684800, 1651276800, 1653955200, 1656547200, 1659225600, 1661904000, 1664496000, 1667174400, 1669766400, 1672444800, 1675123200]|\n+------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 2 rows\n\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"spark.read.parquet(os.path.join(MMT_DATA_PATH, f'fold={1}')).select('trx_event_time').show(1, truncate=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T00:37:31.739913Z","iopub.execute_input":"2025-01-17T00:37:31.740252Z","iopub.status.idle":"2025-01-17T00:37:31.860758Z","shell.execute_reply.started":"2025-01-17T00:37:31.740223Z","shell.execute_reply":"2025-01-17T00:37:31.859869Z"}},"outputs":[{"name":"stdout","text":"+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|trx_event_time                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|[1609736069, 1610098321, 1610256361, 1610574549, 1610599969, 1611179771, 1611241470, 1612126672, 1612213268, 1613031391, 1613722286, 1613727343, 1614140945, 1614584443, 1615186072, 1615549597, 1615552334, 1615612580, 1615639340, 1615714466, 1615795154, 1615876082, 1616222943, 1616562790, 1617171625, 1617346902, 1618356904, 1620774330, 1621743640, 1622274232, 1622534580, 1624250015, 1624890072, 1625225943, 1625826738, 1627428961, 1627801517, 1628223744, 1628398583, 1628767511, 1629358943, 1630528466, 1630981685, 1631088438, 1631403766, 1631518996, 1631976949, 1632150943, 1632401689, 1632800841, 1634989805, 1635231326, 1635315084, 1635714016, 1635978110, 1636092602, 1636167050, 1637039484, 1637300453, 1638274336, 1638802839, 1639638395, 1640093574, 1640330306, 1640430172, 1641544277, 1642059595, 1642062717, 1642231367, 1643265934, 1643275237, 1643979564, 1644053534, 1644809487, 1645077752, 1645182532, 1645437707, 1647765572, 1647869144, 1648127463, 1648293547, 1648391426, 1648700001, 1648783112, 1648900213, 1649525297, 1649570227, 1650044286, 1650304870, 1651127435, 1651576928, 1651900757, 1652020300, 1652333675, 1652546376, 1652939195, 1654320069, 1654494092, 1655205268, 1656488954, 1657121177, 1657142960, 1657342064, 1657427528, 1657805837, 1658029845, 1658053208, 1658460848, 1659446668, 1660049451, 1660466975, 1660657535, 1661079369, 1661287651, 1661438551, 1661861124, 1662258157, 1663419342, 1663765286, 1665922425, 1665984025, 1666938646, 1666951458, 1667056988, 1667195810, 1667462057, 1669820736, 1670684977, 1670912862, 1671087872]|\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 1 row\n\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"%cd /kaggle/working/MBD\n!python ptls_train_inf_down_module.py \\\n    fold=3 \\\n    \"dataset_unsupervised.train.data_files.3='scenario_mbd/data/mm_dataset_fold/fold=4'\"\\\n        --config-dir scenario_mbd/conf --config-name matching_conf_trxdial","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T00:03:06.491860Z","iopub.execute_input":"2025-01-17T00:03:06.492748Z","iopub.status.idle":"2025-01-17T00:03:32.438962Z","shell.execute_reply.started":"2025-01-17T00:03:06.492705Z","shell.execute_reply":"2025-01-17T00:03:32.437828Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/MBD\n[2025-01-17 00:03:16,314][__main__][INFO] - Start train...\n[2025-01-17 00:03:16,493][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True\n[2025-01-17 00:03:16,495][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores\n[2025-01-17 00:03:16,495][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs\n[2025-01-17 00:03:16,562][pytorch_lightning.loggers.tensorboard][WARNING] - Missing logger folder: lightning_logs/matching_trxdial_3\n[2025-01-17 00:03:17,318][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n[2025-01-17 00:03:17,552][pytorch_lightning.callbacks.model_summary][INFO] - \n  | Name               | Type            | Params | Mode \n---------------------------------------------------------------\n0 | _loss              | SoftmaxLoss     | 0      | train\n1 | _seq_encoder       | RnnSeqEncoder   | 92.8 K | train\n2 | _validation_metric | BatchRecallTopK | 0      | train\n3 | seq_encoders       | ModuleDict      | 181 K  | train\n4 | _head              | Head            | 49.5 K | train\n---------------------------------------------------------------\n230 K     Trainable params\n0         Non-trainable params\n230 K     Total params\n0.923     Total estimated model params size (MB)\nSanity Checking: |                                        | 0/? [00:00<?, ?it/s]Error executing job with overrides: ['fold=3', \"dataset_unsupervised.train.data_files.3='scenario_mbd/data/mm_dataset_fold/fold=4'\"]\nTraceback (most recent call last):\n  File \"/kaggle/working/MBD/ptls_train_inf_down_module.py\", line 186, in main\n    train_module(conf)\n  File \"/kaggle/working/MBD/ptls_train_inf_down_module.py\", line 79, in train_module\n    trainer.fit(model, dm)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 543, in fit\n    call._call_and_handle_interrupt(\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 44, in _call_and_handle_interrupt\n    return trainer_fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 579, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _run\n    results = self._run_stage()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1028, in _run_stage\n    self._run_sanity_check()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1057, in _run_sanity_check\n    val_loop.run()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py\", line 182, in _decorator\n    return loop_run(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 128, in run\n    batch, batch_idx, dataloader_idx = next(data_fetcher)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py\", line 133, in __next__\n    batch = super().__next__()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py\", line 60, in __next__\n    batch = next(self.iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py\", line 341, in __next__\n    out = next(self._iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py\", line 142, in __next__\n    out = next(self.iterators[0])\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n    data = self._next_data()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1344, in _next_data\n    return self._process_data(data)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1370, in _process_data\n    data.reraise()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_utils.py\", line 706, in reraise\n    raise exception\nIndexError: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n    data.append(next(self.dataset_iter))\n  File \"/kaggle/working/MBD/modules/matching.py\", line 382, in __iter__\n    for feature_arrays in self.data:\n  File \"/kaggle/working/MBD/modules/processing.py\", line 105, in __iter__\n    features[key] = tensor[mask]\nIndexError: The shape of the mask [12] at index 0 does not match the shape of the indexed tensor [118] at index 0\n\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n                                                                                \r","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**В 19 строке был пробел после цифры**\n\n**В целом что-то крайне странное, гидра просто не может собрать нормально конфиг. Как будто проще написать mm + agg с нуля**","metadata":{}},{"cell_type":"code","source":"# %load scenario_mbd/conf/matching_conf_trxdial.yaml\nnew_config = \"\"\"fold: 3\n\ndefaults:\n  - _self_\n  - dataset_unsupervised/multimodal_parquet_supervised\n\nlogger_name: matching_trxdial_${fold}\n\ninference_run: false\ndownstream_run: false\n\ntrainer:\n    max_epochs: 50\n    accelerator: gpu\n    enable_progress_bar: True\n    gradient_clip_val: 0.5\n    log_every_n_steps: 50\n    limit_val_batches: 512\n\ndata_module:\n  _target_: ptls.frames.PtlsDataModule\n  train_data:\n    _target_: modules.matching.MultiModalDiffSplitIterableDataset\n    splitters:\n        trx:\n          _target_: ptls.frames.coles.split_strategy.SampleSlices\n          split_count: 2\n          cnt_min: 32\n          cnt_max: 180\n        dial:\n          _target_: ptls.frames.coles.split_strategy.SampleSlices\n          split_count: 2\n          cnt_min: 2\n          cnt_max: 10\n    data: ${dataset_unsupervised.train}\n    source_features:\n        trx:\n          - event_type\n          - event_subtype\n          - src_type11\n          - src_type12\n          - dst_type11\n          - dst_type12\n          - src_type22\n          - src_type32\n          - event_time\n        dial:\n          - embedding\n          - event_time\n    col_id: client_id\n    col_time: event_time\n    source_names:\n        - trx\n        - dial\n  valid_data:\n    _target_: modules.matching.MultiModalDiffSplitIterableDataset\n    splitters:\n        trx:\n          _target_: ptls.frames.coles.split_strategy.NoSplit\n        dial:\n          _target_: ptls.frames.coles.split_strategy.NoSplit\n    data: ${dataset_unsupervised.valid}\n    source_features:\n        trx:\n          - event_type\n          - event_subtype\n          - src_type11\n          - src_type12\n          - dst_type11\n          - dst_type12\n          - src_type22\n          - src_type32\n          - event_time\n        dial:\n          - embedding\n          - event_time\n    col_id: user_id\n    col_time: event_time\n    source_names:\n        - trx\n        - dial\n  train_batch_size: 256\n  train_num_workers: 255\n  valid_batch_size: 256\n  valid_num_workers: 255\n\npl_module:\n  _target_: modules.matching.M3CoLESModule\n  validation_metric:\n    _target_: ptls.frames.coles.metric.BatchRecallTopK\n    K: 1\n    metric: cosine\n  head:\n    _target_: ptls.nn.Head\n    input_size: 128\n    use_norm_encoder: True\n    hidden_layers_sizes: \n      - 128\n      - 128\n    objective: \"regression\"\n    num_classes: 128\n  seq_encoders:\n      trx:\n        _target_: ptls.nn.RnnSeqEncoder\n        trx_encoder:\n          _target_: ptls.nn.TrxEncoder\n          norm_embeddings: false\n          embeddings_noise: 0.003\n          linear_projection_size: 32\n          embeddings:\n            event_type:\n              in: 58\n              out: 24\n            event_subtype:\n              in: 59\n              out: 24\n            src_type11:\n              in: 85\n              out: 24\n            src_type12:\n              in: 349\n              out: 24\n            dst_type11:\n              in: 84\n              out: 24\n            dst_type12:\n              in: 417\n              out: 12\n            src_type22:\n              in: 90\n              out: 24\n            src_type32:\n              in: 91\n              out: 24\n          numeric_values:\n            amount: log\n        type: gru\n        hidden_size: 128\n      dial: \n        _target_: ptls.nn.RnnSeqEncoder\n        trx_encoder:\n          _target_: ptls.nn.TrxEncoder\n          embeddings_noise: 0.003\n          linear_projection_size: 32\n          custom_embeddings:\n            embedding: \n              _target_: ptls.nn.trx_encoder.encoders.IdentityEncoder\n              size: 768\n        type: gru\n        hidden_size: 128\n  loss:\n    _target_: ptls.frames.coles.losses.SoftmaxLoss\n  optimizer_partial:\n    _partial_: true\n    _target_: torch.optim.AdamW\n    lr: 0.001\n    weight_decay: 1e-4\n  lr_scheduler_partial:\n    _partial_: true\n    _target_: torch.optim.lr_scheduler.StepLR\n    step_size: 1\n    gamma: 0.9\"\"\"\nwith open('/kaggle/working/MBD/scenario_mbd/conf/matching_conf_trxdial.yaml', mode='w') as file:\n    file.write(new_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T05:22:10.240486Z","iopub.execute_input":"2024-12-27T05:22:10.240846Z","iopub.status.idle":"2024-12-27T05:22:10.248655Z","shell.execute_reply.started":"2024-12-27T05:22:10.240818Z","shell.execute_reply":"2024-12-27T05:22:10.247545Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"!python ptls_train_inf_down_module.py \\\n        fold=3 \\\n        \"dataset_unsupervised.train.data_files.3='/kaggle/working/MBD/scenario_mbd/data/mm_dataset/fold=4'\"\\\n            --config-dir scenario_mbd/conf --config-name matching_conf_trxdial","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T05:22:10.389923Z","iopub.execute_input":"2024-12-27T05:22:10.390606Z","iopub.status.idle":"2024-12-27T05:22:22.832086Z","shell.execute_reply.started":"2024-12-27T05:22:10.390565Z","shell.execute_reply":"2024-12-27T05:22:22.830996Z"}},"outputs":[{"name":"stdout","text":"Could not override 'dataset_unsupervised.train.data_files.3'.\nTo append to your config use +dataset_unsupervised.train.data_files.3='/kaggle/working/MBD/scenario_mbd/data/mm_dataset/fold=4'\nKey 'train' is not in struct\n    full_key: dataset_unsupervised.train\n    object_type=dict\n\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}