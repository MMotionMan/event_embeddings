{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lifestream\n",
      "  Downloading pytorch-lifestream-0.6.0.tar.gz (163 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting duckdb (from pytorch-lifestream)\n",
      "  Downloading duckdb-1.2.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (966 bytes)\n",
      "Collecting hydra-core>=1.1.2 (from pytorch-lifestream)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: numpy>=1.21.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pytorch-lifestream) (2.1.1)\n",
      "Collecting omegaconf (from pytorch-lifestream)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pandas>=1.3.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pytorch-lifestream) (2.2.3)\n",
      "Collecting pyarrow>=6.0.1 (from pytorch-lifestream)\n",
      "  Downloading pyarrow-19.0.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pytorch-lightning>=1.6.0 (from pytorch-lifestream)\n",
      "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting scikit-learn>=1.0.2 (from pytorch-lifestream)\n",
      "  Downloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: torch>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pytorch-lifestream) (2.6.0)\n",
      "Collecting torchmetrics>=0.9.0 (from pytorch-lifestream)\n",
      "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting transformers (from pytorch-lifestream)\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1.2->pytorch-lifestream)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from hydra-core>=1.1.2->pytorch-lifestream) (24.2)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from omegaconf->pytorch-lifestream) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas>=1.3.5->pytorch-lifestream) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas>=1.3.5->pytorch-lifestream) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas>=1.3.5->pytorch-lifestream) (2025.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pytorch-lightning>=1.6.0->pytorch-lifestream) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pytorch-lightning>=1.6.0->pytorch-lifestream) (4.12.2)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>=1.6.0->pytorch-lifestream)\n",
      "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn>=1.0.2->pytorch-lifestream) (1.15.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=1.0.2->pytorch-lifestream)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.0.2->pytorch-lifestream)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.12.0->pytorch-lifestream) (3.17.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.12.0->pytorch-lifestream) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.12.0->pytorch-lifestream) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.12.0->pytorch-lifestream) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.12.0->pytorch-lifestream) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sympy==1.13.1->torch>=1.12.0->pytorch-lifestream) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->pytorch-lifestream) (0.28.1)\n",
      "Collecting regex!=2019.12.17 (from transformers->pytorch-lifestream)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers->pytorch-lifestream) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers->pytorch-lifestream)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers->pytorch-lifestream)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream)\n",
      "  Downloading aiohttp-3.11.12-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-lifestream) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->torch>=1.12.0->pytorch-lifestream) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers->pytorch-lifestream) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers->pytorch-lifestream) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers->pytorch-lifestream) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers->pytorch-lifestream) (2025.1.31)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream)\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (25.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream)\n",
      "  Downloading frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream)\n",
      "  Downloading multidict-6.1.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream)\n",
      "  Downloading propcache-0.2.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream)\n",
      "  Downloading yarl-1.18.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading pyarrow-19.0.0-cp313-cp313-macosx_12_0_arm64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading duckdb-1.2.0-cp313-cp313-macosx_12_0_arm64.whl (15.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
      "Downloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.12-cp313-cp313-macosx_11_0_arm64.whl (453 kB)\n",
      "Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl (50 kB)\n",
      "Downloading multidict-6.1.0-cp313-cp313-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading propcache-0.2.1-cp313-cp313-macosx_11_0_arm64.whl (44 kB)\n",
      "Downloading yarl-1.18.3-cp313-cp313-macosx_11_0_arm64.whl (91 kB)\n",
      "Building wheels for collected packages: pytorch-lifestream, antlr4-python3-runtime\n",
      "  Building wheel for pytorch-lifestream (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytorch-lifestream: filename=pytorch_lifestream-0.6.0-py3-none-any.whl size=274640 sha256=f308725423d7e7ff0321d335d8bb45c613361839f1d33623c5b6a7a39b5f170b\n",
      "  Stored in directory: /Users/anatoliy/Library/Caches/pip/wheels/99/01/f1/911044ee37bdfe07c91f6d8e8fae55aa1df866ef257c7f2cb0\n",
      "  Building wheel for antlr4-python3-runtime (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144591 sha256=5910206ef5b95b59ec6b4237962dff5d7e599ff6e5e14a1bfaf2db78932feee4\n",
      "  Stored in directory: /Users/anatoliy/Library/Caches/pip/wheels/d5/b3/74/a35b66048c9de6631cd74cbc9475e6feb3e69a467983446bd8\n",
      "Successfully built pytorch-lifestream antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, threadpoolctl, safetensors, regex, pyarrow, propcache, omegaconf, multidict, lightning-utilities, joblib, frozenlist, duckdb, aiohappyeyeballs, yarl, scikit-learn, hydra-core, aiosignal, torchmetrics, tokenizers, aiohttp, transformers, pytorch-lightning, pytorch-lifestream\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 antlr4-python3-runtime-4.9.3 duckdb-1.2.0 frozenlist-1.5.0 hydra-core-1.3.2 joblib-1.4.2 lightning-utilities-0.12.0 multidict-6.1.0 omegaconf-2.3.0 propcache-0.2.1 pyarrow-19.0.0 pytorch-lifestream-0.6.0 pytorch-lightning-2.5.0.post0 regex-2024.11.6 safetensors-0.5.2 scikit-learn-1.6.1 threadpoolctl-3.5.0 tokenizers-0.21.0 torchmetrics-1.6.1 transformers-4.48.3 yarl-1.18.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install huggingface_hub\n",
    "# !pip3 install pyspark\n",
    "# !pip3 install pytorch-lifestream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "import time\n",
    "import datetime\n",
    "from ptls.data_load.datasets import ParquetDataset, ParquetFiles\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType\n",
    "from tqdm.notebook import tqdm\n",
    "from ptls.preprocessing import PysparkDataPreprocessor\n",
    "import pytorch_lightning as pl\n",
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter, FeatureFilter\n",
    "from ptls.data_load.iterable_processing.iterable_seq_len_limit import ISeqLenLimit\n",
    "from ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\n",
    "from ptls.frames.coles import CoLESModule\n",
    "from ptls.frames import PtlsDataModule\n",
    "from ptls.frames.coles import ColesDataset\n",
    "from ptls.frames.coles.split_strategy import SampleSlices\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import calendar\n",
    "from glob import glob\n",
    "from ptls.data_load.utils import collate_feature_dict\n",
    "\n",
    "from ptls.data_load.iterable_processing_dataset import IterableProcessingDataset\n",
    "from datetime import datetime\n",
    "from ptls.data_load.padded_batch import PaddedBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/11 19:21:29 WARN Utils: Your hostname, Anatolijs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.3.23 instead (on interface en0)\n",
      "25/02/11 19:21:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/11 19:21:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/11 19:21:30 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.driver.maxResultSize', '16g'),\n",
       " ('spark.app.name', 'JoinModality'),\n",
       " ('spark.local.dir', '../../spark_local_dir'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '16g'),\n",
       " ('spark.driver.port', '54756'),\n",
       " ('spark.app.id', 'local-1739290890738'),\n",
       " ('spark.app.submitTime', '1739290890151'),\n",
       " ('spark.driver.memoryOverhead', '8g'),\n",
       " ('spark.executor.memory', '16g'),\n",
       " ('spark.executor.memoryOverhead', '8g'),\n",
       " ('spark.driver.host', '192.168.3.23'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.startTime', '1739290890281'),\n",
       " ('spark.sql.shuffle.partitions', '200'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.cores.max', '8'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_conf = pyspark.SparkConf()\n",
    "spark_conf.setMaster(\"local[*]\").setAppName(\"JoinModality\")\n",
    "spark_conf.set(\"spark.driver.maxResultSize\", \"16g\")\n",
    "spark_conf.set(\"spark.executor.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.executor.memoryOverhead\", \"8g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.driver.memoryOverhead\", \"8g\")\n",
    "spark_conf.set(\"spark.cores.max\", \"8\")\n",
    "spark_conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark_conf.set(\"spark.local.dir\", \"../../spark_local_dir\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.3.23:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>JoinModality</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x31936c050>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir ./data/mm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRX_DATA_PATH = './data/ptls/trx/'\n",
    "GEO_DATA_PATH = './data/ptls/geo/'\n",
    "DIAL_DATA_PATH = './data/ptls/dialog/'\n",
    "\n",
    "MM_DATA_PATH = './data/mm_dataset'\n",
    "MMT_DATA_PATH = './data/mm_dataset_supervised'\n",
    "\n",
    "TARGETS_DATA_PATH = './data/targets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_col(df, prefix, col_id='client_id'):\n",
    "    new_column_names = [f\"{prefix}_{col}\" for col in df.columns if col != col_id]\n",
    "    old_column_names = [col for col in df.columns if col != col_id]\n",
    "    for old_col, new_col in zip(old_column_names, new_column_names):\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e01714219f4323a7516ede1d0f8111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=21358Kb max_used=21358Kb free=109713Kb\n",
      " bounds [0x0000000104b4c000, 0x00000001060ac000, 0x000000010cb4c000]\n",
      " total_blobs=8187 nmethods=7233 adapters=868\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from ptls.preprocessing import PysparkDataPreprocessor\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "for fold in tqdm(range(0, 5)):\n",
    "    trx = spark.read.parquet(os.path.join(TRX_DATA_PATH, f'fold={fold}'))\n",
    "    dial = spark.read.parquet(os.path.join(DIAL_DATA_PATH, f'fold={fold}'))\n",
    "    \n",
    "    trx = rename_col(trx, 'trx')\n",
    "    dial = rename_col(dial, 'dial')\n",
    "    \n",
    "    mm_dataset = trx.join(dial, on='client_id', how='outer').drop(*['trx_src_type21', 'trx_src_type31'])\n",
    "\n",
    "    mm_dataset.write.mode('overwrite').parquet(os.path.join(MM_DATA_PATH, f'fold={fold}'))\n",
    "    \n",
    "    del trx\n",
    "    del dial\n",
    "    del mm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/IPython/core/formatters.py:406\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    404\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pyspark/sql/session.py:618\u001b[0m, in \u001b[0;36mSparkSession._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124m        <div>\u001b[39m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124m            <p><b>SparkSession - \u001b[39m\u001b[38;5;132;01m{catalogImplementation}\u001b[39;00m\u001b[38;5;124m</b></p>\u001b[39m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;132;01m{sc_HTML}\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124m        </div>\u001b[39m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    617\u001b[0m         catalogImplementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalogImplementation\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m--> 618\u001b[0m         sc_HTML\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_repr_html_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    619\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pyspark/context.py:412\u001b[0m, in \u001b[0;36mSparkContext._repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;43m    <div>\u001b[39;49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;43m        <p><b>SparkContext</b></p>\u001b[39;49m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;43m        <p><a href=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{sc.uiWebUrl}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m>Spark UI</a></p>\u001b[39;49m\n\u001b[1;32m    402\u001b[0m \n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;43m        <dl>\u001b[39;49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;43m          <dt>Version</dt>\u001b[39;49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;43m            <dd><code>v\u001b[39;49m\u001b[38;5;132;43;01m{sc.version}\u001b[39;49;00m\u001b[38;5;124;43m</code></dd>\u001b[39;49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;43m          <dt>Master</dt>\u001b[39;49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;43m            <dd><code>\u001b[39;49m\u001b[38;5;132;43;01m{sc.master}\u001b[39;49;00m\u001b[38;5;124;43m</code></dd>\u001b[39;49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;43m          <dt>AppName</dt>\u001b[39;49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;43m            <dd><code>\u001b[39;49m\u001b[38;5;132;43;01m{sc.appName}\u001b[39;49;00m\u001b[38;5;124;43m</code></dd>\u001b[39;49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;43m        </dl>\u001b[39;49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;43m    </div>\u001b[39;49m\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pyspark/context.py:603\u001b[0m, in \u001b[0;36mSparkContext.uiWebUrl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21muiWebUrl\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    589\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the URL of the SparkUI instance started by this :class:`SparkContext`\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.1.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;124;03m    'http://...'\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m     jurl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m()\u001b[38;5;241m.\u001b[39muiWebUrl()\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jurl\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mif\u001b[39;00m jurl\u001b[38;5;241m.\u001b[39mnonEmpty() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x31936c050>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ptls.data_load.iterable_processing_dataset import IterableProcessingDataset\n",
    "from ptls.data_load import IterableChain\n",
    "from datetime import datetime\n",
    "from ptls.data_load.datasets.parquet_dataset import ParquetDataset, ParquetFiles\n",
    "from ptls.data_load.iterable_processing.feature_filter import FeatureFilter\n",
    "from ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\n",
    "import torch\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from ptls.data_load.padded_batch import PaddedBatch\n",
    "from ptls.data_load.utils import collate_feature_dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TargetToTorch(IterableProcessingDataset):\n",
    "    def __init__(self, col_target):\n",
    "        super().__init__()\n",
    "        self.col_target = col_target\n",
    "\n",
    "    def __iter__(self):\n",
    "        for rec in self._src:\n",
    "            features = rec[0] if type(rec) is tuple else rec\n",
    "            features[self.col_target] = np.stack(np.array(features[self.col_target]))\n",
    "            features[self.col_target] = torch.tensor(features[self.col_target])\n",
    "            yield features\n",
    "\n",
    "class DeleteNan(IterableProcessingDataset):\n",
    "    def __init__(self, col_name):\n",
    "        super().__init__()\n",
    "        self.col_name = col_name\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for rec in self._src:\n",
    "            features = rec[0] if type(rec) is tuple else rec\n",
    "            if features[self.col_name] is not None:\n",
    "                yield features\n",
    "\n",
    "\n",
    "class DialToTorch(IterableProcessingDataset):\n",
    "    def __init__(self, col_time, col_embeds):\n",
    "        super().__init__()\n",
    "        self._year=2022\n",
    "        self.col_embeds = col_embeds\n",
    "        self.col_time = col_time\n",
    "    def __iter__(self):\n",
    "        for rec in self._src:\n",
    "            features = rec[0] if type(rec) is tuple else rec\n",
    "            features = features.copy()\n",
    "            if features[self.col_time] is None:\n",
    "                features[self.col_time] = torch.tensor([0])\n",
    "            if features[self.col_embeds] is None:\n",
    "                features[self.col_embeds] = torch.zeros(768)\n",
    "            \n",
    "            for key, tens in features.items():\n",
    "                if key == self.col_embeds:\n",
    "                    features[key] = torch.tensor(tens.tolist())\n",
    "\n",
    "            yield features\n",
    "\n",
    "class GetSplit(IterableProcessingDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_month,\n",
    "        end_month,\n",
    "        year=2022,\n",
    "        col_id='client_id',\n",
    "        col_time='event_time'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.start_month = start_month\n",
    "        self.end_month = end_month\n",
    "        self._year = year\n",
    "        self._col_id = col_id\n",
    "        self._col_time = col_time\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for rec in self._src:\n",
    "            for month in range(self.start_month, self.end_month+1):\n",
    "                features = rec[0] if type(rec) is tuple else rec\n",
    "                features = features.copy()\n",
    "                \n",
    "                if month == 12:\n",
    "                    month_event_time = datetime(self._year + 1, 1, 1).timestamp()\n",
    "                else:\n",
    "                    month_event_time = datetime(self._year, month + 1, 1).timestamp()\n",
    "                    \n",
    "                year_event_time = datetime(self._year, 1, 1).timestamp()\n",
    "                \n",
    "                mask = features[self._col_time] < month_event_time\n",
    "                \n",
    "                for key, tensor in features.items():\n",
    "                    if key.startswith('target'):\n",
    "                        features[key] = tensor[month - 1].tolist()    \n",
    "                    elif key != self._col_id:\n",
    "                        features[key] = tensor[mask] \n",
    "                            \n",
    "                features[self._col_id] += '_month=' + str(month)\n",
    "\n",
    "                yield features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.datasets import ParquetDataset\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.data_load.iterable_processing.iterable_seq_len_limit import ISeqLenLimit\n",
    "from ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\n",
    "train = ParquetDataset(\n",
    "    data_files=[\n",
    "        os.path.join(MM_DATA_PATH, f'fold={0}'),\n",
    "        os.path.join(MM_DATA_PATH, f'fold={1}'),\n",
    "        os.path.join(MM_DATA_PATH, f'fold={2}')\n",
    "    ],\n",
    "    i_filters=[\n",
    "        DeleteNan('trx_event_time'),\n",
    "        DeleteNan('dial_event_time'),\n",
    "        SeqLenFilter(min_seq_len=8),\n",
    "        ISeqLenLimit(max_seq_len=128),\n",
    "        ToTorch(),\n",
    "        DialToTorch(col_time='dial_event_time', col_embeds='dial_embedding'),\n",
    "        # GetSplit(\n",
    "        #     start_month=1,\n",
    "        #     end_month=12,\n",
    "        #     col_id='client_id'\n",
    "        # )\n",
    "    ],\n",
    "    shuffle_files=True\n",
    ")\n",
    "valid = ParquetDataset(\n",
    "    data_files=[\n",
    "        os.path.join(MM_DATA_PATH, f'fold={3}')\n",
    "    ],\n",
    "    i_filters=[\n",
    "        DeleteNan('trx_event_time'),\n",
    "        DeleteNan('dial_event_time'),\n",
    "        SeqLenFilter(min_seq_len=8),\n",
    "        ISeqLenLimit(max_seq_len=128),\n",
    "        ToTorch(),\n",
    "        DialToTorch(col_time='dial_event_time', col_embeds='dial_embedding'),\n",
    "        # GetSplit(\n",
    "        #     start_month=1,\n",
    "        #     end_month=12,\n",
    "        #     col_id='client_id'\n",
    "        # )\n",
    "    ],\n",
    "    shuffle_files=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8r/k1fwfh3550v1cfwnn4xh0tlm0000gn/T/ipykernel_5928/2824446927.py:58: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  features[key] = torch.tensor(tens.tolist())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'client_id': '00959853604b313670be3e3a8cc0d74cc9989a55b22ebd92342a362ac5a2defd',\n",
       " 'trx_event_time': tensor([1641725919, 1641773548, 1641816440, 1641899299, 1642054006, 1642934607,\n",
       "         1643110593, 1643114702, 1643332030, 1643367020, 1643621814, 1643777779,\n",
       "         1643877381, 1644067076, 1644240593, 1644319728, 1644397246, 1644397352,\n",
       "         1644831396, 1645353809, 1645529321, 1645594823, 1645779761, 1645942207,\n",
       "         1646121979, 1646135130, 1646204679, 1646307889, 1646379997, 1646382051,\n",
       "         1646477477, 1646657779, 1646781742, 1646798000, 1646893096, 1646895279,\n",
       "         1646995401, 1647440561, 1647604113, 1647652927, 1647949839, 1648248346,\n",
       "         1648297950, 1648731753, 1649080534, 1649203518, 1649764018, 1649929946,\n",
       "         1650078981, 1650197506, 1650528968, 1650714186, 1650985929, 1651136867,\n",
       "         1651233506, 1652170784, 1652183180, 1652544138, 1652792063, 1652966193,\n",
       "         1653035412, 1653126769, 1654243334, 1654324912, 1654498475, 1654871704,\n",
       "         1655105736, 1655117981, 1655126712, 1655352753, 1655452053, 1656001332,\n",
       "         1656062585, 1656080301, 1656145979, 1656242739, 1656247348, 1656399189,\n",
       "         1656426587, 1656479417, 1656659404, 1656672907, 1656838668, 1656840807,\n",
       "         1656851466, 1656944269, 1656996652, 1657003769, 1657026126, 1657086541,\n",
       "         1657106297, 1657182021, 1657197416, 1657348316, 1657725666, 1657791441,\n",
       "         1657874712, 1657879921, 1658060102, 1658308700, 1658315743, 1658672601,\n",
       "         1658750361, 1658763214, 1659004684, 1659097885, 1659169247, 1659851611,\n",
       "         1662006556, 1662718250, 1663319136, 1664606121, 1664699896, 1664875933,\n",
       "         1665130748, 1665133068, 1665386778, 1665644114, 1667294844, 1667383518,\n",
       "         1667830408, 1670257113, 1670326585, 1670410301, 1670419636, 1670922297,\n",
       "         1671010368, 1672308946]),\n",
       " 'trx_amount': tensor([1.5275e+05, 3.6109e+04, 4.1386e+05, 6.7060e+03, 3.6923e+04, 3.1246e+04,\n",
       "         6.5112e+04, 2.1630e+05, 2.7987e+05, 1.5154e+06, 1.4437e+04, 4.3529e+04,\n",
       "         3.4833e+04, 7.2073e+04, 6.3942e+05, 1.5532e+05, 2.3549e+04, 1.0064e+06,\n",
       "         3.8518e+03, 4.2838e+06, 7.9882e+04, 1.5338e+05, 2.6260e+03, 1.1661e+06,\n",
       "         3.4551e+06, 1.4269e+05, 3.0834e+04, 5.8642e+05, 6.3570e+05, 5.0967e+04,\n",
       "         3.7644e+04, 1.5170e+06, 1.3355e+06, 8.7184e+04, 1.6635e+04, 2.0002e+06,\n",
       "         1.1968e+05, 3.3961e+04, 2.4111e+04, 1.5927e+03, 6.4985e+05, 7.2623e+05,\n",
       "         2.2105e+04, 2.4434e+06, 3.0459e+05, 6.4910e+04, 4.5123e+03, 1.8893e+05,\n",
       "         1.8317e+05, 1.7988e+04, 8.3615e+04, 8.6633e+05, 1.2327e+06, 3.9654e+04,\n",
       "         1.8945e+05, 4.0865e+04, 2.8778e+06, 2.7043e+06, 1.5009e+05, 3.0848e+05,\n",
       "         6.3291e+03, 1.9811e+04, 1.3090e+05, 8.4741e+01, 1.1579e+06, 1.7530e+03,\n",
       "         7.4285e+01, 1.7556e-01, 1.6041e+05, 2.4818e+04, 6.5040e+05, 4.9573e+02,\n",
       "         2.3753e+06, 8.4518e+04, 4.6187e+00, 7.5478e+05, 5.8547e+04, 2.0437e+06,\n",
       "         1.3392e+04, 5.1604e+03, 1.2440e+04, 5.1812e+04, 6.2135e+04, 1.9795e+01,\n",
       "         8.2933e+04, 9.7510e+03, 7.6212e+04, 2.4141e+05, 5.8929e+01, 3.2284e+04,\n",
       "         3.3764e+04, 9.3326e-02, 2.8078e+04, 1.6637e+05, 1.1008e+05, 3.5671e+04,\n",
       "         8.3880e+04, 6.3877e+02, 2.0006e+00, 5.6511e+04, 3.0167e+04, 1.2282e+03,\n",
       "         8.2103e+03, 7.5716e+01, 2.4871e+04, 4.2695e+03, 3.6752e+04, 2.9160e+05,\n",
       "         5.3824e+05, 2.0050e+03, 3.9895e+03, 9.9497e+02, 2.0309e+03, 8.4063e+01,\n",
       "         3.3330e+03, 2.6412e+03, 2.3024e+02, 3.6042e+03, 3.9765e+03, 1.8373e+03,\n",
       "         6.8419e+03, 1.4985e+01, 7.4746e+03, 6.5556e+01, 9.4320e+02, 9.5990e+04,\n",
       "         2.6829e+03, 1.5232e+05]),\n",
       " 'trx_event_type': tensor([ 2,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2,  2,  2,  1,  1,  1,  1,\n",
       "          1,  1,  2,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,  1,  1,  1,  1,\n",
       "          1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  6,  1, 15,  1,  3,  1,  1,  1,  1,  1,  3,\n",
       "          1, 14,  3,  1,  1,  1, 14, 11,  1,  1,  1, 14, 14, 14,  1,  1,  7,  1,\n",
       "          1, 18, 14,  1, 14,  1,  1,  1,  3,  1,  1,  6,  1,  3,  1, 14, 14, 14,\n",
       "         14, 14, 24, 14, 24,  1, 14, 14, 14, 14, 14, 14, 14, 14, 24, 14, 14,  1,\n",
       "         24,  1], dtype=torch.int32),\n",
       " 'trx_event_subtype': tensor([ 2,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2,  2,  2,  1,  1,  1,  1,\n",
       "          1,  1,  2,  1,  1,  1,  1,  2,  2,  1,  1,  2,  1,  1,  1,  1,  1,  1,\n",
       "          1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1, 15,  1,  3,  1,  1,  1,  1,  1,  3,\n",
       "          1, 13,  3,  1,  1,  1, 13, 12,  1,  1,  1, 13, 13, 13,  1,  1,  7,  1,\n",
       "          1, 19, 13,  1, 13,  1,  1,  1, 10,  1,  1,  6,  1,  3,  1, 13, 13, 13,\n",
       "         13, 13, 25, 13, 25,  1, 13, 13, 13, 13, 13, 13, 13, 13, 25, 13, 13,  1,\n",
       "         25,  1], dtype=torch.int32),\n",
       " 'trx_currency': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32),\n",
       " 'trx_src_type11': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 7, 1, 7, 7, 1, 1, 1, 1,\n",
       "         7, 5, 1, 1, 1, 7, 5, 1, 7, 1, 1, 5, 5, 5, 1, 7, 1, 7, 1, 1, 5, 7, 5, 1,\n",
       "         1, 1, 3, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 1, 5, 1], dtype=torch.int32),\n",
       " 'trx_src_type12': tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1, 15, 15,  1, 15, 15,  1,  1,  1,  1,\n",
       "         15, 10,  1,  1,  1, 15, 10,  1, 15,  1,  1, 10, 10, 10,  1, 15,  1, 15,\n",
       "          1,  1, 10, 15, 10,  1,  1,  1, 11,  1,  1,  1,  1,  1,  1, 10, 10, 18,\n",
       "         18, 18, 10, 10, 10,  1, 18, 10, 10, 10, 10, 10, 10, 10, 18, 18, 18,  1,\n",
       "         18,  1], dtype=torch.int32),\n",
       " 'trx_dst_type11': tensor([ 2,  1,  1,  1,  2,  2,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          1,  1,  2,  1,  2,  2,  1,  2,  2,  2,  2,  2,  1,  2,  1,  1, 11,  1,\n",
       "          2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  2,  1,  1,  1,  1,\n",
       "          2,  2,  1,  1,  1,  1,  2,  2,  1,  1,  1,  3,  1,  1,  2,  1,  1,  3,\n",
       "          1,  8,  6,  2,  2,  1,  8,  1,  1,  1,  1,  8,  8,  8,  1,  1,  4,  1,\n",
       "          1, 12,  8,  1,  8,  1,  2,  1,  3,  2,  1,  4,  2,  3,  2,  8,  8,  8,\n",
       "          8,  8, 14,  8, 14,  1,  8,  8,  8,  8,  8,  8,  8,  8, 14,  8,  8,  2,\n",
       "         14,  1], dtype=torch.int32),\n",
       " 'trx_dst_type12': tensor([ 2,  1,  1,  1,  2,  2,  1,  1,  1,  1,  4,  2,  2,  2,  4,  4,  4,  4,\n",
       "          1,  1,  2,  1,  4,  4,  1,  2,  2,  4,  4,  2,  1,  4,  1,  1, 23,  1,\n",
       "          2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  4,  1,  4,  1,  1,  1,  1,\n",
       "          4,  4,  1,  1,  1,  1,  4,  2,  1,  1,  1,  3,  1,  1,  2,  1,  1,  3,\n",
       "          1,  9,  7,  2,  4,  1,  9,  1,  1,  1,  1,  9,  9,  9,  1,  1,  5,  1,\n",
       "          1, 16,  9,  1,  9,  1,  4,  1,  3,  4,  1,  5,  2,  3,  4,  9,  9,  9,\n",
       "          9,  9, 31,  9, 31,  1,  9,  9,  9,  9,  9,  9,  9,  9, 32,  9,  9,  2,\n",
       "         32,  1], dtype=torch.int32),\n",
       " 'trx_src_type22': tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15], dtype=torch.int32),\n",
       " 'trx_src_type32': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2], dtype=torch.int32),\n",
       " 'dial_event_time': tensor([1656325210, 1656408674]),\n",
       " 'dial_embedding': tensor([[ 0.3576, -0.2206,  0.5902,  ..., -0.4524,  0.5307,  0.3897],\n",
       "         [ 0.0693, -0.0656,  0.2024,  ..., -0.1139,  0.3511,  0.0432]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.feature_dict import FeatureDict\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "class MultiModalDiffSplitDataset(FeatureDict, torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        splitters,\n",
    "        source_features,\n",
    "        col_id,\n",
    "        source_names,\n",
    "        col_time='event_time',\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataset for multimodal learning.\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data:\n",
    "            concatinated data with feature dicts.\n",
    "        splitter:\n",
    "            object from from `ptls.frames.coles.split_strategy`.\n",
    "            Used to split original sequence into subsequences which are samples from one client.\n",
    "        source_features:\n",
    "            list of column names \n",
    "        col_id:\n",
    "            column name with user_id\n",
    "        source_names:\n",
    "            column name with name sources\n",
    "        col_time:\n",
    "            column name with event_time\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.data = data\n",
    "        self.splitters = splitters\n",
    "        self.col_time = col_time\n",
    "        self.col_id = col_id\n",
    "        self.source_names = source_names\n",
    "        self.source_features = source_features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature_arrays = self.data[idx]\n",
    "        split_data = self.split_source(feature_arrays)\n",
    "        # print(self.get_split(split_data))\n",
    "        return self.get_splits(split_data)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for feature_arrays in self.data:\n",
    "            split_data = self.split_source(feature_arrays)\n",
    "            yield self.get_splits(split_data)\n",
    "            \n",
    "    def split_source(self, feature_arrays):\n",
    "        res = defaultdict(dict)\n",
    "        for feature_name, feature_array in feature_arrays.items():\n",
    "            if feature_name == self.col_id:\n",
    "                res[self.col_id] = feature_array\n",
    "            else:\n",
    "                source_name, feature_name_transform = self.get_names(feature_name)\n",
    "                res[source_name][feature_name_transform] = feature_array\n",
    "        for source in self.source_names:\n",
    "            if source not in res:\n",
    "                res[source] = {source_feature: torch.tensor([]) for source_feature in self.source_features[source]}\n",
    "        # print(f'res = {res}')\n",
    "        return res\n",
    "    \n",
    "    def get_names(self, feature_name):\n",
    "        idx_del = feature_name.find('_')\n",
    "        return feature_name[:idx_del], feature_name[idx_del + 1:]\n",
    "    \n",
    "    def get_splits(self, feature_arrays):\n",
    "        res = {}\n",
    "        for source_name, feature_array in feature_arrays.items():\n",
    "            if source_name != self.col_id:\n",
    "                local_date = feature_array[self.col_time]\n",
    "                if source_name not in self.splitters:\n",
    "                    continue\n",
    "                indexes = self.splitters[source_name].split(local_date)\n",
    "                res[source_name] = [{k: v[ix] for k, v in feature_array.items() if self.is_seq_feature(k, v)} for ix in indexes]\n",
    "        return res\n",
    "    #Вернуть диалоги с транзакциями (без таргетов)\n",
    "    def collate_fn(self, batch, return_dct_labels=False):\n",
    "        dict_class_labels = get_dict_class_labels(batch)\n",
    "        batch = reduce(lambda x, y: {k: x[k] + y[k] for k in x if k in y}, batch)\n",
    "        padded_batch = collate_multimodal_feature_dict(batch)\n",
    "        if return_dct_labels:\n",
    "            return padded_batch, dict_class_labels\n",
    "        return padded_batch, dict_class_labels[list(dict_class_labels.keys())[0]]\n",
    "\n",
    "def collate_multimodal_feature_dict(batch):\n",
    "    res = {}\n",
    "    for source, source_batch in batch.items():\n",
    "        res[source] = collate_feature_dict(source_batch)\n",
    "    # print(f\"multimodal_feature_dict = {res['trx'].payload['event_time'].size()}\")\n",
    "    # print()\n",
    "    return res\n",
    "\n",
    "def collate_feature_dict(batch):\n",
    "    new_x_ = defaultdict(list)\n",
    "    for i, x in enumerate(batch):\n",
    "        for k, v in x.items():\n",
    "            new_x_[k].append(v)\n",
    "    \n",
    "    seq_col = next(k for k, v in batch[0].items() if FeatureDict.is_seq_feature(k, v))\n",
    "    lengths = torch.LongTensor([len(rec[seq_col]) for rec in batch])\n",
    "    new_x = {}\n",
    "    for k, v in new_x_.items():\n",
    "        # print(new_x)\n",
    "        if type(v[0]) is torch.Tensor:\n",
    "            if k.startswith('target'):\n",
    "                new_x[k] = torch.stack(v, dim=0)\n",
    "            else:\n",
    "                new_x[k] = torch.nn.utils.rnn.pad_sequence(v, batch_first=True)\n",
    "        elif type(v[0]) is np.ndarray:\n",
    "            new_x[k] = v  # list of arrays[object]\n",
    "        else:\n",
    "            v = np.array(v)\n",
    "            if v.dtype.kind == 'i':\n",
    "                new_x[k] = torch.from_numpy(v).long()\n",
    "            elif v.dtype.kind == 'f':\n",
    "                new_x[k] = torch.from_numpy(v).float()\n",
    "            elif v.dtype.kind == 'b':\n",
    "                new_x[k] = torch.from_numpy(v).bool()\n",
    "            else:\n",
    "                new_x[k] = v\n",
    "    return PaddedBatch(new_x, lengths)\n",
    "    \n",
    "def get_dict_class_labels(batch):\n",
    "    res = defaultdict(list)\n",
    "    for i, samples in enumerate(batch):\n",
    "        for source, values in samples.items():\n",
    "            for _ in values:\n",
    "                res[source].append(i)\n",
    "    for source in res:\n",
    "        res[source] = torch.LongTensor(res[source])\n",
    "    return dict(res)\n",
    "\n",
    "class MultiModalDiffSplitIterableDataset(MultiModalDiffSplitDataset, torch.utils.data.IterableDataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.frames.coles import MultiModalIterableDataset\n",
    "import ptls\n",
    "\n",
    "train_data = MultiModalDiffSplitIterableDataset(\n",
    "        data=train,\n",
    "        splitters= {\n",
    "            'trx': SampleSlices(\n",
    "                split_count=3,\n",
    "                cnt_min=16,\n",
    "                cnt_max=90\n",
    "            ),\n",
    "            'dial': SampleSlices(\n",
    "                split_count=3,\n",
    "                cnt_min=2,\n",
    "                cnt_max=10\n",
    "            ),\n",
    "        },\n",
    "        source_features={\n",
    "            \"trx\": [\n",
    "                \"event_type\",\n",
    "                \"event_subtype\",\n",
    "                \"src_type11\",\n",
    "                \"src_type12\",\n",
    "                \"dst_type11\",\n",
    "                \"dst_type12\",\n",
    "                \"src_type22\",\n",
    "                \"src_type32\"\n",
    "            ],\n",
    "            \"dial\": [\n",
    "                \"embedding\"\n",
    "            ],\n",
    "        },\n",
    "        col_id='client_id',\n",
    "        col_time='event_time',\n",
    "        source_names=['trx', 'dial'],\n",
    "    )\n",
    "\n",
    "valid_data = MultiModalDiffSplitIterableDataset(\n",
    "        data=valid,\n",
    "        splitters= {\n",
    "            'trx': SampleSlices(\n",
    "                split_count=2,\n",
    "                cnt_min=5,\n",
    "                cnt_max=64\n",
    "            ),\n",
    "            'dial': SampleSlices(\n",
    "                split_count=2,\n",
    "                cnt_min=2,\n",
    "                cnt_max=10\n",
    "            ),\n",
    "            },\n",
    "        source_features={\n",
    "            \"trx\": [\n",
    "                \"event_type\",\n",
    "                \"event_subtype\",\n",
    "                \"src_type11\",\n",
    "                \"src_type12\",\n",
    "                \"dst_type11\",\n",
    "                \"dst_type12\",\n",
    "                \"src_type22\",\n",
    "                \"src_type32\"\n",
    "            ],\n",
    "            \"dial\": [\n",
    "                \"embedding\"\n",
    "            ],\n",
    "        },\n",
    "        col_id='client_id',\n",
    "        col_time='event_time',\n",
    "        source_names=['trx', 'dial'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_val=0.5,\n",
    "    log_every_n_steps=50,\n",
    "    limit_val_batches=36\n",
    ")\n",
    "\n",
    "data_module = PtlsDataModule(\n",
    "    train_data=train_data,\n",
    "    valid_data=valid_data,\n",
    "    train_batch_size=128,\n",
    "    train_num_workers=4,\n",
    "    valid_batch_size=256,\n",
    "    valid_num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'generator' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_fork.py:20\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'generator' object"
     ]
    }
   ],
   "source": [
    "next(iter(data_module.train_dl(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.frames.abs_module import ABSModule\n",
    "from ptls.frames.coles.metric import metric_recall_top_K, outer_cosine_similarity, outer_pairwise_distance\n",
    "from ptls.frames.coles.losses import ContrastiveLoss\n",
    "\n",
    "def first(iterable, default=None):\n",
    "    iterator = iter(iterable)\n",
    "    return next(iterator, default)\n",
    "\n",
    "\n",
    "def metric_real_recall_top_K(X, y, K, num_pos=1, metric='cosine'):\n",
    "    \"\"\"\n",
    "        calculate metric R@K\n",
    "        X - tensor with size n x d, where n - number of examples, d - size of embedding vectors\n",
    "        y - true labels\n",
    "        N - count of closest examples, which we consider for recall calcualtion\n",
    "        metric: 'cosine' / 'euclidean'.\n",
    "            !!! 'euclidean' - to slow for datasets bigger than 100K rows\n",
    "    \"\"\"\n",
    "    # TODO: take K from `y`\n",
    "    K_adjusted = min(X.size(0) - 1, K)\n",
    "    \n",
    "    res = []\n",
    "\n",
    "    n = X.size(0)\n",
    "    d = X.size(1)\n",
    "    max_size = 2 ** 32\n",
    "    batch_size = max(1, max_size // (n * d))\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(1 + (len(X) - 1) // batch_size):\n",
    "\n",
    "            id_left = i * batch_size\n",
    "            id_right = min((i + 1) * batch_size, len(y))\n",
    "            y_batch = y[id_left:id_right]\n",
    "\n",
    "            if metric == 'cosine':\n",
    "                pdist = -1 * outer_cosine_similarity(X, X[id_left:id_right])\n",
    "            elif metric == 'euclidean':\n",
    "                pdist = outer_pairwise_distance(X, X[id_left:id_right])\n",
    "            else:\n",
    "                raise AttributeError(f'wrong metric \"{metric}\"')\n",
    "\n",
    "            values, indices = pdist.topk(K_adjusted + 1, 0, largest=False)\n",
    "\n",
    "            y_rep = y_batch.repeat(K_adjusted, 1)\n",
    "            res.append((y[indices[1:]] == y_rep).sum().item())\n",
    "\n",
    "    return np.sum(res) / len(y) / num_pos\n",
    "\n",
    "\n",
    "def cosine_similarity_matrix(x1, x2):\n",
    "    x1_norm = x1 / x1.norm(dim=1)[:, None]\n",
    "    x2_norm = x2 / x2.norm(dim=1)[:, None]\n",
    "    return torch.mm(x1_norm, x2_norm.transpose(0, 1))\n",
    "\n",
    "\n",
    "def metric_recall_top_K_for_embs(embs_1, embs_2, true_matches, K=100):\n",
    "    similarity_matrix = cosine_similarity_matrix(embs_1, embs_2)\n",
    "    K_adjusted = min(len(embs_1), K)\n",
    "    top_k = similarity_matrix.topk(k=K_adjusted, dim=1).indices\n",
    "    correct_matches = 0\n",
    "    for i, indices in enumerate(top_k):\n",
    "        if true_matches[i] in indices:\n",
    "            correct_matches += 1\n",
    "    recall_at_k = correct_matches / len(similarity_matrix)\n",
    "    return recall_at_k\n",
    "\n",
    "\n",
    "class M3CoLESModule(ABSModule):\n",
    "    \"\"\"\n",
    "    Multi-Modal Matching\n",
    "    Contrastive Learning for Event Sequences ([CoLES](https://arxiv.org/abs/2002.08232))\n",
    "\n",
    "    Subsequences are sampled from original sequence.\n",
    "    Samples from the same sequence are `positive` examples\n",
    "    Samples from the different sequences are `negative` examples\n",
    "    Embeddings for all samples are calculated.\n",
    "    Paired distances between all embeddings are calculated.\n",
    "    The loss function tends to make positive distances smaller and negative ones larger.\n",
    "\n",
    "    Parameters\n",
    "        seq_encoder:\n",
    "            Model which calculate embeddings for original raw transaction sequences\n",
    "            `seq_encoder` is trained by `CoLESModule` to get better representations of input sequences\n",
    "        head:\n",
    "            Model which helps to train. Not used during inference\n",
    "            Can be normalisation layer which make embedding l2 length equals 1\n",
    "            Can be MLP as `projection head` like in SymCLR framework.\n",
    "        loss:\n",
    "            loss object from `ptls.frames.coles.losses`.\n",
    "            There are paired and triplet loss. They are required sampling strategy\n",
    "            from `ptls.frames.coles.sampling_strategies`. Sampling strategy takes a relevant pairs or triplets from\n",
    "            pairwise distance matrix.\n",
    "        validation_metric:\n",
    "            Keep None. `ptls.frames.coles.metric.BatchRecallTopK` used by default.\n",
    "        optimizer_partial:\n",
    "            optimizer init partial. Network parameters are missed.\n",
    "        lr_scheduler_partial:\n",
    "            scheduler init partial. Optimizer are missed.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 seq_encoders=None,\n",
    "                 mod_names=None,\n",
    "                 head=None,\n",
    "                 loss=None,\n",
    "                 validation_metric=None,\n",
    "                 optimizer_partial=None,\n",
    "                 lr_scheduler_partial=None):\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "        if head is None:\n",
    "            head = Head(use_norm_encoder=True)\n",
    "\n",
    "        if loss is None:\n",
    "            loss = ContrastiveLoss(margin=0.5,\n",
    "                                   sampling_strategy=HardNegativePairSelector(neg_count=5))\n",
    "\n",
    "        if validation_metric is None:\n",
    "            validation_metric = BatchRecallTopK(K=4, metric='cosine')\n",
    "        \n",
    "        for k in seq_encoders.keys():\n",
    "            if type(seq_encoders[k]) is str:\n",
    "                seq_encoders[k] = seq_encoders[seq_encoders[k]]\n",
    "                \n",
    "        super().__init__(validation_metric,\n",
    "                         first(seq_encoders.values()),\n",
    "                         loss,\n",
    "                         optimizer_partial,\n",
    "                         lr_scheduler_partial)\n",
    "        \n",
    "        self.seq_encoders = torch.nn.ModuleDict(seq_encoders)\n",
    "        self._head = head   \n",
    "        self.y_h_cache = {'train':[], 'valid': []}\n",
    "        \n",
    "    @property\n",
    "    def metric_name(self):\n",
    "        return 'recall_top_k'\n",
    "\n",
    "    @property\n",
    "    def is_requires_reduced_sequence(self):\n",
    "        return True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = {}\n",
    "        \n",
    "        for mod_name in x.keys():\n",
    "            # print(f'mod_name = {mod_name}')\n",
    "            res[mod_name] = self.seq_encoders[mod_name](x[mod_name])\n",
    "        # print(f\"forward res = {res['trx'].size()}\")\n",
    "        return res\n",
    "\n",
    "    def shared_step(self, x, y):\n",
    "        y_h = self(x)\n",
    "        \n",
    "        if self._head is not None:\n",
    "            y_h_head = {k: self._head(y_h_k) for k, y_h_k in y_h.items()}\n",
    "            y_h = y_h_head\n",
    "        # print('share_step........')\n",
    "        # print(f\"y_h = {y_h['trx'].size()}\")\n",
    "        # print(f'y = {y.size()}')\n",
    "        # print('-----------------------')\n",
    "        return y_h, y\n",
    "    \n",
    "    def _one_step(self, batch, _, stage):\n",
    "        y_h, y = self.shared_step(*batch)\n",
    "        y_h_list = list(y_h.values())\n",
    "        # print(\"loss count.....\")\n",
    "        # print(f\"y_h_list_obj = {y_h_list[0].size()}\")\n",
    "        # print(f\"y_true_loss = {torch.cat([y, y]).size()}\")\n",
    "        # print(\"--------------------------\")\n",
    "        loss = self._loss(torch.cat(y_h_list), torch.cat([y, y]))\n",
    "        self.log(f'loss/{stage}', loss.detach())\n",
    "        \n",
    "        x, y = batch\n",
    "        # print(\"step.............\")\n",
    "        # print(f\"x_step = {x['trx'].payload['event_time'].size()}\")\n",
    "        # print(f\"y_step_true = {y.size()}\")\n",
    "        # print(f\"y_h_step = {y_h['trx'].size()}\")\n",
    "        # print(\"-----------------------\")\n",
    "        for mod_name, mod_x in x.items():\n",
    "            self.log(f'seq_len/{stage}/{mod_name}', x[mod_name].seq_lens.float().mean().detach(), prog_bar=True)\n",
    "        \n",
    "        if stage == \"valid\":\n",
    "            n, d = y_h_list[0].shape\n",
    "            y_h_concat = torch.zeros((2*n, d), device = y_h_list[0].device)\n",
    "            \n",
    "            for i in range(2):\n",
    "                y_h_concat[range(i,2*n,2)] = y_h_list[i] \n",
    "            # print('valid.......')\n",
    "            # print(f'y_h_concat = {y_h_concat[0].size}')\n",
    "            # print(f\"y_h_valid = {y_h['trx'].size()}\")\n",
    "            # print('------------------------')\n",
    "            if len(self.y_h_cache[stage]) <= 380:\n",
    "                self.y_h_cache[stage].append((y_h_concat.cpu(), {k: y_h_k.cpu() for k, y_h_k in y_h.items()} , \n",
    "                                             {k:x_k.seq_lens.cpu() for k, x_k in x.items()})) \n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, _):\n",
    "        return self._one_step(batch, _, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, _):\n",
    "        return self._one_step(batch, _, \"valid\")\n",
    "    \n",
    "    def on_validation_epoch_end(self):        \n",
    "        #len_intervals = [(0, 10), (10, 20), (20, 30), (30, 40), (40, 60), (60, 80), (80, 120), (120, 160), (160, 240)]\n",
    "        self.log_recall_top_K(self.y_h_cache['valid'], len_intervals=None, stage=\"valid\", K=100)\n",
    "        self.log_recall_top_K(self.y_h_cache['valid'], len_intervals=None, stage=\"valid\", K=50)\n",
    "        self.log_recall_top_K(self.y_h_cache['valid'], len_intervals=None, stage=\"valid\", K=1)\n",
    "        \n",
    "        \n",
    "        del self.y_h_cache[\"valid\"]\n",
    "        self.y_h_cache[\"valid\"] = []\n",
    "        \n",
    "    def log_recall_top_K(self, y_h_cache, len_intervals=None, stage=\"valid\", K=100):\n",
    "        y_h = torch.cat([item[0] for item in y_h_cache], dim = 0)\n",
    "        y_h_mods = defaultdict(list)\n",
    "        seq_lens_dict = defaultdict(list)\n",
    "        \n",
    "        for item in y_h_cache:\n",
    "            for k, emb in item[1].items():\n",
    "                y_h_mods[k].append(emb)\n",
    "                \n",
    "            for k, l in item[2].items():\n",
    "                seq_lens_dict[k].append(l)\n",
    "        \n",
    "        y_h_mods = {k: torch.cat(el, dim=0) for k ,el in y_h_mods.items()}\n",
    "        seq_lens_dict = {k: torch.cat(el) for k ,el in seq_lens_dict.items()}\n",
    "\n",
    "        #n, _ = y_h.shape\n",
    "        #y = torch.zeros((n,)).cpu().long()\n",
    "        #y[range(0,n,2)] = torch.arange(0, n//2)\n",
    "        #y[range(1,n,2)] = torch.arange(0, n//2)\n",
    "        #computed_metric = metric_real_recall_top_K(y_h, y, K=100)\n",
    "        y_h_bank, y_h_rmb = list(y_h_mods.values())\n",
    "        computed_metric_b2r = metric_recall_top_K_for_embs(y_h_bank, y_h_rmb, torch.arange(y_h_rmb.shape[0]), K=K)\n",
    "        computed_metric_r2b = metric_recall_top_K_for_embs(y_h_rmb, y_h_bank, torch.arange(y_h_rmb.shape[0]), K=K)\n",
    "        \n",
    "        if len_intervals != None:\n",
    "            for mod, seq_lens in seq_lens_dict.items():\n",
    "                for start, end in len_intervals:\n",
    "                    mask = ((seq_lens > start) & (seq_lens <= end))\n",
    "\n",
    "                    if torch.any(mask):\n",
    "                        #y_h_filtered = y_h[mask.repeat_interleave(2)]\n",
    "                        y_h_bank_filtered = y_h_bank[mask]\n",
    "                        y_h_rmb_filtered = y_h_rmb[mask]\n",
    "\n",
    "                        #y = torch.div(torch.arange(len(y_h_filtered)), 2, rounding_mode='floor')\n",
    "                        #recall = metric_real_recall_top_K(y_h_filtered, y, K=100)\n",
    "                        recall_r2b = metric_recall_top_K_for_embs(y_h_rmb_filtered, y_h_bank_filtered, torch.arange(y_h_rmb_filtered.shape[0]), K=100)\n",
    "                        recall_b2r = metric_recall_top_K_for_embs(y_h_bank_filtered, y_h_rmb_filtered, torch.arange(y_h_rmb_filtered.shape[0]), K=100)\n",
    "\n",
    "                        #self.log(f\"{mode}/R@100_len_from_{start}_to_{end}\", recall, prog_bar=True)\n",
    "                        print(f\"{stage}/{mod}/r2b_R@100_len_from_{start}_to_{end}\", recall_r2b, prog_bar=True)\n",
    "                        self.log(f\"{stage}/{mod}/b2r_R@100_len_from_{start}_to_{end}\", recall_b2r, prog_bar=True)\n",
    "        \n",
    "        #self.log(f\"{mode}/R@100\", computed_metric, prog_bar=True)\n",
    "        self.log(f\"{stage}/click2trx_R@{K}\", computed_metric_r2b, prog_bar=True)\n",
    "        self.log(f\"{stage}/trx2click_R@{K}\", computed_metric_b2r, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.nn import TrxEncoder, RnnSeqEncoder\n",
    "from ptls.frames.coles import CoLESModule\n",
    "from functools import partial\n",
    "import torch\n",
    "from ptls.frames.coles import MultiModalSortTimeSeqEncoderContainer\n",
    "from ptls.nn.trx_encoder.encoders import IdentityEncoder\n",
    "from ptls.nn.seq_encoder.rnn_encoder import RnnEncoder\n",
    "\n",
    "head = ptls.nn.Head(\n",
    "    input_size=128,\n",
    "    use_norm_encoder=True,\n",
    "    hidden_layers_sizes=[128, 128],\n",
    "    objective=\"regression\",\n",
    "    num_classes=128\n",
    ")\n",
    "\n",
    "loss = ptls.frames.coles.losses.SoftmaxLoss()\n",
    "\n",
    "seq_encoders = {\n",
    "    'trx': RnnSeqEncoder(\n",
    "        trx_encoder=TrxEncoder(\n",
    "            norm_embeddings=False,\n",
    "            embeddings_noise=0.003,\n",
    "            linear_projection_size=32,\n",
    "            embeddings={\n",
    "                'event_type': {\"in\": 58, \"out\": 24},\n",
    "                'event_subtype': {\"in\": 59, \"out\": 24},\n",
    "                'src_type11': {\"in\": 85, \"out\": 24},\n",
    "                'src_type12': {\"in\": 349, \"out\": 24},\n",
    "                'dst_type11': {\"in\": 84, \"out\": 24},\n",
    "                'dst_type12': {\"in\": 417, \"out\": 24},\n",
    "                'src_type22': {\"in\": 90, \"out\": 24},\n",
    "                'src_type32': {\"in\": 91, \"out\": 24}\n",
    "            },\n",
    "            numeric_values={\n",
    "                'amount': 'log'\n",
    "            }\n",
    "        ),\n",
    "        type='gru',\n",
    "        hidden_size=128\n",
    "    ),\n",
    "    'dial': RnnSeqEncoder(\n",
    "        trx_encoder=TrxEncoder(\n",
    "            embeddings_noise=0.003,\n",
    "            linear_projection_size=32,\n",
    "            custom_embeddings={\n",
    "                'embedding': IdentityEncoder(768)\n",
    "            }\n",
    "        ),\n",
    "        type='gru',\n",
    "        hidden_size=128\n",
    "    )\n",
    "}\n",
    "\n",
    "optimizer_partial = partial(\n",
    "    torch.optim.AdamW,\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "lr_scheduler_partial = partial(\n",
    "    torch.optim.lr_scheduler.StepLR,\n",
    "    step_size=1,\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "pl_module = M3CoLESModule(\n",
    "    validation_metric=ptls.frames.coles.metric.BatchRecallTopK(\n",
    "        K=1,\n",
    "        metric='cosine',\n",
    "    ),\n",
    "    head=head,\n",
    "    seq_encoders=seq_encoders,\n",
    "    loss=loss,\n",
    "    optimizer_partial=optimizer_partial,\n",
    "    lr_scheduler_partial=lr_scheduler_partial\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name               | Type            | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | _loss              | SoftmaxLoss     | 0      | train\n",
      "1 | _seq_encoder       | RnnSeqEncoder   | 98.1 K | train\n",
      "2 | _validation_metric | BatchRecallTopK | 0      | train\n",
      "3 | seq_encoders       | ModuleDict      | 186 K  | train\n",
      "4 | _head              | Head            | 49.5 K | train\n",
      "---------------------------------------------------------------\n",
      "236 K     Trainable params\n",
      "0         Non-trainable params\n",
      "236 K     Total params\n",
      "0.945     Total estimated model params size (MB)\n",
      "49        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |                                                                                                                | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: | | 206/? [00:37<00:00,  5.48it/s, v_num=0, seq_len/train/trx=48.70, seq_len/train/dial=2.620, seq_len/valid/trx=31.90, seq_len/valid/dial=2.89"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x112b2acf0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: | | 73/? [00:13<00:00,  5.54it/s, v_num=0, seq_len/train/trx=48.00, seq_len/train/dial=2.800, seq_len/valid/trx=32.00, seq_len/valid/dial=2.880"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/loops/training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:171\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/core/module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1302\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/core/optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/plugins/precision/precision.py:123\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/optim/adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 220\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/plugins/precision/precision.py:109\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/loops/optimization/automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/loops/optimization/automatic.py:131\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 131\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/loops/optimization/automatic.py:319\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 319\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/strategies/strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[63], line 201\u001b[0m, in \u001b[0;36mM3CoLESModule.training_step\u001b[0;34m(self, batch, _)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, _):\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[63], line 166\u001b[0m, in \u001b[0;36mM3CoLESModule._one_step\u001b[0;34m(self, batch, _, stage)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_one_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, _, stage):\n\u001b[0;32m--> 166\u001b[0m     y_h, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     y_h_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(y_h\u001b[38;5;241m.\u001b[39mvalues())\n",
      "Cell \u001b[0;32mIn[63], line 154\u001b[0m, in \u001b[0;36mM3CoLESModule.shared_step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshared_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[0;32m--> 154\u001b[0m     y_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[63], line 149\u001b[0m, in \u001b[0;36mM3CoLESModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod_name \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# print(f'mod_name = {mod_name}')\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     res[mod_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_encoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# print(f\"forward res = {res['trx'].size()}\")\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ptls/nn/seq_encoder/containers.py:125\u001b[0m, in \u001b[0;36mRnnSeqEncoder.forward\u001b[0;34m(self, x, h_0)\u001b[0m\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrx_encoder(x)\n\u001b[0;32m--> 125\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/ptls/nn/seq_encoder/rnn_encoder.py:136\u001b[0m, in \u001b[0;36mRnnEncoder.forward\u001b[0;34m(self, x, h_0)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgru\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/nn/modules/rnn.py:1393\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1393\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1397\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.fit(pl_module, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "ml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
