{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"179edf6d50774ea8823653a673ac997b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a9f4a84ca6e4954995fa850077eb0a8","max":375,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d12dd2772b6946a093dc5974d1aa5a9d","value":375}},"1a9f4a84ca6e4954995fa850077eb0a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40ee75066543437cb4c17128bd9855f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a4b263221944879a783e4ba7f6a7b09","placeholder":"​","style":"IPY_MODEL_7cf17338700143c8b905c0d784925c87","value":" 375/375 [26:26&lt;00:00,  4.23s/it]"}},"41e9d55b47cc4ff8b2f33f63c16d3f67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"5a4b263221944879a783e4ba7f6a7b09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7217a51f613f43fc9cc2501c47511049":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a0591b832ff460b923b935fc4b5f6c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c672d54f30fe46af82a7bcacab95e0eb","IPY_MODEL_179edf6d50774ea8823653a673ac997b","IPY_MODEL_40ee75066543437cb4c17128bd9855f8"],"layout":"IPY_MODEL_41e9d55b47cc4ff8b2f33f63c16d3f67"}},"7cf17338700143c8b905c0d784925c87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c672d54f30fe46af82a7bcacab95e0eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7217a51f613f43fc9cc2501c47511049","placeholder":"​","style":"IPY_MODEL_e209a754fca54141b4eaf5ffdaefbf5e","value":"Predicting DataLoader 0: 100%"}},"d12dd2772b6946a093dc5974d1aa5a9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e209a754fca54141b4eaf5ffdaefbf5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11735172,"sourceType":"datasetVersion","datasetId":7367023}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"930901fd-b4d4-4bf9-b769-7c26673f85d3","cell_type":"code","source":"#Commands for downgrading Python version in Kaggle notebook\n\n#adds external APT repository\n!add-apt-repository -y 'ppa:deadsnakes/ppa'\n\n#installs specific python version\n!apt install -y python3.10\n\n#installs distutils package for the specific python version\n!apt-get install -y python3.10-distutils\n\n#Install Python modules using the newly installed Python version\n#-I, Ignores and overwrites the installed packages.\n!sudo /usr/bin/python3.10 -m pip install -Iv <package-name>\n\n#Run Python scripts using the newly installed Python version\n!/usr/bin/python3.10 <python_script.py>","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"184df581-add0-479c-a484-e5cdc7b768ec","cell_type":"code","source":"!mkdir -p data\n!mkdir -p predictions\n!mkdir -p embeddings\n!mkdir -p models\n!cd data/\n\n!curl -OL https://storage.yandexcloud.net/ds-ods/files/data/docs/competitions/DataFusion2024/Data/clients.csv\n!curl -OL https://storage.yandexcloud.net/ds-ods/files/data/docs/competitions/DataFusion2024/Data/train.csv\n!curl -OL https://storage.yandexcloud.net/ds-ods/files/data/docs/competitions/DataFusion2024/Data/report_dates.csv\n!curl -OL https://storage.yandexcloud.net/ds-ods/files/data/docs/competitions/DataFusion2024/Data/transactions.csv.zip\n!curl -OL https://storage.yandexcloud.net/ds-ods/files/data/docs/competitions/DataFusion2024/Data/sample_submit_naive.csv\n\n!unzip transactions.csv.zip\n\n!rm transactions.csv.zip\n\n!cd ../\n\n# python -m get_test_ids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"99cfbc29-7034-4045-b3f8-f73854d55310","cell_type":"code","source":"!pip install pyspark\n!pip install lightning\n!pip install pytorch-lifestream","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7ba386a0-b1a9-430c-93b5-0289b85f550c","cell_type":"code","source":"# !python --version","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cdc63532-5b3d-4fc6-b758-059f2fd300e2","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os","metadata":{"id":"cdc63532-5b3d-4fc6-b758-059f2fd300e2","trusted":true},"outputs":[],"execution_count":null},{"id":"12f47e69-672f-4a9e-95c4-3d0dd3efccfe","cell_type":"code","source":"import wandb\n\nwandb.login(key=\"79f2120f8d4212aceb2c60b3c89a1b6727c19cff\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8932504b-29c7-475a-a15a-0a778d9ea931","cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"id":"574daafc-7c2f-44f4-ae74-0baf654ce049","cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nmain = pd.read_csv('/kaggle/input/datafusion2024-contest/train.csv')\ntrain = main[main.target != -1]\ntrain_, test_ = train_test_split(train, random_state=42, test_size=0.2)\ntest_[['user_id']].to_csv('data/test_ids.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a10e56c5-a61d-4734-8347-63f78d79f9a6","cell_type":"code","source":"train_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"91caba9c-7cd7-48a2-b198-599ab111e3d1","cell_type":"code","source":"transactions_df = pd.read_csv('/kaggle/input/datafusion2024-contest/transactions.csv')\nclients_df = pd.read_csv('/kaggle/input/datafusion2024-contest/clients.csv')\nreports_df = pd.read_csv('/kaggle/input/datafusion2024-contest/report_dates.csv')","metadata":{"id":"91caba9c-7cd7-48a2-b198-599ab111e3d1","trusted":true},"outputs":[],"execution_count":null},{"id":"32d2e681-512d-4213-aca6-9a68d657e72a","cell_type":"code","source":"# train_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"871f7349-2fbd-43e0-a8cb-9c862d0870fd","cell_type":"code","source":"# transactions_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"037c4ae3-d028-45f3-b6f5-9aff51c69c1f","cell_type":"code","source":"transactions_df['transaction_dttm'] = pd.to_datetime(transactions_df.transaction_dttm)\nreport_dates = pd.read_csv('/kaggle/input/datafusion2024-contest/report_dates.csv', parse_dates=['report_dt'])\ndf_ = transactions_df.merge(clients_df[['user_id', 'report']], how='left', on='user_id')\ndf_ = df_.merge(report_dates, how='left', on='report')\ntransactions_df['days_to_report'] = (df_['report_dt'] - df_['transaction_dttm']).dt.days","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e7de392a-d56e-4a8d-b398-3ca17ea734d0","cell_type":"code","source":"# Добавляем количество дней, часов с момента первой и предыдущей транзакций\nfirst_trx = transactions_df.groupby('user_id')['transaction_dttm'].min().reset_index()\nfirst_trx.rename(columns={'transaction_dttm': 'first_tr'}, inplace=True)\ntransactions_df = transactions_df.merge(first_trx, on='user_id', how='left')\n\ntransactions_df['days_from_first_tr'] = (transactions_df['transaction_dttm']-transactions_df['first_tr'])/ np.timedelta64(1, 'D')\ntransactions_df['days_from_first_tr'] = (transactions_df['days_from_first_tr']).astype('int')\ntransactions_df['days_from_prev_tr'] = transactions_df['transaction_dttm'].diff()/ np.timedelta64(1, 'D')\ntransactions_df['days_from_prev_tr'] = transactions_df['days_from_prev_tr'].fillna(0)\n\ntransactions_df['days_from_prev_tr'] = (transactions_df['days_from_prev_tr']).astype('int')\n\ntransactions_df['hours_from_first_tr'] = (transactions_df['transaction_dttm']-transactions_df['first_tr'])/ np.timedelta64(1, 'h')\ntransactions_df['hours_from_prev_tr'] = transactions_df['transaction_dttm'].diff()/ np.timedelta64(1, 'h')\ntransactions_df['hours_from_prev_tr'] = transactions_df['hours_from_prev_tr'].fillna(0)\n\ntransactions_df = transactions_df.drop(columns=['first_tr'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"43b3f46d-e579-48b3-b443-e013f62b5d67","cell_type":"code","source":"# Кодируем день недели, добавляем флаг выходного дня\ndays_of_week = {'Monday': 1,\n                'Tuesday': 2,\n                'Wednesday': 3,\n                'Thursday': 4,\n                'Friday': 5,\n                'Saturday': 6,\n                'Sunday': 7\n               }\n\ntransactions_df['day_of_week'] = transactions_df['transaction_dttm'].dt.day_name()\nfor k, v in days_of_week.items():\n    transactions_df['day_of_week'].replace(k,v,inplace= True)\n    \ntransactions_df[\"is_day_off\"] = transactions_df['day_of_week'].map(lambda x: 1 if x in (6,7) else 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b64450a9-e81a-4cd0-9a1e-be526c834bd0","cell_type":"code","source":"transactions_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"84b6c8cf-a91e-4fcf-b3f7-ff0bc5fec2b8","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ba49bc11-0dcc-40d8-88fd-ae1e48768045","cell_type":"code","source":"cat_cols_ = ['mcc_code',\n             'currency_rk',\n             'day_of_week',\n             'is_day_off',]\nnum_cols_ = ['transaction_amt',\n              'days_from_first_tr',\n              'days_from_prev_tr',\n              'hours_from_first_tr',\n              'hours_from_prev_tr',\n            ]                              ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1316f0b1-c642-4ee5-8a3a-267396da91bc","cell_type":"code","source":"from ptls.preprocessing import PandasDataPreprocessor\n\ntrx_preprocessor = PandasDataPreprocessor(\n    col_id='user_id',\n    col_event_time='transaction_dttm',\n    event_time_transformation='dt_to_timestamp',\n    cols_category=cat_cols_,\n    cols_numerical=num_cols_,\n    return_records=True,\n)","metadata":{"id":"1316f0b1-c642-4ee5-8a3a-267396da91bc","trusted":true},"outputs":[],"execution_count":null},{"id":"26ec4170-8e78-4cbd-abad-de09eeb7f9ca","cell_type":"code","source":"transactions_df_train = transactions_df[transactions_df['user_id'].isin(train_['user_id'])]\ntransactions_df_test = transactions_df[transactions_df['user_id'].isin(test_['user_id'])]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a02fc012-7d0d-4174-ae65-90ec8f69ccf9","cell_type":"code","source":"%%time\n\ndataset_train = trx_preprocessor.fit_transform(transactions_df_train)\ndataset_test = trx_preprocessor.fit_transform(transactions_df_test)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a02fc012-7d0d-4174-ae65-90ec8f69ccf9","outputId":"2639f831-58e6-4143-bf98-005600dfd4a1","trusted":true},"outputs":[],"execution_count":null},{"id":"708a6b74-e530-438c-ac37-22aa5c5df36c","cell_type":"markdown","source":"# Baseline","metadata":{}},{"id":"56072b32-0a74-48fc-b0ed-cdf789ab0471","cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\nimport torch\nimport pytorch_lightning as pl\nfrom functools import partial\nfrom ptls.nn import TrxEncoder, RnnSeqEncoder\nfrom ptls.frames.coles import CoLESModule\n# import lion_pytorch\nfrom ptls.frames.coles.losses import SoftmaxLoss\nfrom ptls.data_load.datasets import MemoryMapDataset\nfrom ptls.data_load.iterable_processing import SeqLenFilter\nfrom ptls.frames.coles import ColesIterableDataset\nfrom ptls.frames.coles.split_strategy import SampleSlices\nfrom ptls.frames import PtlsDataModule\nfrom ptls.frames.inference_module import InferenceModule\nfrom ptls.data_load.utils import collate_feature_dict\nfrom lightning.pytorch.loggers import WandbLogger\n\ntrx_encoder_params = dict(\n    embeddings_noise=0.003,\n    numeric_values={'transaction_amt': 'identity',\n                    'days_from_prev_tr': 'identity',\n                   },\n    embeddings={\n        'currency_rk': {'in': 5, 'out': 8},\n        'day_of_week': {'in': 8, 'out': 8},\n        'mcc_code': {'in': 333, 'out': 16},\n        },\n    )\n\ntrain_dl = PtlsDataModule(\n    train_data=ColesIterableDataset(\n        MemoryMapDataset(\n            data=dataset_train,\n            i_filters=[\n                SeqLenFilter(min_seq_len=20)\n            ],\n        ),\n        splitter=SampleSlices(\n            split_count=5,\n            cnt_min=20,\n            cnt_max=150,\n        ),\n    ),\n    train_num_workers=2,\n    train_batch_size=128,\n)\n\ninference_dataset = MemoryMapDataset(\n    data=dataset_test,\n)\n\ninference_dl = torch.utils.data.DataLoader(\n    dataset=inference_dataset,\n    collate_fn=collate_feature_dict,\n    shuffle=False,\n    batch_size=128,\n    num_workers=8,\n)\n\ndef get_coles_embeddings(random_seed):\n\n    seq_encoder = RnnSeqEncoder(\n        trx_encoder=TrxEncoder(**trx_encoder_params),\n        hidden_size=512,\n        type='gru',\n    )\n\n    model = CoLESModule(\n        seq_encoder=seq_encoder,\n        optimizer_partial=partial(torch.optim.AdamW, lr=0.001, weight_decay=1e-4),\n        lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=5, gamma=0.9),\n        loss = SoftmaxLoss()\n    )\n    wandb_logger = WandbLogger(project=\"MBD_My_Code\", log_model=\"all\", name=\"df2024_coles_base\")\n\n    trainer = pl.Trainer(\n        logger=wandb_logger,\n        max_epochs=20,\n        accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        enable_progress_bar=True,\n    )\n    print(f'logger.version = {trainer.logger.version}')\n    trainer.fit(model, train_dl)\n    torch.save(model.seq_encoder.state_dict(), f\"./models/coles-model{random_seed}.pt\")\n    inference_module = InferenceModule(\n        model=seq_encoder,\n        pandas_output=True,\n        drop_seq_features=True,\n        model_out_name=f'emb_{random_seed}')\n    \n    predict = pl.Trainer(accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\").predict(inference_module, inference_dl)\n    full_predict = pd.concat(predict, axis=0)\n    full_predict.to_csv(f'./embeddings/coles_{random_seed}.csv', index=False) ","metadata":{"id":"56072b32-0a74-48fc-b0ed-cdf789ab0471","trusted":true},"outputs":[],"execution_count":null},{"id":"9acef2da-bbea-4a18-8417-13922b43deeb","cell_type":"code","source":"pl.seed_everything(0)\nget_coles_embeddings(0)","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"8548fd66-8556-4a2b-9be3-0078c87ea180","cell_type":"code","source":"inference_embeddings = pd.read_csv('./embeddings/coles_0.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"084bbacd-c72f-4905-a36d-a2fd420eba1a","cell_type":"code","source":"inference_embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7572c2fe-2bba-44d3-a3e8-ec27b3ffee47","cell_type":"code","source":"inference_embeddings_t = inference_embeddings.merge(test_[['user_id', 'target']], how='left', on='user_id')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d5dc5f9c-0f79-4004-902c-ff04a3397766","cell_type":"code","source":"inference_embeddings_t","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"261542ee-2bff-489f-a359-19fd66fb7526","cell_type":"code","source":"inf_emb_train, inf_emb_test = train_test_split(inference_embeddings_t, random_state=42, test_size=0.2)\n\nX_train, y_train = inf_emb_train.drop(columns=['user_id', 'target']), inf_emb_train['target']\nX_test, y_test = inf_emb_test.drop(columns=['user_id', 'target']), inf_emb_test['target']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ca4b70df-0f60-41e2-b915-718d36e5626f","cell_type":"code","source":"from lightgbm import LGBMClassifier\n\ndown_model = LGBMClassifier(\n    n_estimators=600,\n    boosting_type='gbdt',\n    subsample=0.5,\n    subsample_freq=1,\n    learning_rate=0.02,\n    feature_fraction=0.75,\n    max_depth=6,\n    lambda_l1=1,\n    lambda_l2=1,\n    min_data_in_leaf=50,\n    random_state=42,\n    n_jobs=8,\n    verbose=-1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"91b8801c-44c9-4882-b09b-f49b49ba96dc","cell_type":"code","source":"%%time\n\ndown_model.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ae78c23a-0c30-4116-bc49-7c8f0e7894c8","cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\npredict = down_model.predict_proba(X_test)\nprint(f\"ROC-AUC target = {roc_auc_score(y_test, predict[:, 1])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"82313486-5891-4dee-846b-27e1a33630f5","cell_type":"code","source":"predict = down_model.predict(X_test)\nprint(classification_report(y_test, predict))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7115d98b-768e-4234-9a1b-92f378d307ec","cell_type":"markdown","source":"# Regional Attention","metadata":{}},{"id":"0da6f66e-5396-4014-a0b7-d41d7288b766","cell_type":"code","source":"import torch\nimport pytorch_lightning as pl\nfrom functools import partial\nfrom ptls.nn import TrxEncoder, RnnSeqEncoder\nfrom ptls.frames.coles import CoLESModule\n# import lion_pytorch\nfrom ptls.frames.coles.losses import SoftmaxLoss\nfrom ptls.data_load.datasets import MemoryMapDataset\nfrom ptls.data_load.iterable_processing import SeqLenFilter\nfrom ptls.frames.coles import ColesIterableDataset\nfrom ptls.frames.coles.split_strategy import SampleSlices\nfrom ptls.frames import PtlsDataModule\nfrom ptls.frames.inference_module import InferenceModule\nfrom ptls.data_load.utils import collate_feature_dict\n\ntrain_dl = PtlsDataModule(\n    train_data=ColesIterableDataset(\n        MemoryMapDataset(\n            data=dataset_train,\n            i_filters=[\n                SeqLenFilter(min_seq_len=20)\n            ],\n        ),\n        splitter=SampleSlices(\n            split_count=5,\n            cnt_min=20,\n            cnt_max=150,\n        ),\n    ),\n    train_num_workers=2,\n    train_batch_size=128,\n)\n\ninference_dataset = MemoryMapDataset(\n    data=dataset_test,\n)\n\ninference_dl = torch.utils.data.DataLoader(\n    dataset=inference_dataset,\n    collate_fn=collate_feature_dict,\n    shuffle=False,\n    batch_size=128,\n    num_workers=8,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e597010f-1b27-483a-b092-0f767d530022","cell_type":"code","source":"import torch\nfrom torch import nn\n\nfrom ptls.data_load import PaddedBatch\nfrom ptls.nn.seq_encoder.rnn_encoder import RnnEncoder\nfrom ptls.nn.seq_encoder.transformer_encoder import TransformerEncoder\nfrom ptls.nn.seq_encoder.longformer_encoder import LongformerEncoder\nfrom ptls.nn.seq_encoder.custom_encoder import Encoder\nfrom ptls.nn.trx_encoder import TrxEncoder\nfrom ptls.nn.seq_encoder.containers import SeqEncoderContainer\nimport torch.nn.functional as F\n\n# class RnnSeqEncoderRegAttn(SeqEncoderContainer):\n#     def __init__(self,\n#                  trx_encoder=None,\n#                  input_size=None,\n#                  is_reduce_sequence=True,\n#                  **seq_encoder_params,\n#                  ):\n#         super().__init__(\n#             trx_encoder=trx_encoder,\n#             seq_encoder_cls=RnnEncoder,\n#             input_size=input_size,\n#             seq_encoder_params=seq_encoder_params,\n#             is_reduce_sequence=is_reduce_sequence,\n#         )\n        \n#         self.reg_seq_encoder = RnnEncoder(\n#             input_size=input_size if input_size is not None else trx_encoder.output_size,\n#             is_reduce_sequence=is_reduce_sequence,\n#             type='gru',\n#             hidden_size=34,\n#         )\n\n#         self.emb_dim = 34\n#         self.regional_attention = nn.MultiheadAttention(\n#             embed_dim=self.emb_dim,\n#             num_heads=2,\n#             dropout=0.3,\n#             batch_first=True\n#         )\n\nclass RnnSeqEncoderRegAttn(SeqEncoderContainer):\n    def __init__(self,\n                 trx_encoder=None,\n                 input_size=None,\n                 is_reduce_sequence=True,\n                 **seq_encoder_params,\n                 ):\n        super().__init__(\n            trx_encoder=trx_encoder,\n            seq_encoder_cls=RnnEncoder,\n            input_size=input_size,\n            seq_encoder_params=seq_encoder_params,\n            is_reduce_sequence=is_reduce_sequence,\n        )\n        \n        self.reg_seq_encoder = RnnEncoder(\n            input_size=input_size if input_size is not None else trx_encoder.output_size,\n            is_reduce_sequence=is_reduce_sequence,\n            **seq_encoder_params,\n        )\n\n        self.emb_dim = 34\n        self.regional_attention = nn.MultiheadAttention(\n            embed_dim=self.emb_dim,\n            num_heads=2,\n            dropout=0.2,\n            batch_first=True\n        )\n\n    def forward(self, x, names=None, seq_len=None, h_0=None):\n        # print(f\"x_in = {x.payload['amount'].size()}\")\n        x = self.trx_encoder(x)\n\n        x_new = x.payload\n        \n        segment_length = 10\n        pad_length = (segment_length - (x_new.size()[1] % segment_length)) % segment_length\n        padded_x_new = F.pad(x_new, ((0, 0, 0, pad_length, 0, 0)), 'constant', 0)\n        segmented_tensors = torch.stack(torch.split(padded_x_new, segment_length, dim=1)).to(x_new.device)\n        \n        regional_embeddings = torch.Tensor().to(x.device)\n        for tensor in segmented_tensors:\n            tensor = PaddedBatch(tensor.permute(1, 0, 2), [tensor.size()[0]] * tensor.size()[1])\n            regional_embed = self.reg_seq_encoder(tensor)\n            regional_embeddings = torch.cat((regional_embeddings, regional_embed), 0)\n        \n        regional_embeddings = regional_embeddings[:len(regional_embeddings) - pad_length, :]\n        # layer_norm = nn.LayerNorm([regional_embeddings.size()[0], regional_embeddings.size()[1]])\n        # layer_norm.to(x.device)\n        # regional_embeddings = layer_norm(regional_embeddings)\n        # print(regional_embeddings.size())\n        if regional_embeddings.size()[1] != self.emb_dim:\n            regional_embeddings = F.pad(regional_embeddings, ((0, abs(regional_embeddings.size()[1] - self.emb_dim), 0, 0)), 'constant', 0)\n        x_reg_embed, _ = self.regional_attention(regional_embeddings, regional_embeddings, regional_embeddings)\n        # print(x_reg_embed.size())\n        # print(x_new.size())\n        if x_reg_embed.size()[0] != x_new.size()[1]:\n            x_reg_embed = x_reg_embed[:, :-abs(x_reg_embed.size()[1] - x_new.size()[0])]\n        # x_reg_embed = x_reg_embed.permute(1, 0)\n        x_reg_embed = x_reg_embed[None, :, :]\n        # print(f\"{x_new.size()=}\")\n        # print(f\"{x_reg_embed.size()=}\")\n        x_new = x_new + x_reg_embed\n        x_new.to(x.device)\n        x_new = PaddedBatch(x_new, x.seq_lens)\n        # x_new.to(x.device)\n        x = self.seq_encoder(x_new, h_0)\n        # print(f\"rnn_x_size = {x.size()}\")\n        return x\n    \n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f1dcd233-ab4f-45e2-8145-bc183cd06b08","cell_type":"code","source":"import ptls\nfrom functools import partial\n\noptimizer_partial = partial(\n    torch.optim.AdamW,\n    lr=0.001,\n    weight_decay=1e-4\n)\n\nlr_scheduler_partial = partial(\n    torch.optim.lr_scheduler.StepLR,\n    step_size=2,\n    gamma=0.9025\n)\n\nseq_encoder = RnnSeqEncoderRegAttn(\n        trx_encoder=ptls.nn.TrxEncoder(\n            embeddings_noise=0.003,\n            numeric_values={'transaction_amt': 'identity',\n                    'days_from_prev_tr': 'identity',\n                   },\n            embeddings={\n            'currency_rk': {'in': 5, 'out': 8},\n            'day_of_week': {'in': 8, 'out': 8},\n            'mcc_code': {'in': 333, 'out': 16},\n            },\n        ),\n        type='gru',\n        hidden_size=34\n    )\n\npl_module = ptls.frames.coles.CoLESModule(\n    validation_metric=ptls.frames.coles.metric.BatchRecallTopK(\n        K=4,\n        metric='cosine'\n    ),\n    seq_encoder=seq_encoder,\n    optimizer_partial=optimizer_partial,\n    lr_scheduler_partial=lr_scheduler_partial\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"916a373a-6ae8-4532-a5f6-b226c607c150","cell_type":"code","source":"from lightning.pytorch.loggers import WandbLogger\n\nwandb_logger = WandbLogger(project=\"MBD_My_Code\", log_model=\"all\", name='df2024_reg_attn')\n\ntrainer = pl.Trainer(\n    logger=wandb_logger,\n    max_epochs=20,\n    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    enable_progress_bar=True,\n    gradient_clip_val=0.5,\n    log_every_n_steps=50,\n    limit_val_batches=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9f3a8588-7a14-4a17-a805-44599b7daa49","cell_type":"code","source":"trainer.fit(pl_module, train_dl)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4a187a1c-00e1-4b7c-8142-c67d0330cebe","cell_type":"code","source":"import pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport numpy as np\nfrom itertools import chain\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom datetime import datetime\nfrom ptls.custom_layers import StatPooling\nfrom ptls.nn.seq_step import LastStepEncoder\n\n\nclass InferenceModuleMultimodal(pl.LightningModule):\n    def __init__(\n        self,\n        model,\n        pandas_output=True,\n        col_id='client_id',\n        target_col_names=None,\n        model_out_name='emb',\n        model_type='notab'\n    ):\n        super().__init__()\n\n        self.model = model\n        self.pandas_output = pandas_output\n        self.target_col_names = target_col_names\n        self.col_id = col_id\n        self.model_out_name = model_out_name\n        self.model_type = model_type\n\n        self.stat_pooler = StatPooling()\n        self.last_step = LastStepEncoder()\n\n    def forward(self, x):\n        x_len = len(x)\n        if x_len == 3:\n            x, batch_ids, target_cols = x\n        else: \n            x, batch_ids = x\n        if 'seq_encoder' in dir(self.model):\n            out = self.model.seq_encoder(x)\n        else:\n            out = self.model(x)\n            \n        if x_len == 3:\n            target_cols = torch.tensor(target_cols)\n            x_out = {\n                self.col_id: batch_ids,\n                self.model_out_name: out\n            }\n            if len(target_cols.size()) > 1:\n                for idx, target_col in enumerate(self.target_col_names):\n                    x_out[target_col] = target_cols[:, idx]\n            else: \n                x_out[self.target_col_names[0]] = target_cols[::4]\n        else:\n            x_out = {\n                self.col_id: batch_ids,\n                self.model_out_name: out\n            }\n\n        if self.pandas_output:\n            return self.to_pandas(x_out)\n        return x_out\n\n    @staticmethod\n    def to_pandas(x):\n        expand_cols = []\n        scalar_features = {}\n\n        for k, v in x.items():\n            if type(v) is torch.Tensor:\n                v = v.cpu().numpy()\n\n            if type(v) is list or len(v.shape) == 1:\n                scalar_features[k] = v\n            elif len(v.shape) == 2:\n                expand_cols.append(k)\n            else:\n                scalar_features[k] = None\n\n        dataframes = [pd.DataFrame(scalar_features)]\n        for col in expand_cols:\n            v = x[col].cpu().numpy()\n            dataframes.append(pd.DataFrame(v, columns=[f'{col}_{i:04d}' for i in range(v.shape[1])]))\n\n        return pd.concat(dataframes, axis=1)\n\ndef collate_feature_dict_with_target(batch, col_id='client_id', target_col_names=None):\n    batch_ids = []\n    target_cols = []\n    for sample in batch:\n        batch_ids.append(sample[col_id])\n        del sample[col_id]\n        \n        if target_col_names is not None:\n            sample_targets = []\n            for target_col in target_col_names:\n                sample_targets.append(sample[target_col])\n                del sample[target_col]\n            target_cols.append(sample_targets)\n                \n            \n    padded_batch = collate_feature_dict(batch)\n    if target_col_names is not None:\n        return padded_batch, batch_ids, target_cols\n    return padded_batch, batch_ids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"33f0b2c7-365a-4bad-a999-dc9330077b4e","cell_type":"code","source":"inference_module = InferenceModule(\n        model=seq_encoder,\n        pandas_output=True,\n        drop_seq_features=True)\n        # model_out_name=f'emb_{random_seed}')\n\npredict = pl.Trainer(accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\").predict(inference_module, inference_dl)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0ca666d1-7828-4f53-9b0e-28b8f8413a74","cell_type":"code","source":"inference_embeddings = pd.concat(predict, axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"082c9a25-82e5-4053-baed-e65039d0fd48","cell_type":"code","source":"inference_embeddings_t = inference_embeddings.merge(test_[['user_id', 'target']], how='left', on='user_id')\n\ninf_emb_train, inf_emb_test = train_test_split(inference_embeddings_t, random_state=42, test_size=0.2)\n\nX_train, y_train = inf_emb_train.drop(columns=['user_id', 'target']), inf_emb_train['target']\nX_test, y_test = inf_emb_test.drop(columns=['user_id', 'target']), inf_emb_test['target']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b0b27b48-91d6-40eb-9e6a-ab8f1ecc7ae8","cell_type":"code","source":"from lightgbm import LGBMClassifier\n\ndown_model = LGBMClassifier(\n    n_estimators=600,\n    boosting_type='gbdt',\n    subsample=0.5,\n    subsample_freq=1,\n    learning_rate=0.02,\n    feature_fraction=0.75,\n    max_depth=6,\n    lambda_l1=1,\n    lambda_l2=1,\n    min_data_in_leaf=50,\n    random_state=42,\n    n_jobs=8,\n    verbose=-1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"01fd0cfc-1d7f-4e31-8f9c-bb97f78f0daf","cell_type":"code","source":"%%time\n\ndown_model.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a0878686-c439-426d-9637-5f61d9d6cd4d","cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\npredict = down_model.predict_proba(X_test)\nprint(f\"ROC-AUC target = {roc_auc_score(y_test, predict[:, 1])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8da29e9f-e976-43e7-baf0-d7620ad4d40b","cell_type":"code","source":"predict = down_model.predict(X_test)\nprint(classification_report(y_test, predict))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d4e42cf7-1cff-441f-9af0-a9ec0c45eeba","cell_type":"markdown","source":"# Cross-attention","metadata":{}},{"id":"9b7ff5c7-81c3-4683-969a-c6e9781eca8e","cell_type":"code","source":"import torch\nfrom torch import nn\n\nfrom ptls.data_load import PaddedBatch\nfrom ptls.nn.seq_encoder.rnn_encoder import RnnEncoder\nfrom ptls.nn.seq_encoder.transformer_encoder import TransformerEncoder\nfrom ptls.nn.seq_encoder.longformer_encoder import LongformerEncoder\nfrom ptls.nn.seq_encoder.custom_encoder import Encoder\nfrom ptls.nn.trx_encoder import TrxEncoder\nfrom ptls.nn.seq_encoder.containers import SeqEncoderContainer\nimport torch.nn.functional as F\nfrom ptls.nn.seq_encoder.custom_encoder import MLP\n\nclass RnnSeqEncoderCrossAttn(SeqEncoderContainer):\n    def __init__(self,\n                 trx_encoder=None,\n                 input_size=None,\n                 small_patches_size=3,\n                 large_patches_size=12,\n                 is_reduce_sequence=True,\n                 **seq_encoder_params,\n                 ):\n        super().__init__(\n            trx_encoder=trx_encoder,\n            seq_encoder_cls=RnnEncoder,\n            input_size=input_size,\n            seq_encoder_params=seq_encoder_params,\n            is_reduce_sequence=is_reduce_sequence,\n        )\n        self.small_patches_size = small_patches_size\n        self.large_patches_size = large_patches_size\n        self.emb_dim = 128\n\n        self.small_attn = nn.MultiheadAttention(\n            embed_dim=self.emb_dim,\n            num_heads=2,\n            dropout=0.2,\n            batch_first=True\n        )\n        \n        self.large_attn = nn.MultiheadAttention(\n            embed_dim=self.emb_dim,\n            num_heads=2,\n            dropout=0.2,\n            batch_first=True\n        )\n\n        self.small_seq_encoder = RnnEncoder(\n            input_size=input_size if input_size is not None else trx_encoder.output_size,\n            is_reduce_sequence=is_reduce_sequence,\n            type='gru',\n            hidden_size=256\n        )\n\n        # MLP variation\n        self.head_small = MLP(\n                n_in=256,\n                n_hidden=256,\n                n_out=128\n            )\n        self.head_large = MLP(\n                n_in=256,\n                n_hidden=256,\n                n_out=128\n            ) \n\n    def forward(self, x, names=None, seq_len=None, h_0=None):\n        \"\"\"\n        Возможно стоит использовать разные енкодеры для маленьких и для больших патчей\n        \"\"\"\n        # for key, value in x.payload.items():\n        #     small_x[key] = list(torch.split(value, self.small_patches_size, dim=1))\n        #     if small_x[key][-1].size()[-1] != self.small_patches_size:\n        #         pad_size = self.small_patches_size - small_x[key][-1].size()[-1]\n        #         small_x[key][-1] = F.pad(small_x[key][-1], (0, pad_size), \"replicate\")\n        #     small_x[key] = torch.stack(small_x[key])\n\n        #     large_x[key] = list(torch.split(value, self.large_patches_size, dim=1))\n        #     if large_x[key][-1].size()[-1] != self.large_patches_size:\n        #         pad_size = self.large_patches_size - large_x[key][-1].size()[-1]\n        #         large_x[key][-1] = F.pad(large_x[key][-1], (0, pad_size), \"replicate\")\n        #     large_x[key] = torch.stack(large_x[key])\n        x = self.trx_encoder(x)\n        x_new = x.payload\n        # print(f\"{x_new.size()=}\")\n        # x_embs = self.seq_encoder(x)\n        # print(f\"{x_embs.size()=}\")\n\n        real_size = x_new.size()[1]\n        small_patches = list(torch.split(x_new, self.small_patches_size, dim=1))\n        large_patches = list(torch.split(x_new, self.large_patches_size, dim=1))\n\n        if small_patches[-1].size()[1] != self.large_patches_size:\n            pad_size = self.small_patches_size - small_patches[-1].size()[1]\n            # print(pad_size)\n            # print(small_patches[-1].size())\n            small_patches[-1] = F.pad(small_patches[-1], (0, 0, pad_size, 0), \"replicate\")\n\n        if large_patches[-1].size()[1] != self.large_patches_size:\n            pad_size = self.large_patches_size - large_patches[-1].size()[1]\n            large_patches[-1] = F.pad(large_patches[-1], (0, 0, pad_size, 0), \"replicate\")\n\n        small_patches = torch.stack(small_patches)\n        large_patches = torch.stack(large_patches)\n        \n        \n\n        large_comp_embed = torch.zeros(x_new.size()[0], 128).to(x.device)\n        for large_patch in large_patches:\n            large_patch = PaddedBatch(large_patch, [large_patch.size()[1]] * large_patch.size()[0])\n            large_emb = self.seq_encoder(large_patch)\n            # large_emb = self.head_large(large_emb)\n            large_comp_embed = (large_comp_embed + large_emb) / 2\n\n        small_comp_embed = torch.zeros(x_new.size()[0], 128).to(x.device)\n        for small_patch in small_patches:\n            small_patch = PaddedBatch(small_patch, [small_patch.size()[1]] * small_patch.size()[0])\n            small_emb = self.small_seq_encoder(small_patch)\n            # small_emb = self.head_small(small_emb)\n            small_comp_embed = (small_comp_embed + small_emb) / 2\n\n        small_attn_emb, _ = self.small_attn(small_comp_embed, large_comp_embed, large_comp_embed)\n        # large_attn_emb, _ = self.large_attn(large_comp_embed, small_comp_embed, small_comp_embed)\n\n        out = torch.cat((small_attn_emb, large_comp_embed), dim=1)\n\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2b90cc35-a327-48ec-a523-ebe674b7342e","cell_type":"code","source":"import ptls\n\noptimizer_partial = partial(\n    torch.optim.AdamW,\n    lr=0.001,\n    weight_decay=1e-4\n)\n\nlr_scheduler_partial = partial(\n    torch.optim.lr_scheduler.StepLR,\n    step_size=2,\n    gamma=0.9025\n)\n\nseq_encoder = RnnSeqEncoderCrossAttn(\n        trx_encoder=ptls.nn.TrxEncoder(\n            embeddings_noise=0.003,\n            numeric_values={'transaction_amt': 'identity',\n                    'days_from_prev_tr': 'identity',\n                   },\n            embeddings={\n            'currency_rk': {'in': 5, 'out': 8},\n            'day_of_week': {'in': 8, 'out': 8},\n            'mcc_code': {'in': 333, 'out': 16},\n            },\n        ),\n        type='gru',\n        hidden_size=256,\n        small_patches_size=3,\n        large_patches_size=12\n    )\n\npl_module = ptls.frames.coles.CoLESModule(\n    validation_metric=ptls.frames.coles.metric.BatchRecallTopK(\n        K=4,\n        metric='cosine'\n    ),\n    seq_encoder=seq_encoder,\n    optimizer_partial=optimizer_partial,\n    lr_scheduler_partial=lr_scheduler_partial\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8efc2a5e-a912-4ab5-ae4b-14b8ad56ec99","cell_type":"code","source":"from lightning.pytorch.loggers import WandbLogger\n\nwandb_logger = WandbLogger(project=\"MBD_My_Code\", log_model=\"all\", name='df2024_cross_attn')\n\ntrainer = pl.Trainer(\n    logger=wandb_logger,\n    max_epochs=15,\n    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    enable_progress_bar=True,\n    gradient_clip_val=0.5,\n    log_every_n_steps=50,\n    limit_val_batches=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"68b55304-f78f-4628-9ddc-b74e85edb62c","cell_type":"code","source":"trainer.fit(pl_module, train_dl)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e51c415c-4c77-46e2-bf3a-a8a9c615a2b6","cell_type":"code","source":"inference_module = InferenceModule(\n        model=seq_encoder,\n        pandas_output=True,\n        drop_seq_features=True)\n        # model_out_name=f'emb_{random_seed}')\n\npredict = pl.Trainer(accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\").predict(inference_module, inference_dl)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a6b2be42-4f54-4d43-9f57-568cebf56acf","cell_type":"code","source":"inference_embeddings = pd.concat(predict, axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9fa8e60e-a7ec-4eed-ab18-f2aaf489500f","cell_type":"code","source":"inference_embeddings_t = inference_embeddings.merge(test_[['user_id', 'target']], how='left', on='user_id')\n\ninf_emb_train, inf_emb_test = train_test_split(inference_embeddings_t, random_state=42, test_size=0.2)\n\nX_train, y_train = inf_emb_train.drop(columns=['user_id', 'target']), inf_emb_train['target']\nX_test, y_test = inf_emb_test.drop(columns=['user_id', 'target']), inf_emb_test['target']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d8d828ee-8351-41e3-a21c-c3da8e4a45cc","cell_type":"code","source":"from lightgbm import LGBMClassifier\n\ndown_model = LGBMClassifier(\n    n_estimators=500,\n    boosting_type='gbdt',\n    subsample=0.5,\n    subsample_freq=1,\n    learning_rate=0.02,\n    feature_fraction=0.75,\n    max_depth=6,\n    lambda_l1=1,\n    lambda_l2=1,\n    min_data_in_leaf=50,\n    random_state=42,\n    n_jobs=8,\n    verbose=-1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"44d65cff-d24c-4a9b-9de2-45aede241176","cell_type":"code","source":"%%time\n\ndown_model.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1374dc74-94ef-452e-8e40-b8679693f99e","cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n\npredict = down_model.predict_proba(X_test)\nprint(f\"ROC-AUC target = {roc_auc_score(y_test, predict[:, 1])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d37e8175-2702-4015-84b6-3b1348552b5b","cell_type":"code","source":"predict = down_model.predict(X_test)\nprint(f\"accuracy = {accuracy_score(y_test, predict)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"82fba168-964a-46ed-9213-b1e7bb0b9e48","cell_type":"markdown","source":"# GPT Baseline","metadata":{}},{"id":"0cf86401-6866-4286-a302-fa9807cb5e5b","cell_type":"code","source":"dataset_train[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7886356c-074f-4b1c-a613-2d7c0c8701e9","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom ptls.data_load.iterable_processing_dataset import IterableProcessingDataset\nfrom ptls.data_load import IterableChain\nfrom datetime import datetime\nfrom ptls.data_load.datasets.parquet_dataset import ParquetDataset, ParquetFiles\nfrom ptls.data_load.iterable_processing.feature_filter import FeatureFilter\nfrom ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\nimport torch\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom ptls.data_load.utils import collate_feature_dict\nfrom tqdm import tqdm\nimport ptls\nimport torch.nn.functional as F\n\nclass QuantilfyAmount(IterableProcessingDataset):\n    def __init__(self, col_amt='amount', quantilies=None):\n        super().__init__()\n        self.col_amt = col_amt\n        if quantilies is None:\n            self.quantilies = [0., 267.6, 1198.65, 3667.2, 8639.8, 18325.7, 36713.2, 68950.3, 143969.1, 421719.1]\n        else: \n            self.quantilies = quantilies\n    \n    def __iter__(self):\n        for rec in self._src:\n            features = rec[0] if type(rec) is tuple else rec\n            amount = features[self.col_amt]\n            am_quant = torch.zeros(len(amount), dtype=torch.int)\n            for i, q in enumerate(self.quantilies):\n                am_quant = torch.where(amount>q, i, am_quant)\n            features[self.col_amt] = am_quant\n            yield features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3572597c-5801-43ee-b293-39f15301d175","cell_type":"code","source":"from ptls.frames.gpt.gpt_dataset import GptIterableDataset\n\ndates_quantiles = [0, 5, 14, 36.7, 132.4, 256, 366]\n\ntrain_dl = PtlsDataModule(\n    train_data=GptIterableDataset(\n        MemoryMapDataset(\n            data=dataset_train,\n            i_filters=[\n        ptls.data_load.iterable_processing.SeqLenFilter(min_seq_len=16),\n        ptls.data_load.iterable_processing.SeqLenFilter(max_seq_len=2048),\n        QuantilfyAmount(col_amt='transaction_amt'),\n        QuantilfyAmount(col_amt='days_from_prev_tr', quantilies=dates_quantiles),\n        ptls.data_load.iterable_processing.CategorySizeClip(\n            category_max_size={\n                'transaction_amt': 10,\n                'days_from_prev_tr': 365,\n                'currency_rk': 80,\n                'day_of_week': 7,\n                'mcc_code': 90\n              }\n        ),\n        ptls.data_load.iterable_processing.to_torch_tensor.ToTorch(),\n        \n        \n        # GetPatches()\n    ],\n        ),\n        min_len=40,\n        max_len=500\n    ),\n    train_num_workers=0,\n    train_batch_size=128,\n)\n\ninference_dataset = MemoryMapDataset(\n    data=dataset_test,\n    i_filters=[\n        ptls.data_load.iterable_processing.SeqLenFilter(min_seq_len=16),\n        ptls.data_load.iterable_processing.SeqLenFilter(max_seq_len=2048),\n        QuantilfyAmount(col_amt='transaction_amt'),\n        QuantilfyAmount(col_amt='days_from_prev_tr', quantilies=dates_quantiles),\n        ptls.data_load.iterable_processing.CategorySizeClip(\n            category_max_size={\n                'transaction_amt': 10,\n                'days_from_prev_tr': 365,\n                'currency_rk': 80,\n                'day_of_week': 7,\n                'mcc_code': 90\n              }\n        ),\n        ptls.data_load.iterable_processing.to_torch_tensor.ToTorch(),\n        \n        \n        # GetPatches()\n    ]\n)\n\ninference_dl = torch.utils.data.DataLoader(\n    dataset=inference_dataset,\n    collate_fn=collate_feature_dict,\n    shuffle=False,\n    batch_size=128,\n    num_workers=2,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"216cdaa3-3475-45d2-8f97-f83e814f99b5","cell_type":"code","source":"import pytorch_lightning as pl\nimport torch\nfrom torch import nn\nimport warnings\nfrom torchmetrics import MeanMetric\nfrom typing import Tuple, Dict, List, Union\n\nfrom ptls.nn.seq_encoder.abs_seq_encoder import AbsSeqEncoder\nfrom ptls.nn import PBL2Norm\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom ptls.custom_layers import StatPooling, GEGLU\nfrom ptls.nn.seq_step import LastStepEncoder\n\nclass Head(nn.Module):   \n    def __init__(self, input_size, n_classes, hidden_size=64, drop_p=0.1):\n        super().__init__()\n        self.head = nn.Sequential(\n            nn.Linear(input_size, hidden_size, bias=True),\n            nn.GELU(),\n            nn.Dropout(drop_p),\n            nn.Linear(hidden_size, n_classes)\n        )\n    def forward(self, x):\n        x = self.head(x)\n        return x\n\nclass GptPretrainModule(pl.LightningModule):\n    \"\"\"GPT2 Language model\n\n    Original sequence are encoded by `TrxEncoder`.\n    Model `seq_encoder` predicts embedding of next transaction.\n    Heads are used to predict each feature class of future transaction.\n\n    Parameters\n    ----------\n    trx_encoder:\n        Module for transform dict with feature sequences to sequence of transaction representations\n    seq_encoder:\n        Module for sequence processing. Generally this is transformer based encoder. Rnn is also possible\n        Should works without sequence reduce\n    head_hidden_size:\n        Hidden size of heads for feature prediction\n    seed_seq_len:\n         Size of starting sequence without loss \n    total_steps:\n        total_steps expected in OneCycle lr scheduler\n    max_lr:\n        max_lr of OneCycle lr scheduler\n    weight_decay:\n        weight_decay of Adam optimizer\n    pct_start:\n        % of total_steps when lr increase\n    norm_predict:\n        use l2 norm for transformer output or not\n    inference_pooling_strategy:\n        'out' - `seq_encoder` forward (`is_reduce_requence=True`) (B, H)\n        'out_stat' - min, max, mean, std statistics pooled from `seq_encoder` layer (B, H) -> (B, 4H)\n        'trx_stat' - min, max, mean, std statistics pooled from `trx_encoder` layer (B, H) -> (B, 4H)\n        'trx_stat_out' - min, max, mean, std statistics pooled from `trx_encoder` layer + 'out' from `seq_encoder` (B, H) -> (B, 5H)\n    \"\"\"\n\n    def __init__(self,\n                 trx_encoder: torch.nn.Module,\n                 seq_encoder: AbsSeqEncoder,\n                 head_hidden_size: int = 64,\n                 total_steps: int = 64000,\n                 seed_seq_len: int = 16,\n                 max_lr: float = 0.00005,\n                 weight_decay: float = 0.0,\n                 pct_start: float = 0.1,\n                 norm_predict: bool = False,\n                 inference_pooling_strategy: str = 'out_stat'\n                 ):\n\n        super().__init__()\n        self.save_hyperparameters(ignore=['trx_encoder', 'seq_encoder'])\n\n        self.trx_encoder = trx_encoder\n        # assert self.trx_encoder.embeddings, '`embeddings` parameter for `trx_encoder` should contain at least 1 feature!'\n\n        self._seq_encoder = seq_encoder\n        self._seq_encoder.is_reduce_sequence = False\n\n        self.head = nn.ModuleDict()\n        for col_name, noisy_emb in self.trx_encoder.embeddings.items():\n            self.head[col_name] = Head(input_size=self._seq_encoder.embedding_size, hidden_size=head_hidden_size, n_classes=noisy_emb.num_embeddings)\n\n        if self.hparams.norm_predict:\n            self.fn_norm_predict = PBL2Norm()\n\n        self.loss = nn.CrossEntropyLoss(ignore_index=0)\n\n        self.train_gpt_loss = MeanMetric()\n        self.valid_gpt_loss = MeanMetric()\n\n    def forward(self, batch: PaddedBatch):\n        # print(f\"{batch.payload['amount']}\")\n        z_trx = self.trx_encoder(batch) \n        out = self._seq_encoder(z_trx)\n        if self.hparams.norm_predict:\n            out = self.fn_norm_predict(out)\n        return out\n    \n    def loss_gpt(self, predictions, labels, is_train_step):\n        loss = 0\n        for col_name, head in self.head.items():\n            y_pred = head(predictions[:, self.hparams.seed_seq_len:-1, :])\n            y_pred = y_pred.view(-1, y_pred.size(-1))\n\n            y_true = labels[col_name][:, self.hparams.seed_seq_len+1:]\n            y_true = torch.flatten(y_true.long())\n\n            # print(f\"{y_pred.size()=}\")\n            # print(f\"{y_true=}\")\n            # print(y_true[10952])\n            \n            loss += self.loss(y_pred, y_true)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        out = self.forward(batch)  # PB: B, T, H\n        out = out.payload if isinstance(out, PaddedBatch) else out\n        labels = batch.payload\n        \n        loss_gpt = self.loss_gpt(out, labels, is_train_step=True)\n        self.train_gpt_loss(loss_gpt)\n        self.log(f'gpt/loss', loss_gpt, sync_dist=True)\n        return loss_gpt\n\n    def validation_step(self, batch, batch_idx):\n        out = self.forward(batch)  # PB: B, T, H\n        out = out.payload if isinstance(out, PaddedBatch) else out\n        labels = batch.payload\n\n        loss_gpt = self.loss_gpt(out, labels, is_train_step=False)\n        self.valid_gpt_loss(loss_gpt)\n\n    def on_training_epoch_end(self):\n        self.log(f'gpt/train_gpt_loss', self.train_gpt_loss, prog_bar=False, sync_dist=True, rank_zero_only=True)\n        # self.train_gpt_loss reset not required here\n\n    def on_validation_epoch_end(self):\n        self.log(f'gpt/valid_gpt_loss', self.valid_gpt_loss, prog_bar=True, sync_dist=True, rank_zero_only=True)\n        # self.valid_gpt_loss reset not required here\n\n    def configure_optimizers(self):\n        optim = torch.optim.NAdam(self.parameters(),\n                                 lr=self.hparams.max_lr,\n                                 weight_decay=self.hparams.weight_decay,\n                                 )\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer=optim,\n            max_lr=self.hparams.max_lr,\n            total_steps=self.hparams.total_steps,\n            pct_start=self.hparams.pct_start,\n            anneal_strategy='cos',\n            cycle_momentum=False,\n            div_factor=25.0,\n            final_div_factor=10000.0,\n            three_phase=False,\n        )\n        scheduler = {'scheduler': scheduler, 'interval': 'step'}\n        return [optim], [scheduler]\n    \n    @property\n    def seq_encoder(self):\n        return GPTInferenceModule(pretrained_model=self)\n\nclass GPTInferenceModule(torch.nn.Module):\n    def __init__(self, pretrained_model):\n        super().__init__()\n        self.model = pretrained_model\n        self.model.is_reduce_sequence = False\n\n        self.stat_pooler = StatPooling()\n        self.last_step = LastStepEncoder()\n\n    def forward(self, batch):\n        z_trx = self.model.trx_encoder(batch)\n        out = self.model._seq_encoder(z_trx)\n        out = out if isinstance(out, PaddedBatch) else PaddedBatch(out, batch.seq_lens)\n        if self.model.hparams.inference_pooling_strategy=='trx_stat_out':\n            stats = self.stat_pooler(z_trx)\n            out = self.last_step(out)\n            out = torch.cat([stats, out], dim=1)\n        elif self.model.hparams.inference_pooling_strategy=='trx_stat':\n            out = self.stat_pooler(z_trx)\n        elif self.model.hparams.inference_pooling_strategy=='out_stat':\n            out = self.stat_pooler(out)\n        elif self.model.hparams.inference_pooling_strategy=='out':\n            out = self.last_step(out)\n        else:\n            raise\n        if self.model.hparams.norm_predict:\n            out = out / (out.pow(2).sum(dim=-1, keepdim=True) + 1e-9).pow(0.5)\n        return out\n\nclass CorrGptPretrainModule(GptPretrainModule):\n    def __init__(self, feature_encoder, seq_encoder, trx_encoder, *args, **kwargs):\n        trx_encoder.numeric_values = {}\n        super().__init__(trx_encoder=trx_encoder, seq_encoder=seq_encoder, *args, **kwargs)\n        self.save_hyperparameters(ignore=['trx_encoder', 'seq_encoder', 'feature_encoder'])\n        self.feature_encoder = feature_encoder\n\n    def forward(self, batch):\n        # print(f\"{batch.payload['event_time'].size()=}\")\n        z_trx = self.trx_encoder(batch)\n        # print(z_trx.payload.size())\n        payload = z_trx.payload.view(z_trx.payload.shape[:-1] + (-1, 16))\n        print(payload.size())\n        payload = self.feature_encoder(payload)\n        encoded_trx = PaddedBatch(payload=payload, length=z_trx.seq_lens)\n        out = self._seq_encoder(encoded_trx)\n        # print(out.payload.size())\n        if self.hparams.norm_predict:\n            out = self.fn_norm_predict(out)\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"be3045ba-4fb9-4856-9756-32539d73dd53","cell_type":"code","source":"from ptls.nn import TabFormerFeatureEncoder\nfrom ptls.nn import TransformerEncoder\nfrom ptls.nn import TrxEncoder\nimport ptls\n\nfeature_encoder = TabFormerFeatureEncoder(\n    n_cols=5,\n    emb_dim=16\n)\n\nseq_encoder = TransformerEncoder(\n    n_heads=2,\n    n_layers=2,\n    input_size=80,\n    use_positional_encoding=True\n)\n\ntrx_encoder=ptls.nn.TrxEncoder(\n            embeddings_noise=0.003,\n            # numeric_values={'transaction_amt': 'identity',\n            #         'days_from_prev_tr': 'identity',\n            #        },\n            embeddings={\n            'transaction_amt': {'in': 11 , 'out': 16},\n            'days_from_prev_tr': {'in':10 , 'out': 16},\n            'currency_rk': {'in': 5, 'out': 16},\n            'day_of_week': {'in': 8, 'out': 16},\n            'mcc_code': {'in': 333, 'out': 16},\n            },\n        )\n\npl_module = GptPretrainModule(\n    # total_steps=20000,\n    # feature_encoder=feature_encoder,\n    trx_encoder=trx_encoder,\n    seq_encoder=seq_encoder\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ff6929f0-f9b1-4f2b-9440-72b3b632f34b","cell_type":"code","source":"from lightning.pytorch.loggers import WandbLogger\n\nwandb_logger = WandbLogger(project=\"MBD_My_Code\", log_model=\"all\")\n\ntrainer = pl.Trainer(\n    logger=wandb_logger,\n    max_epochs=20,\n    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    enable_progress_bar=True,\n    gradient_clip_val=0.5,\n    log_every_n_steps=50,\n    limit_val_batches=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c975188a-57f7-4ca3-8022-457ac55953ee","cell_type":"code","source":"trainer.fit(pl_module, train_dl)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ee81869c-8113-4dc3-a9d9-86409fe1fbf7","cell_type":"code","source":"# Удалить!\n\nimport os\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport numpy as np\n\nfrom itertools import chain\nfrom ptls.data_load.padded_batch import PaddedBatch\n\n\nclass InferenceModule(pl.LightningModule):\n    def __init__(self, model, pandas_output=True, drop_seq_features=True, model_out_name='out'):\n        super().__init__()\n\n        self.model = model\n        self.pandas_output = pandas_output\n        self.drop_seq_features = drop_seq_features\n        self.model_out_name = model_out_name\n\n    def forward(self, x: PaddedBatch):\n        out = self.model(x)\n        print(out)\n        print(out.payload.size())\n        if self.drop_seq_features:\n            x = x.drop_seq_features()\n            x[self.model_out_name] = out\n        else:\n            x.payload[self.model_out_name] = out\n        if self.pandas_output:\n            return self.to_pandas(x)\n        return x\n\n    def to_pandas(self, x):\n        is_reduced = None\n        scalar_features, seq_features, expand_features = {}, {}, {}\n        df_scalar, df_seq, df_expand, out_df = None, None, None, None\n        len_mask = None\n\n        x_ = x\n        print(x_)\n        if type(x_) is PaddedBatch:\n            len_mask = x_.seq_len_mask.bool().cpu().numpy()\n            x_ = x_.payload\n        is_reduced = (type(x_[self.model_out_name]) is not PaddedBatch)\n        for k, v in x_.items():\n            if type(v) is PaddedBatch:\n                len_mask = v.seq_len_mask.bool().cpu().numpy()\n                v = v.payload\n            if type(v) is torch.Tensor:\n                v = v.detach().cpu().numpy()\n            if type(v) is list or len(v.shape) == 1:\n                scalar_features[k] = v\n            elif k.startswith('target'):\n                scalar_features[k] = v\n            elif len(v.shape) == 3:\n                expand_features[k] = v\n            elif k == self.model_out_name and len(v.shape) == 2:\n                expand_features[k] = v\n            elif len(v.shape) == 2:\n                seq_features[k] = v\n\n        if is_reduced:\n            df_scalar, df_seq, df_expand = self.to_pandas_record(x, expand_features, scalar_features, seq_features, len_mask)\n        else:\n            df_scalar, df_seq, df_expand = self.to_pandas_sequence(x, expand_features, scalar_features, seq_features, len_mask)\n\n        out_df = df_scalar\n        if df_seq:\n            df_seq = pd.concat(df_seq, axis = 1)\n            out_df = pd.concat([df_scalar, df_seq], axis = 1)\n        if df_expand:\n            df_expand = pd.concat(df_expand, axis = 0).reset_index(drop=True)\n            out_df = pd.concat([out_df.reset_index(drop=True), df_expand], axis = 1)\n\n        return out_df\n    \n    def to_numpy(self, tensor):\n        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\n    @staticmethod\n    def to_pandas_record(x, expand_features, scalar_features, seq_features, len_mask):\n        dataframes_scalar = []\n        for k, v in scalar_features.items():\n            dataframes_scalar.append(pd.DataFrame(v, columns=[k]))\n        dataframes_scalar = pd.concat(dataframes_scalar, axis = 1)\n\n        dataframes_seq = []\n        for k, v in seq_features.items():\n            data_lst = [usr[len_mask[i]] for i, usr in enumerate(v)]\n            dataframes_seq.append(pd.DataFrame(zip(data_lst), columns=[k]))\n\n\n        dataframes_expand = []\n        for k, v in expand_features.items():\n            for i, usr in enumerate(v):\n                exp_num = usr.shape[1] if len(usr.shape) == 2 else usr.shape[0]\n                df_trx = pd.DataFrame([usr], columns=[f'{k}_{j:04d}' for j in range(exp_num)])\n                dataframes_expand.append(df_trx)\n\n        return dataframes_scalar, dataframes_seq, dataframes_expand\n\n    @staticmethod\n    def to_pandas_sequence(x, expand_features, scalar_features, seq_features, len_mask):\n        dataframes_scalar = []\n        for k, v in scalar_features.items():\n            data_lst = [[data]*np.sum(len_mask[i]) for i, data in enumerate(v)]\n            data_lst = list(chain(*data_lst))\n            dataframes_scalar.append(pd.DataFrame(data_lst, columns=[k]))\n        dataframes_scalar = pd.concat(dataframes_scalar, axis = 1)\n\n        dataframes_seq = []\n        for k, v in seq_features.items():\n            data_lst = [data[len_mask[i]] for i, data in enumerate(v)]\n            data_lst = list(chain(*data_lst))\n            dataframes_seq.append(pd.DataFrame(data_lst, columns=[k]))\n\n        dataframes_expand = []\n        for k, v in expand_features.items():\n            for i, usr in enumerate(v):\n                exp_num = usr.shape[1] if len(usr.shape) == 2 else usr.shape[0]\n                df_trx = pd.DataFrame(usr[len_mask[i]], columns=[f'{k}_{j:04d}' for j in range(exp_num)])\n                dataframes_expand.append(df_trx)\n\n        return dataframes_scalar, dataframes_seq, dataframes_expand","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2ed3be22-47d9-44aa-865f-13ffcd5e78f7","cell_type":"code","source":"inference_module = InferenceModule(\n        model=pl_module,\n        pandas_output=True,\n        drop_seq_features=True)\n        # model_out_name=f'emb_{random_seed}')\n\npredict = trainer.predict(inference_module, inference_dl)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"58e77343-0ed4-445c-98ec-30e579ad4b3d","cell_type":"code","source":"inference_embeddings = pd.concat(predict, axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"657d5c45-a22a-4d36-bfe4-0230989d3ef3","cell_type":"code","source":"inference_embeddings_t = inference_embeddings.merge(test_[['user_id', 'target']], how='left', on='user_id')\n\ninf_emb_train, inf_emb_test = train_test_split(inference_embeddings_t, random_state=42, test_size=0.2)\n\nX_train, y_train = inf_emb_train.drop(columns=['user_id', 'target']), inf_emb_train['target']\nX_test, y_test = inf_emb_test.drop(columns=['user_id', 'target']), inf_emb_test['target']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e1f6478f-a496-4370-951d-fc81e7eaf825","cell_type":"code","source":"from lightgbm import LGBMClassifier\n\ndown_model = LGBMClassifier(\n    n_estimators=500,\n    boosting_type='gbdt',\n    subsample=0.5,\n    subsample_freq=1,\n    learning_rate=0.02,\n    feature_fraction=0.75,\n    max_depth=6,\n    lambda_l1=1,\n    lambda_l2=1,\n    min_data_in_leaf=50,\n    random_state=42,\n    n_jobs=8,\n    verbose=-1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6052e779-ddf3-4358-8156-b1c92dba90bc","cell_type":"code","source":"%%time\n\ndown_model.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d95986bb-e01a-456f-9947-007afe835a4a","cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n\npredict = down_model.predict_proba(X_test)\nprint(f\"ROC-AUC target = {roc_auc_score(y_test, predict[:, 1])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5445ed4c-9fcd-4292-9614-256b52741571","cell_type":"code","source":"predict = down_model.predict(X_test)\nprint(f\"accuracy = {accuracy_score(y_test, predict)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8a74e042-45c1-4770-afb2-2ee7d6fec415","cell_type":"markdown","source":"# CoLES with SWIN enc","metadata":{}},{"id":"a5dc2d6c-eeaf-47ab-8162-6ef84042490c","cell_type":"code","source":"trx_encoder_params = dict(\n    embeddings_noise=0.003,\n    numeric_values={'transaction_amt': 'identity',\n                    'days_from_prev_tr': 'identity',\n                   },\n    embeddings={\n        'currency_rk': {'in': 5, 'out': 8},\n        'day_of_week': {'in': 8, 'out': 8},\n        'mcc_code': {'in': 333, 'out': 16},\n        },\n    )\n\ntrain_dl = PtlsDataModule(\n    train_data=ColesIterableDataset(\n        MemoryMapDataset(\n            data=dataset_train,\n            i_filters=[\n                SeqLenFilter(min_seq_len=20)\n            ],\n        ),\n        splitter=SampleSlices(\n            split_count=5,\n            cnt_min=20,\n            cnt_max=150,\n        ),\n    ),\n    train_num_workers=2,\n    train_batch_size=128,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1a3484cb-a6be-4f94-902c-6dcae2eeb692","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nimport numpy as np\n\n\n#------------------------------------------------------------------------------------------------------------\n# Based on https://github.com/yukara-ikemiya/Swin-Transformer-1d/tree/main and adapted to pytorch-lifestream\n#------------------------------------------------------------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nfrom ptls.data_load.padded_batch import PaddedBatch\n\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n    # copied from timm/models/layers/drop.py\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:\n        random_tensor.div_(keep_prob)\n    return x * random_tensor\n\n\nclass DropPath(nn.Module):\n    # copied from timm/models/layers/drop.py\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None, scale_by_keep=True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, L, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, C)\n    \"\"\"\n    B, L, C = x.shape\n    x = x.view(B, L // window_size, window_size, C)\n    windows = x.contiguous().view(-1, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, L):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, C)\n        window_size (int): Window size\n        L (int): Length of data\n\n    Returns:\n        x: (B, L, C)\n    \"\"\"\n    B = int(windows.shape[0] / (L / window_size))\n    x = windows.view(B, L // window_size, window_size, -1)\n    x = x.contiguous().view(B, L, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (int): The width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n    def __init__(self, dim: int, window_size: int, num_heads: int,\n                 qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(2 * window_size - 1, num_heads))  # 2*window_size - 1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_w = torch.arange(self.window_size)\n        relative_coords = coords_w[:, None] - coords_w[None, :]  # W, W\n        relative_coords[:, :] += self.window_size - 1  # shift to start from 0\n\n        # relative_position_index | example\n        # [2, 1, 0]\n        # [3, 2, 1]\n        # [4, 3, 2]\n        self.register_buffer(\"relative_position_index\", relative_coords)  # (W, W): range of 0 -- 2*(W-1)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        torch.nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask_add, mask_mult):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, W, C)\n            mask: (0/-inf) mask with shape of (num_windows, W, W) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size, self.window_size, -1)  # W, W, nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, W, W\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        nW = mask_add.shape[1]\n        attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask_add\n        attn = attn.view(-1, self.num_heads, N, N)\n        attn = self.softmax(attn)\n        attn = attn * mask_mult\n        \n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        \n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n        decoder (bool, optional): Flag that shows whether this block is decoder-like (hence, attn_mask should prevent from seeing future tokens). True => decoder-like; False => encoder-like. Default: False\n        start_end_fusion (bool, optional): Flag that shows if the last and the first half-windows should merge (True) or not (False).\n    \"\"\"\n\n    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 decoder=False, start_end_fusion=True):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=self.window_size, num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        attn_mask = None\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n        self.decoder = decoder\n        self.start_end_fusion = start_end_fusion\n\n    def forward(self, x):\n        seq_lens = x.seq_lens\n        x = x.payload\n        \n        B, L, C = x.shape\n\n        # define seq_len_mask\n        mask = torch.arange(L, device=x.device)[None, :] + torch.ones((B, L), device=x.device)\n        mask[mask > seq_lens[:, None]] = 0.\n        mask[mask > 0.] = 1.\n        mask = mask[:, :, None]\n\n        # make new max seq_len `L` divisible by `self.window_size` by adding 'zero' samples\n        num_samples_to_add = self.window_size - (L % self.window_size)\n        \n        if num_samples_to_add < self.window_size:\n            additional_samples = torch.zeros((B, num_samples_to_add, C), device=x.device)\n            x = torch.cat((x, additional_samples), dim=1)\n            mask_additional_samples = torch.zeros((B, num_samples_to_add, mask.shape[2]), device=mask.device)\n            mask = torch.cat((mask, mask_additional_samples), dim=1)\n            L += num_samples_to_add\n\n        # zero out padding transactions\n        x = x * mask\n        \n        assert L >= self.window_size, f'input length ({L}) must be >= window size ({self.window_size})'\n        assert L % self.window_size == 0, f'input length ({L}) must be divisible by window size ({self.window_size})'\n\n        shortcut = x\n        x = self.norm1(x)\n\n        # shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=-self.shift_size, dims=1) # cyclic shift \n            if not self.start_end_fusion:\n                shifted_x[:, -self.shift_size:] = 0. # zero out invalid embs\n            mask = torch.roll(mask, shifts=-self.shift_size, dims=1) # cyclic shift of the mask\n            if not self.start_end_fusion:\n                mask[:, -self.shift_size:] = 0.\n        else:\n            shifted_x = x\n        \n        # partition\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, C\n        mask = window_partition(mask, self.window_size) # nW*B, window_size, 1\n        \n        # calculate attn_mask\n        attn_mask = (mask @ mask.transpose(-2, -1)) # nW*B, window_size, window_size\n        \n        if self.decoder:\n            no_look_ahead_attn_mask = 1. - torch.triu(torch.ones_like(attn_mask), diagonal=1)\n            attn_mask *= no_look_ahead_attn_mask\n        \n        attn_mask_real = attn_mask.clone().detach()\n        attn_mask_real = attn_mask_real.view(attn_mask_real.shape[0], self.window_size, self.window_size).unsqueeze(1).expand(-1, self.num_heads, -1, -1) # B*nW, nH, window_size, window_size\n        \n        attn_mask[attn_mask == 0.] = -torch.inf\n        attn_mask[attn_mask == 1.] = 0.\n        attn_mask[:, torch.arange(attn_mask.shape[-1]), torch.arange(attn_mask.shape[-1])] = 0.\n        attn_mask = attn_mask.view(B, attn_mask.shape[0] // B, self.window_size, self.window_size).unsqueeze(2).expand(-1, -1, self.num_heads, -1, -1) # B, nW, nH, window_size, window_size\n        \n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask_add=attn_mask, mask_mult=attn_mask_real)  # nW*B, window_size, C\n        \n        # merge windows\n        shifted_x = window_reverse(attn_windows, self.window_size, L)  # (B, L, C)\n\n        # reverse zero-padding shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=self.shift_size, dims=1) # cyclic shift\n            if not self.start_end_fusion:\n                x[:, :self.shift_size] = 0. # zero out invalid embs\n        else:\n            x = shifted_x\n\n        x = shortcut + self.drop_path(x)\n\n        # FFN\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        \n        return PaddedBatch(x, seq_lens)\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n\nclass SwinTransformerLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        decoder (bool, optional): Flag that shows whether blocks in this layer are decoder-like. True => decoder-like; False => encoder-like. Default: False\n        start_end_fusion (bool, optional): Flag that shows if the last and the first half-windows should merge (True) or not (False).\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        depth: int,\n        num_heads: int,\n        window_size: int,\n        mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n        drop_path=0., norm_layer=nn.LayerNorm,\n        decoder=False, start_end_fusion=True\n    ):\n        super().__init__()\n        self.dim = dim\n        self.depth = depth\n        self.num_heads = num_heads\n        self.window_size = window_size\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer,\n                                 decoder=decoder,\n                                 start_end_fusion=start_end_fusion)\n            for i in range(depth)])\n\n    def forward(self, x):\n        for blk in self.blocks:\n            x = blk(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, depth={self.depth}, num_heads={self.num_heads}, window_size={self.window_size}\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"80177510-ceae-4a10-84de-8f062de73e8b","cell_type":"code","source":"import warnings\n\nimport torch\n\n# from ptls.constant_repository import TORCH_EMB_DTYPE\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom ptls.nn.trx_encoder.batch_norm import RBatchNorm, RBatchNormWithLens\nfrom ptls.nn.trx_encoder.noisy_embedding import NoisyEmbedding\nfrom ptls.nn.trx_encoder.trx_encoder_base import TrxEncoderBase\n\nTORCH_EMB_DTYPE = torch.float32\n\nclass TrxEncoderSWIN(TrxEncoderBase):\n    \"\"\"Network layer which makes representation for single transactions\n\n     Input is `PaddedBatch` with ptls-format dictionary, with feature arrays of shape (B, T)\n     Output is `PaddedBatch` with transaction embeddings of shape (B, T, H)\n     where:\n        B - batch size, sequence count in batch\n        T - sequence length\n        H - hidden size, representation dimension\n\n    `ptls.nn.trx_encoder.noisy_embedding.NoisyEmbedding` implementation are used for categorical features.\n\n    Parameters\n        embeddings:\n            dict with categorical feature names.\n            Values must be like this `{'in': dictionary_size, 'out': embedding_size}`\n            These features will be encoded with lookup embedding table of shape (dictionary_size, embedding_size)\n            Values can be a `torch.nn.Embedding` implementation\n        numeric_values:\n            dict with numerical feature names.\n            Values must be a string with scaler_name.\n            Possible values are: 'identity', 'sigmoid', 'log', 'year'.\n            These features will be scaled with selected scaler.\n            Values can be `ptls.nn.trx_encoder.scalers.BaseScaler` implementatoin\n\n            One field can have many scalers. In this case key become alias and col name should be in scaler.\n            Check `TrxEncoderBase.numeric_values` for more details\n\n        embeddings_noise (float):\n            Noise level for embedding. `0` meens without noise\n        emb_dropout (float):\n            Probability of an element of embedding to be zeroed\n        spatial_dropout (bool):\n            Whether to dropout full dimension of embedding in the whole sequence\n\n        use_batch_norm:\n            True - All numerical values will be normalized after scaling\n            False - No normalizing for numerical values\n        use_batch_norm_with_lens:\n            True - Respect seq_lens during batch_norm. Padding zeroes will be ignored\n            False - Batch norm ower all time axis. Padding zeroes will included.\n\n        orthogonal_init:\n            if True then `torch.nn.init.orthogonal` applied\n        linear_projection_size:\n            Linear layer at the end will be added for non-zero value\n\n        out_of_index:\n            How to process a categorical indexes which are greater than dictionary size.\n            'clip' - values will be collapsed to maximum index. This works well for frequency encoded categories.\n                We join infrequent categories to one.\n            'assert' - raise an error of invalid index appear.\n\n        norm_embeddings: keep default value for this parameter\n        clip_replace_value: Not useed. keep default value for this parameter\n        positions: Not used. Keep default value for this parameter\n\n    Examples:\n        >>> B, T = 5, 20\n        >>> trx_encoder = TrxEncoder(\n        >>>     embeddings={'mcc_code': {'in': 100, 'out': 5}},\n        >>>     numeric_values={'amount': 'log'},\n        >>> )\n        >>> x = PaddedBatch(\n        >>>     payload={\n        >>>         'mcc_code': torch.randint(0, 99, (B, T)),\n        >>>         'amount': torch.randn(B, T),\n        >>>     },\n        >>>     length=torch.randint(10, 20, (B,)),\n        >>> )\n        >>> z = trx_encoder(x)\n        >>> assert z.payload.shape == (5, 20, 6)  # B, T, H\n    \"\"\"\n    def __init__(self,\n                 embeddings=None,\n                 numeric_values=None,\n                 custom_embeddings=None,\n                 embeddings_noise: float = 0,\n                 norm_embeddings=None,\n                 use_batch_norm=False,\n                 use_batch_norm_with_lens=False,\n                 clip_replace_value=None,\n                 positions=None,\n                 emb_dropout=0,\n                 spatial_dropout=False,\n                 orthogonal_init=False,\n                 linear_projection_size=0,\n                 out_of_index: str = 'clip',\n                 ):\n        if clip_replace_value is not None:\n            warnings.warn('`clip_replace_value` attribute is deprecated. Always \"clip to max\" used. '\n                          'Use `out_of_index=\"assert\"` to avoid categorical values clip', DeprecationWarning)\n\n        if positions is not None:\n            warnings.warn('`positions` is deprecated. positions is not used', UserWarning)\n\n        if embeddings is None:\n            embeddings = {}\n        if custom_embeddings is None:\n            custom_embeddings = {}\n\n        noisy_embeddings = {}\n        for emb_name, emb_props in embeddings.items():\n            if emb_props.get('disabled', False):\n                continue\n            if emb_props['in'] == 0 or emb_props['out'] == 0:\n                continue\n            noisy_embeddings[emb_name] = NoisyEmbedding(\n                num_embeddings=emb_props['in'],\n                embedding_dim=emb_props['out'],\n                padding_idx=0,\n                max_norm=1 if norm_embeddings else None,\n                noise_scale=embeddings_noise,\n                dropout=emb_dropout,\n                spatial_dropout=spatial_dropout,\n            )\n\n        super().__init__(\n            embeddings=noisy_embeddings,\n            numeric_values=numeric_values,\n            custom_embeddings=custom_embeddings,\n            out_of_index=out_of_index,\n        )\n        self.swin_encoder = SwinTransformerV2Layer(\n            num_heads=5,\n            depth=3,\n            dim=225,\n            window_size=15\n        )\n        custom_embedding_size = self.custom_embedding_size\n        if use_batch_norm and custom_embedding_size > 0:\n            # :TODO: Should we use Batch norm with not-numerical custom embeddings?\n            if use_batch_norm_with_lens:\n                self.custom_embedding_batch_norm = RBatchNormWithLens(custom_embedding_size)\n            else:\n                self.custom_embedding_batch_norm = RBatchNorm(custom_embedding_size)\n        else:\n            self.custom_embedding_batch_norm = None\n\n        if linear_projection_size > 0:\n            self.linear_projection_head = torch.nn.Linear(super().output_size, linear_projection_size)\n        else:\n            self.linear_projection_head = None\n\n        if orthogonal_init:\n            for n, p in self.named_parameters():\n                if n.startswith('embeddings.') and n.endswith('.weight'):\n                    torch.nn.init.orthogonal_(p.data[1:])\n                if n == 'linear_projection_head.weight':\n                    torch.nn.init.orthogonal_(p.data)\n\n    def forward(self, x: PaddedBatch, names=None, seq_len=None):\n        if isinstance(x, PaddedBatch) is False:\n            pre_x = dict()\n            for i, field_name in enumerate(names):\n                pre_x[field_name] = x[i]\n            x = PaddedBatch(pre_x, seq_len)\n\n        processed_embeddings = [self.get_category_embeddings(x, field_name)\n                                for field_name in self.embeddings.keys()]\n        processed_custom_embeddings = [self.get_custom_embeddings(x, field_name)\n                                       for field_name in self.custom_embeddings.keys()]\n        if len(processed_custom_embeddings):\n            processed_custom_embeddings = torch.cat(processed_custom_embeddings, dim=2)\n            if self.custom_embedding_batch_norm is not None:\n                processed_custom_embeddings = PaddedBatch(processed_custom_embeddings, x.seq_lens)\n                processed_custom_embeddings = self.custom_embedding_batch_norm(processed_custom_embeddings)\n                processed_custom_embeddings = processed_custom_embeddings.payload\n            processed_embeddings.append(processed_custom_embeddings)\n\n        out = torch.cat(processed_embeddings, dim=2)\n        out = out.type(TORCH_EMB_DTYPE)\n        out = self.linear_projection_head(out) if self.linear_projection_head is not None else out\n        # print(out.size())\n        out = PaddedBatch(out, x.seq_lens)\n        out = self.swin_encoder(out)\n        return out\n\n    @property\n    def output_size(self):\n        \"\"\"Returns hidden size of output representation\n        \"\"\"\n        if self.linear_projection_head is not None:\n            return self.linear_projection_head.out_features\n        return super().output_size\n\nfrom ptls.nn.seq_encoder.rnn_encoder import RnnEncoder\nfrom ptls.nn.seq_encoder.containers import SeqEncoderContainer\n\n\nclass SwinTransformerBackbone(nn.Module):\n    \"\"\" Swin Transformer Backbone (4 stages as in orig. 2D impl.).\n\n    Args:\n        dim (int): Number of input channels.\n        depths (list[int]): Numbers of blocks in stages.\n        num_heads (int): Number of attention heads in W-MSA layers.\n        start_window_size (int): Local window size of stage 1.\n        window_size_mult (int): the number by which the `window_size` is being multiplied when moving to another stage\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        decoder (bool, optional): Flag that shows whether blocks in this backbone are decoder-like. True => decoder-like; False => encoder-like. Default: False\n        start_end_fusion (bool, optional): Flag that shows if the last and the first half-windows should merge (True) or not (False).\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        depths: list[int],\n        num_heads,\n        start_window_size: int,\n        window_size_mult: int = 1,\n        mlp_ratio=4.,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.,\n        attn_drop=0.,\n        drop_path=0.,\n        norm_layer=nn.LayerNorm,\n        decoder=False,\n        start_end_fusion=True\n    ):\n        super().__init__()\n        self.dim = dim\n        self.depths = depths\n        \n        if type(num_heads) == int:\n            self.num_heads = [num_heads] * len(depths)\n        else:\n            self.num_heads = num_heads\n        \n        self.window_sizes = [start_window_size]\n        \n        for i in range(len(self.depths) - 1):\n            self.window_sizes += [self.window_sizes[-1] * window_size_mult]\n\n        # build model\n        self.backbone = nn.ModuleList([\n            SwinTransformerLayer(dim=self.dim,\n                                 depth=self.depths[i],\n                                 num_heads=self.num_heads[i],\n                                 window_size=self.window_sizes[i],\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias,\n                                 qk_scale=qk_scale,\n                                 drop=drop,\n                                 attn_drop=attn_drop,\n                                 drop_path=drop_path,\n                                 norm_layer=norm_layer,\n                                 decoder=decoder,\n                                 start_end_fusion=start_end_fusion)\n            for i in range(len(self.depths))])\n\n    def forward(self, x):\n        for layer in self.backbone:\n            x = layer(x)\n        return x\n\n\nclass SWIN_RNN_SeqEncoder(SeqEncoderContainer):\n    \"\"\"SeqEncoderContainer with SWIN transformer backbone for features hierarchic fusion and RnnEncoder for feature aggregation.\n    \n    Parameters\n        trx_encoder:\n            TrxEncoder object\n        input_size:\n            input_size parameter for RnnEncoder\n            If None: input_size = trx_encoder.output_size\n            Set input_size explicitly or use None if your trx_encoder object has output_size attribute\n        is_reduce_sequence:\n            False - returns PaddedBatch with all transactions embeddings\n            True - returns one embedding for sequence based on CLS token\n        swin_depths: Numbers of blocks in stages (SWIN backbone).\n        swin_num_heads: Number of attention heads in W-MSA layers (SWIN backbone).\n        swin_start_window_size: Local window size of stage 1 (SWIN backbone).\n        swin_window_size_mult (int): the number by which the `window_size` is being multiplied when moving to another stage (SWIN backbone).\n        swin_drop: Dropout rate (SWIN backbone). Default: 0.0\n        swin_attn_drop: Attention dropout rate (SWIN backbone). Default: 0.0\n        swin_drop_path: Stochastic depth rate (SWIN backbone). Default: 0.0\n        swin_decoder: Flag that shows whether blocks in SWIN backbone are decoder-like. True => decoder-like; False => encoder-like. Default: False\n        swin_start_end_fusion: Flag that shows if the last and the first half-windows should merge (True) or not (False). Must be False for CPC and GPT.\n        **rnn_seq_encoder_params:\n            RnnEncoder params\n    \"\"\"\n    def __init__(self,\n                 trx_encoder=None,\n                 input_size=None,\n                 is_reduce_sequence=True,\n                 swin_depths=[],\n                 swin_num_heads=4,\n                 swin_start_window_size=4,\n                 swin_window_size_mult=1,\n                 swin_drop=0.,\n                 swin_attn_drop=0.,\n                 swin_drop_path=0.,\n                 swin_decoder=False,\n                 swin_start_end_fusion=True,\n                 **rnn_seq_encoder_params\n                 ):\n        super().__init__(\n            trx_encoder=trx_encoder,\n            seq_encoder_cls=RnnEncoder,\n            input_size=input_size,\n            seq_encoder_params=rnn_seq_encoder_params,\n            is_reduce_sequence=is_reduce_sequence,\n        )\n        self.swin_fusion = SwinTransformerBackbone(\n                               dim=trx_encoder.output_size,\n                               depths=swin_depths,\n                               num_heads=swin_num_heads,\n                               start_window_size=swin_start_window_size,\n                               window_size_mult=swin_window_size_mult,\n                               drop=swin_drop,\n                               attn_drop=swin_attn_drop,\n                               drop_path=swin_drop_path,\n                               decoder=swin_decoder,\n                               start_end_fusion=swin_start_end_fusion \n                              )\n\n    def forward(self, x, names=None, seq_len=None, h_0=None):\n        x = self.trx_encoder(x)\n        x = self.swin_fusion(x)\n        x = self.seq_encoder(x, h_0)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cc153d0f-18b2-4040-9d5d-74297419d3fd","cell_type":"code","source":"from ptls.nn import TabFormerFeatureEncoder\nfrom ptls.nn import TransformerEncoder\nfrom ptls.nn import TrxEncoder\nimport ptls\n\n# trx_encoder_params = dict(\n#     embeddings_noise=0.003,\n#     numeric_values={'transaction_amt': 'identity',\n#                     'days_from_prev_tr': 'identity',\n#                    },\n#     embeddings={\n#         'currency_rk': {'in': 5, 'out': 8},\n#         'day_of_week': {'in': 8, 'out': 8},\n#         'mcc_code': {'in': 333, 'out': 16},\n#         },\n#     )\n\ntrx_encoder = ptls.nn.TrxEncoder(\n            embeddings_noise=0.003,\n            # numeric_values={'transaction_amt': 'identity',\n            #         'days_from_prev_tr': 'identity',\n            #        },\n            embeddings={\n            'transaction_amt': {'in': 11 , 'out': 16},\n            'days_from_prev_tr': {'in':10 , 'out': 16},\n            'currency_rk': {'in': 5, 'out': 16},\n            'day_of_week': {'in': 8, 'out': 16},\n            'mcc_code': {'in': 333, 'out': 16},\n            },\n        )\n\n# seq_encoder_params = dict(\n#         trx_encoder=TrxEncoder(**trx_encoder_params),\n#         hidden_size=512,\n#         type='gru',\n# )\n\noptimizer_partial = partial(\n    torch.optim.AdamW,\n    lr=1e-3,\n    weight_decay=1e-4\n)\n\nlr_scheduler_partial = partial(\n    torch.optim.lr_scheduler.StepLR,\n    step_size=2,\n    gamma=0.9025\n)\n\n\nseq_encoder = SWIN_RNN_SeqEncoder(\n    trx_encoder=trx_encoder,\n    swin_depths=[2, 2, 6, 2],\n    swin_num_heads=[2, 4, 8, 16],\n    swin_start_window_size=4,\n    swin_window_size_mult=2,\n    swin_drop=0.1,\n    swin_attn_drop=0.1,\n    swin_drop_path=0.1,\n    swin_decoder=True,\n    swin_start_end_fusion=False,\n    hidden_size=512,\n    type=\"gru\")\n\npl_module = ptls.frames.coles.CoLESModule(\n    # validation_metric=ptls.frames.coles.metric.BatchRecallTopK(\n    #     K=4,\n    #     metric='cosine'\n    # ),\n    seq_encoder=seq_encoder,\n    optimizer_partial=optimizer_partial,\n    lr_scheduler_partial=lr_scheduler_partial\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3a1be7ed-a064-4945-91da-c1d9b697b10a","cell_type":"code","source":"from lightning.pytorch.loggers import WandbLogger\n\nwandb_logger = WandbLogger(project=\"MBD_My_Code\", log_model=\"all\", name=\"df24_coles_swin\")\n\ntrainer = pl.Trainer(\n    logger=wandb_logger,\n    max_epochs=20,\n    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    enable_progress_bar=True,\n    gradient_clip_val=0.3,\n    log_every_n_steps=50,\n    limit_val_batches=32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ae9a00b7-3d30-4d7b-9da5-48766f470696","cell_type":"code","source":"trainer.fit(pl_module, train_dl)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b230cf91-8c89-491d-9978-b9c04da3138d","cell_type":"code","source":"inference_module = InferenceModule(\n        model=pl_module,\n        pandas_output=True,\n        drop_seq_features=True)\n        # model_out_name=f'emb_{random_seed}')\n\npredict = trainer.predict(inference_module, inference_dl)\n\ninference_embeddings = pd.concat(predict, axis=0)\n\ninference_embeddings_t = inference_embeddings.merge(test_[['user_id', 'target']], how='left', on='user_id')\n\ninf_emb_train, inf_emb_test = train_test_split(inference_embeddings_t, random_state=42, test_size=0.2)\n\nX_train, y_train = inf_emb_train.drop(columns=['user_id', 'target']), inf_emb_train['target']\nX_test, y_test = inf_emb_test.drop(columns=['user_id', 'target']), inf_emb_test['target']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a6c59d9b-fd0f-4d58-8978-91e6bd48c2b6","cell_type":"code","source":"from lightgbm import LGBMClassifier\n\ndown_model = LGBMClassifier(\n    n_estimators=700,\n    boosting_type='gbdt',\n    subsample=0.5,\n    subsample_freq=1,\n    learning_rate=0.02,\n    feature_fraction=0.75,\n    max_depth=6,\n    lambda_l1=1,\n    lambda_l2=1,\n    min_data_in_leaf=50,\n    random_state=42,\n    n_jobs=8,\n    verbose=-1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b99e544e-e095-459e-9a66-e6e2dae4f38e","cell_type":"code","source":"%%time\n\ndown_model.fit(X_train, y_train)\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n\npredict = down_model.predict_proba(X_test)\nprint(f\"ROC-AUC target = {roc_auc_score(y_test, predict[:, 1])}\")\n\npredict = down_model.predict(X_test)\nprint(f\"accuracy = {accuracy_score(y_test, predict)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"94ec378e-589a-4e71-8738-843a75255e62","cell_type":"markdown","source":"# Results\n\n**Baseline (CoLES)**\n\nROC-AUC target = 0.6885229034842104\n\naccuracy = 0.91\n\n              precision    recall  f1-score   support\n\n           0       0.91      1.00      0.95      2326\n           \n           1       0.69      0.05      0.09       234\n\n    accuracy                           0.91      2560\n    macro avg       0.80      0.52      0.52      2560\n    weighted avg       0.89      0.91      0.87      2560\n\n\n**CoLES + regional-attention**\n\nROC-AUC target = 0.705245790800391\n\naccuracy = 0.91\n\n              precision    recall  f1-score   support\n\n           0       0.91      1.00      0.95      2326\n           1       0.58      0.05      0.09       234\n\n    accuracy                           0.91      2560\n    macro avg       0.75      0.52      0.52      2560\n    weigh avg       0.88      0.91      0.87      2560\n\n**CoLES + cross-attention**\n\nsmall patches = 4\n\nlarge patches = 15\n\nROC-AUC target = 0.6635\n\naccuracy = 0.90\n\n**CoLES + cross-attention**\n\nsmall patches = 5\n\nlarge patches = 16\n\nROC-AUC target = 0.6513\n\naccuracy = 0.90\n\n**CoLES + cross-attention**\n\nsmall patches = 3\n\nlarge patches = 12\n\nROC-AUC target = 0.6671\n\naccuracy = 0.91\n\n**GPT Baseline**\n\nROC-AUC target = 0.7306271092054157\n\naccuracy = 0.9293908996750897\n\n**GPT With SWIN enc**\n\nROC-AUC target = 0.7012497152222003\n\naccuracy = 0.909765625","metadata":{}},{"id":"ec1bc5c7-006e-43b9-b76d-b20ac013e844","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fe8f6033-8803-4724-ae2f-31df774a36b1","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"749a414b-4970-47ec-a29e-0b78bb8f9c8c","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}