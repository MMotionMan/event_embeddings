{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyNf/d19uVxFxPMfFmtk3j7z"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Импортируем необходимые библиотеки","metadata":{}},{"cell_type":"code","source":"!pip install pytorch-lifestream\n!pip install comet_ml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:04:58.576070Z","iopub.execute_input":"2025-05-09T15:04:58.576369Z","iopub.status.idle":"2025-05-09T15:05:16.015628Z","shell.execute_reply.started":"2025-05-09T15:04:58.576336Z","shell.execute_reply":"2025-05-09T15:05:16.014668Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting pytorch-lifestream\n  Downloading pytorch-lifestream-0.6.0.tar.gz (163 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.4/163.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: duckdb in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (1.1.3)\nCollecting hydra-core>=1.1.2 (from pytorch-lifestream)\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: numpy>=1.21.5 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (1.26.4)\nRequirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (2.3.0)\nRequirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (2.2.3)\nRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (19.0.1)\nRequirement already satisfied: pytorch-lightning>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (2.5.0.post0)\nRequirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (1.2.2)\nRequirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (2.5.1+cu121)\nRequirement already satisfied: torchmetrics>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (1.6.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from pytorch-lifestream) (4.47.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1.2->pytorch-lifestream) (4.9.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1.2->pytorch-lifestream) (24.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.5->pytorch-lifestream) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.5->pytorch-lifestream) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.5->pytorch-lifestream) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.5->pytorch-lifestream) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.5->pytorch-lifestream) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.5->pytorch-lifestream) (2.4.1)\nRequirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf->pytorch-lifestream) (6.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-lifestream) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-lifestream) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-lifestream) (2025.1)\nRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.0->pytorch-lifestream) (4.67.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (2024.12.0)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.0->pytorch-lifestream) (4.12.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.0->pytorch-lifestream) (0.12.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-lifestream) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-lifestream) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-lifestream) (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lifestream) (3.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lifestream) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lifestream) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lifestream) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.12.0->pytorch-lifestream) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-lifestream) (0.29.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-lifestream) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-lifestream) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-lifestream) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-lifestream) (0.4.5)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (3.11.12)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (75.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-lifestream) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lifestream) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.5->pytorch-lifestream) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.5->pytorch-lifestream) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.5->pytorch-lifestream) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.5->pytorch-lifestream) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->pytorch-lifestream) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->pytorch-lifestream) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->pytorch-lifestream) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->pytorch-lifestream) (2025.1.31)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (1.18.3)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.5->pytorch-lifestream) (2024.2.0)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pytorch-lifestream\n  Building wheel for pytorch-lifestream (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pytorch-lifestream: filename=pytorch_lifestream-0.6.0-py3-none-any.whl size=274670 sha256=ac168cfbfeabbaa3d77eb90a3ddca67b1e0822f1a9fb2ff2b480fec5db897193\n  Stored in directory: /root/.cache/pip/wheels/90/76/b4/0a944bc7c5a69201e4d757cc54886971117a2a581740e7f11d\nSuccessfully built pytorch-lifestream\nInstalling collected packages: hydra-core, pytorch-lifestream\nSuccessfully installed hydra-core-1.3.2 pytorch-lifestream-0.6.0\nCollecting comet_ml\n  Downloading comet_ml-3.49.9-py3-none-any.whl.metadata (4.1 kB)\nCollecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n  Downloading dulwich-0.22.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (4.23.0)\nRequirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (5.9.5)\nCollecting python-box<7.0.0 (from comet_ml)\n  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\nRequirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.0.0)\nRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.32.3)\nRequirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (13.9.4)\nCollecting semantic-version>=2.8.0 (from comet_ml)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.19.2)\nCollecting simplejson (from comet_ml)\n  Downloading simplejson-3.20.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.3.0)\nRequirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.17.0)\nRequirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (3.1.1)\nCollecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.22.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (2.19.1)\nRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (4.12.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\nDownloading comet_ml-3.49.9-py3-none-any.whl (726 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.0/727.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading dulwich-0.22.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\nDownloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading simplejson-3.20.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\nInstalling collected packages: everett, simplejson, semantic-version, python-box, dulwich, configobj, comet_ml\n  Attempting uninstall: python-box\n    Found existing installation: python-box 7.3.0\n    Uninstalling python-box-7.3.0:\n      Successfully uninstalled python-box-7.3.0\nSuccessfully installed comet_ml-3.49.9 configobj-5.0.9 dulwich-0.22.8 everett-3.1.0 python-box-6.1.0 semantic-version-2.10.0 simplejson-3.20.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# data preprocessing\nimport os\nimport numpy as np \nimport pandas as pd\nimport pickle\n\n# misc\nfrom tqdm import tqdm\nfrom functools import partial\n\n# logging\nimport comet_ml \n\n# classical ML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom catboost import CatBoostClassifier\n\n# basic deep learning libs\nimport torch\nimport pytorch_lightning as pl\nimport torchmetrics\n\n# ptls\nfrom ptls.nn import TrxEncoder, RnnSeqEncoder, TransformerEncoder, GptEncoder, Head\nfrom ptls.frames import PtlsDataModule\nfrom ptls.frames.coles import CoLESModule\nfrom ptls.frames.coles import ColesDataset\nfrom ptls.frames.coles.split_strategy import SampleSlices\nfrom ptls.frames.cpc import CpcModule\nfrom ptls.frames.cpc import CpcDataset\nfrom ptls.frames.gpt import GptDataset\nfrom ptls.frames.supervised import SeqToTargetDataset, SequenceToTarget\nfrom ptls.data_load.datasets import MemoryMapDataset\nfrom ptls.data_load.datasets import inference_data_loader\nfrom ptls.frames.inference_module import InferenceModule\nfrom ptls.data_load.iterable_processing import SeqLenFilter\nfrom ptls.preprocessing import PandasDataPreprocessor\nfrom ptls.data_load.utils import collate_feature_dict\nfrom ptls.frames.inference_module import InferenceModule\nfrom ptls.frames.coles.losses.softmax_loss import SoftmaxLoss","metadata":{"id":"4KxO1qd6hUTG","executionInfo":{"status":"ok","timestamp":1732826830664,"user_tz":-180,"elapsed":233,"user":{"displayName":"Антон Коротков","userId":"12465883653538096788"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:05:34.237957Z","iopub.execute_input":"2025-05-09T15:05:34.238285Z","iopub.status.idle":"2025-05-09T15:06:01.480279Z","shell.execute_reply.started":"2025-05-09T15:05:34.238257Z","shell.execute_reply":"2025-05-09T15:06:01.479337Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def seed_everything(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:06:48.037224Z","iopub.execute_input":"2025-05-09T15:06:48.037914Z","iopub.status.idle":"2025-05-09T15:06:48.042009Z","shell.execute_reply.started":"2025-05-09T15:06:48.037886Z","shell.execute_reply":"2025-05-09T15:06:48.041267Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"comet_ml.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:06:53.549380Z","iopub.execute_input":"2025-05-09T15:06:53.549702Z","iopub.status.idle":"2025-05-09T15:06:54.332726Z","shell.execute_reply.started":"2025-05-09T15:06:53.549678Z","shell.execute_reply":"2025-05-09T15:06:54.331821Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from pytorch_lightning.loggers import CometLogger","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:06:55.448212Z","iopub.execute_input":"2025-05-09T15:06:55.448546Z","iopub.status.idle":"2025-05-09T15:06:55.452071Z","shell.execute_reply.started":"2025-05-09T15:06:55.448509Z","shell.execute_reply":"2025-05-09T15:06:55.451269Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Time2Vec:**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom ptls.data_load.padded_batch import PaddedBatch\nfrom ptls.nn.trx_encoder.batch_norm import RBatchNorm, RBatchNormWithLens\nfrom ptls.nn.trx_encoder.noisy_embedding import NoisyEmbedding\nfrom ptls.nn.trx_encoder.trx_encoder_base import TrxEncoderBase\nimport torch.nn as nn\n\n\nclass Time2Vec(nn.Module):\n    def __init__(self, k, interval=86400):\n        super(Time2Vec, self).__init__()\n        self.k = k\n        self.w = nn.Parameter(torch.randn(k))\n        self.b = nn.Parameter(torch.randn(k))\n        self.w0 = nn.Parameter(torch.randn(1))\n        self.b0 = nn.Parameter(torch.randn(1))\n        self.interval = interval\n        \n    def forward(self, event_time, t0):\n        t0_ = torch.zeros_like(event_time)\n        time_diff=None\n        if type(t0)!=int:\n            first_column = t0[:, 0].unsqueeze(1)\n            t0_ = first_column.expand(-1, t0.size(1))\n        time_diff = (event_time - t0_)/self.interval\n        v1 = self.w0 * time_diff.unsqueeze(-1) + self.b0\n        v2 = torch.cos(self.w * time_diff.unsqueeze(-1) + self.b)\n        \n        return torch.cat([v1, v2], -1)\n\n        \nclass TrxEncoderT2V(TrxEncoderBase):\n    def __init__(self,\n                 embeddings=None,\n                 numeric_values=None,\n                 custom_embeddings=None,\n                 time_values=None,\n                 embeddings_noise: float = 0,\n                 norm_embeddings=None,\n                 use_batch_norm=True,\n                 use_batch_norm_with_lens=False,\n                 clip_replace_value=None,\n                 positions=None,\n                 emb_dropout=0,\n                 spatial_dropout=False,\n                 orthogonal_init=False,\n                 linear_projection_size=0,\n                 out_of_index: str = 'clip',\n                 k=2,\n                 time_col='event_time'\n                 ):\n        if clip_replace_value is not None:\n            warnings.warn('`clip_replace_value` attribute is deprecated. Always \"clip to max\" used. '\n                          'Use `out_of_index=\"assert\"` to avoid categorical values clip', DeprecationWarning)\n\n        if positions is not None:\n            warnings.warn('`positions` is deprecated. positions is not used', UserWarning)\n\n        if embeddings is None:\n            embeddings = {}\n        if custom_embeddings is None:\n            custom_embeddings = {}\n        if time_values is None:\n            time_values = {}\n\n        noisy_embeddings = {}\n        for emb_name, emb_props in embeddings.items():\n            if emb_props.get('disabled', False):\n                continue\n            if emb_props['in'] == 0 or emb_props['out'] == 0:\n                continue\n            noisy_embeddings[emb_name] = NoisyEmbedding(\n                num_embeddings=emb_props['in'],\n                embedding_dim=emb_props['out'],\n                padding_idx=0,\n                max_norm=1 if norm_embeddings else None,\n                noise_scale=embeddings_noise,\n                dropout=emb_dropout,\n                spatial_dropout=spatial_dropout,\n            )\n\n        super().__init__(\n            embeddings=noisy_embeddings,\n            numeric_values=numeric_values,\n            custom_embeddings=custom_embeddings,\n            out_of_index=out_of_index,\n        )\n\n        custom_embedding_size = self.custom_embedding_size\n        if use_batch_norm and custom_embedding_size > 0:\n            # :TODO: Should we use Batch norm with not-numerical custom embeddings?\n            if use_batch_norm_with_lens:\n                self.custom_embedding_batch_norm = RBatchNormWithLens(custom_embedding_size)\n            else:\n                self.custom_embedding_batch_norm = RBatchNorm(custom_embedding_size)\n        else:\n            self.custom_embedding_batch_norm = None\n        \n        self.k = k\n        self.time2vec_days = Time2Vec(k=self.k)\n        self.time_col = time_col\n        \n        if linear_projection_size > 0:\n            self.linear_projection_head = torch.nn.Linear(super().output_size+k+1, linear_projection_size)\n        else:\n            self.linear_projection_head = None\n            \n\n        if orthogonal_init:\n            for n, p in self.named_parameters():\n                if n.startswith('embeddings.') and n.endswith('.weight'):\n                    torch.nn.init.orthogonal_(p.data[1:])\n                if n == 'linear_projection_head.weight':\n                    torch.nn.init.orthogonal_(p.data)\n\n    def forward(self, x: PaddedBatch):\n        processed_embeddings = []\n        processed_custom_embeddings = []\n\n        for field_name in self.embeddings.keys():\n            processed_embeddings.append(self.get_category_embeddings(x, field_name))\n        \n        for field_name in self.custom_embeddings.keys():\n            processed_custom_embeddings.append(self.get_custom_embeddings(x, field_name))\n\n        if len(processed_custom_embeddings):\n            processed_custom_embeddings = torch.cat(processed_custom_embeddings, dim=2)\n            if self.custom_embedding_batch_norm is not None:\n                processed_custom_embeddings = PaddedBatch(processed_custom_embeddings, x.seq_lens)\n                processed_custom_embeddings = self.custom_embedding_batch_norm(processed_custom_embeddings)\n                processed_custom_embeddings = processed_custom_embeddings.payload\n            processed_embeddings.append(processed_custom_embeddings)\n\n        out = torch.cat(processed_embeddings, dim=2)\n\n        time_encoded_days = self.time2vec_days(x.payload[self.time_col], x.payload[self.time_col])\n        out = torch.cat((out, time_encoded_days), dim=2)\n\n        if self.linear_projection_head is not None:\n            out = self.linear_projection_head(out)\n        return PaddedBatch(out, x.seq_lens)\n\n    @property\n    def output_size(self):\n        \"\"\"Returns hidden size of output representation\n        \"\"\"\n        if self.linear_projection_head is not None:\n            return self.linear_projection_head.out_features\n        return super().output_size + self.k + 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:06:56.905735Z","iopub.execute_input":"2025-05-09T15:06:56.906015Z","iopub.status.idle":"2025-05-09T15:06:56.922344Z","shell.execute_reply.started":"2025-05-09T15:06:56.905994Z","shell.execute_reply":"2025-05-09T15:06:56.921416Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**SWIN1D_Encoder (orig. implementation by Yukara Ikemiya):**","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------------------------------------------------------\n# Based on https://github.com/yukara-ikemiya/Swin-Transformer-1d/tree/main and adapted to pytorch-lifestream\n#------------------------------------------------------------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nfrom ptls.data_load.padded_batch import PaddedBatch\n\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n    # copied from timm/models/layers/drop.py\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:\n        random_tensor.div_(keep_prob)\n    return x * random_tensor\n\n\nclass DropPath(nn.Module):\n    # copied from timm/models/layers/drop.py\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None, scale_by_keep=True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, L, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, C)\n    \"\"\"\n    B, L, C = x.shape\n    x = x.view(B, L // window_size, window_size, C)\n    windows = x.contiguous().view(-1, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, L):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, C)\n        window_size (int): Window size\n        L (int): Length of data\n\n    Returns:\n        x: (B, L, C)\n    \"\"\"\n    B = int(windows.shape[0] / (L / window_size))\n    x = windows.view(B, L // window_size, window_size, -1)\n    x = x.contiguous().view(B, L, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (int): The width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim: int, window_size: int, num_heads: int,\n                 qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(2 * window_size - 1, num_heads))  # 2*window_size - 1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_w = torch.arange(self.window_size)\n        relative_coords = coords_w[:, None] - coords_w[None, :]  # W, W\n        relative_coords[:, :] += self.window_size - 1  # shift to start from 0\n\n        # relative_position_index | example\n        # [2, 1, 0]\n        # [3, 2, 1]\n        # [4, 3, 2]\n        self.register_buffer(\"relative_position_index\", relative_coords)  # (W, W): range of 0 -- 2*(W-1)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        torch.nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask_add, mask_mult):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, W, C)\n            mask: (0/-inf) mask with shape of (num_windows, W, W) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size, self.window_size, -1)  # W, W, nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, W, W\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        nW = mask_add.shape[1]\n        attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask_add\n        attn = attn.view(-1, self.num_heads, N, N)\n        attn = self.softmax(attn)\n        attn = attn * mask_mult\n        \n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        \n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n        decoder (bool, optional): Flag that shows whether this block is decoder-like (hence, attn_mask should prevent from seeing future tokens). True => decoder-like; False => encoder-like. Default: False\n        start_end_fusion (bool, optional): Flag that shows if the last and the first half-windows should merge (True) or not (False).\n    \"\"\"\n\n    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 decoder=False, start_end_fusion=True):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=self.window_size, num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        attn_mask = None\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n        self.decoder = decoder\n        self.start_end_fusion = start_end_fusion\n\n    def forward(self, x):\n        seq_lens = x.seq_lens\n        x = x.payload\n        \n        B, L, C = x.shape\n\n        # define seq_len_mask\n        mask = torch.arange(L, device=x.device)[None, :] + torch.ones((B, L), device=x.device)\n        mask[mask > seq_lens[:, None]] = 0.\n        mask[mask > 0.] = 1.\n        mask = mask[:, :, None]\n\n        # make new max seq_len `L` divisible by `self.window_size` by adding 'zero' samples\n        num_samples_to_add = self.window_size - (L % self.window_size)\n        \n        if num_samples_to_add < self.window_size:\n            additional_samples = torch.zeros((B, num_samples_to_add, C), device=x.device)\n            x = torch.cat((x, additional_samples), dim=1)\n            mask_additional_samples = torch.zeros((B, num_samples_to_add, mask.shape[2]), device=mask.device)\n            mask = torch.cat((mask, mask_additional_samples), dim=1)\n            L += num_samples_to_add\n\n        # zero out padding transactions\n        x = x * mask\n        \n        assert L >= self.window_size, f'input length ({L}) must be >= window size ({self.window_size})'\n        assert L % self.window_size == 0, f'input length ({L}) must be divisible by window size ({self.window_size})'\n\n        shortcut = x\n        x = self.norm1(x)\n\n        # shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=-self.shift_size, dims=1) # cyclic shift \n            if not self.start_end_fusion:\n                shifted_x[:, -self.shift_size:] = 0. # zero out invalid embs\n            mask = torch.roll(mask, shifts=-self.shift_size, dims=1) # cyclic shift of the mask\n            if not self.start_end_fusion:\n                mask[:, -self.shift_size:] = 0.\n        else:\n            shifted_x = x\n        \n        # partition\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, C\n        mask = window_partition(mask, self.window_size) # nW*B, window_size, 1\n        \n        # calculate attn_mask\n        attn_mask = (mask @ mask.transpose(-2, -1)) # nW*B, window_size, window_size\n        \n        if self.decoder:\n            no_look_ahead_attn_mask = 1. - torch.triu(torch.ones_like(attn_mask), diagonal=1)\n            attn_mask *= no_look_ahead_attn_mask\n        \n        attn_mask_real = attn_mask.clone().detach()\n        attn_mask_real = attn_mask_real.view(attn_mask_real.shape[0], self.window_size, self.window_size).unsqueeze(1).expand(-1, self.num_heads, -1, -1) # B*nW, nH, window_size, window_size\n        \n        attn_mask[attn_mask == 0.] = -torch.inf\n        attn_mask[attn_mask == 1.] = 0.\n        attn_mask[:, torch.arange(attn_mask.shape[-1]), torch.arange(attn_mask.shape[-1])] = 0.\n        attn_mask = attn_mask.view(B, attn_mask.shape[0] // B, self.window_size, self.window_size).unsqueeze(2).expand(-1, -1, self.num_heads, -1, -1) # B, nW, nH, window_size, window_size\n        \n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask_add=attn_mask, mask_mult=attn_mask_real)  # nW*B, window_size, C\n        \n        # merge windows\n        shifted_x = window_reverse(attn_windows, self.window_size, L)  # (B, L, C)\n\n        # reverse zero-padding shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=self.shift_size, dims=1) # cyclic shift\n            if not self.start_end_fusion:\n                x[:, :self.shift_size] = 0. # zero out invalid embs\n        else:\n            x = shifted_x\n\n        x = shortcut + self.drop_path(x)\n\n        # FFN\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        \n        return PaddedBatch(x, seq_lens)\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n\n\nclass SwinTransformerLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        decoder (bool, optional): Flag that shows whether blocks in this layer are decoder-like. True => decoder-like; False => encoder-like. Default: False\n        start_end_fusion (bool, optional): Flag that shows if the last and the first half-windows should merge (True) or not (False).\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        depth: int,\n        num_heads: int,\n        window_size: int,\n        mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n        drop_path=0., norm_layer=nn.LayerNorm,\n        decoder=False, start_end_fusion=True\n    ):\n        super().__init__()\n        self.dim = dim\n        self.depth = depth\n        self.num_heads = num_heads\n        self.window_size = window_size\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer,\n                                 decoder=decoder,\n                                 start_end_fusion=start_end_fusion)\n            for i in range(depth)])\n\n    def forward(self, x):\n        for blk in self.blocks:\n            x = blk(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, depth={self.depth}, num_heads={self.num_heads}, window_size={self.window_size}\"\n\n\nclass SwinTransformerBackbone(nn.Module):\n    \"\"\" Swin Transformer Backbone (4 stages as in orig. 2D impl.).\n\n    Args:\n        dim (int): Number of input channels.\n        depths (list[int]): Numbers of blocks in stages.\n        num_heads (int): Number of attention heads in W-MSA layers.\n        start_window_size (int): Local window size of stage 1.\n        window_size_mult (int): the number by which the `window_size` is being multiplied when moving to another stage\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        decoder (bool, optional): Flag that shows whether blocks in this backbone are decoder-like. True => decoder-like; False => encoder-like. Default: False\n        start_end_fusion (bool, optional): Flag that shows if the last and the first half-windows should merge (True) or not (False).\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        depths: list[int],\n        num_heads,\n        start_window_size: int,\n        window_size_mult: int = 1,\n        mlp_ratio=4.,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.,\n        attn_drop=0.,\n        drop_path=0.,\n        norm_layer=nn.LayerNorm,\n        decoder=False,\n        start_end_fusion=True\n    ):\n        super().__init__()\n        self.dim = dim\n        self.depths = depths\n        \n        if type(num_heads) == int:\n            self.num_heads = [num_heads] * len(depths)\n        else:\n            self.num_heads = num_heads\n        \n        self.window_sizes = [start_window_size]\n        \n        for i in range(len(self.depths) - 1):\n            self.window_sizes += [self.window_sizes[-1] * window_size_mult]\n\n        # build model\n        self.backbone = nn.ModuleList([\n            SwinTransformerLayer(dim=self.dim,\n                                 depth=self.depths[i],\n                                 num_heads=self.num_heads[i],\n                                 window_size=self.window_sizes[i],\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias,\n                                 qk_scale=qk_scale,\n                                 drop=drop,\n                                 attn_drop=attn_drop,\n                                 drop_path=drop_path,\n                                 norm_layer=norm_layer,\n                                 decoder=decoder,\n                                 start_end_fusion=start_end_fusion)\n            for i in range(len(self.depths))])\n\n    def forward(self, x):\n        for layer in self.backbone:\n            x = layer(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:00.100045Z","iopub.execute_input":"2025-05-09T15:07:00.100409Z","iopub.status.idle":"2025-05-09T15:07:00.134046Z","shell.execute_reply.started":"2025-05-09T15:07:00.100381Z","shell.execute_reply":"2025-05-09T15:07:00.133098Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def change_to_enc(swin_model):\n    for i in range(len(swin_model.backbone)):\n        for j in range(len(swin_model.backbone[i].blocks)):\n            swin_model.backbone[i].blocks[j].decoder = False\n\ndef change_to_dec(swin_model):\n    for i in range(len(swin_model.backbone)):\n        for j in range(len(swin_model.backbone[i].blocks)):\n            swin_model.backbone[i].blocks[j].decoder = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:08.533035Z","iopub.execute_input":"2025-05-09T15:07:08.533478Z","iopub.status.idle":"2025-05-09T15:07:08.538086Z","shell.execute_reply.started":"2025-05-09T15:07:08.533447Z","shell.execute_reply":"2025-05-09T15:07:08.537253Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Эксперименты.","metadata":{}},{"cell_type":"markdown","source":"**Данные:**","metadata":{}},{"cell_type":"code","source":"path_data = \"https://huggingface.co/datasets/dllllb/rosbank-churn/resolve/main/train.csv.gz?download=true\"\ndata = pd.read_csv(path_data, compression=\"gzip\")\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:10.845610Z","iopub.execute_input":"2025-05-09T15:07:10.845893Z","iopub.status.idle":"2025-05-09T15:07:12.330950Z","shell.execute_reply.started":"2025-05-09T15:07:10.845870Z","shell.execute_reply":"2025-05-09T15:07:12.330246Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"            PERIOD  cl_id   MCC channel_type  currency        TRDATETIME  \\\n0       01/10/2017      0  5200          NaN       810  21OCT17:00:00:00   \n1       01/10/2017      0  6011          NaN       810  12OCT17:12:24:07   \n2       01/12/2017      0  5921          NaN       810  05DEC17:00:00:00   \n3       01/10/2017      0  5411          NaN       810  21OCT17:00:00:00   \n4       01/10/2017      0  6012          NaN       810  24OCT17:13:14:24   \n...            ...    ...   ...          ...       ...               ...   \n490508  01/04/2017  10176  6011        type1       810  24APR17:14:05:26   \n490509  01/06/2017  10171  5411        type1       810  06JUN17:00:00:00   \n490510  01/02/2017  10167  5541        type1       810  03FEB17:00:00:00   \n490511  01/06/2017  10163  5941        type1       810  08JUN17:00:00:00   \n490512  01/06/2017  10162  5411        type1       810  15JUN17:00:00:00   \n\n          amount trx_category  target_flag  target_sum  \n0        5023.00          POS            0         0.0  \n1       20000.00      DEPOSIT            0         0.0  \n2         767.00          POS            0         0.0  \n3        2031.00          POS            0         0.0  \n4       36562.00      C2C_OUT            0         0.0  \n...          ...          ...          ...         ...  \n490508    600.00   WD_ATM_ROS            1       405.0  \n490509    132.00          POS            0         0.0  \n490510   1000.00          POS            1    280428.2  \n490511    100.00          POS            0         0.0  \n490512    441.33          POS            1       253.0  \n\n[490513 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERIOD</th>\n      <th>cl_id</th>\n      <th>MCC</th>\n      <th>channel_type</th>\n      <th>currency</th>\n      <th>TRDATETIME</th>\n      <th>amount</th>\n      <th>trx_category</th>\n      <th>target_flag</th>\n      <th>target_sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>01/10/2017</td>\n      <td>0</td>\n      <td>5200</td>\n      <td>NaN</td>\n      <td>810</td>\n      <td>21OCT17:00:00:00</td>\n      <td>5023.00</td>\n      <td>POS</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>01/10/2017</td>\n      <td>0</td>\n      <td>6011</td>\n      <td>NaN</td>\n      <td>810</td>\n      <td>12OCT17:12:24:07</td>\n      <td>20000.00</td>\n      <td>DEPOSIT</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>01/12/2017</td>\n      <td>0</td>\n      <td>5921</td>\n      <td>NaN</td>\n      <td>810</td>\n      <td>05DEC17:00:00:00</td>\n      <td>767.00</td>\n      <td>POS</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01/10/2017</td>\n      <td>0</td>\n      <td>5411</td>\n      <td>NaN</td>\n      <td>810</td>\n      <td>21OCT17:00:00:00</td>\n      <td>2031.00</td>\n      <td>POS</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>01/10/2017</td>\n      <td>0</td>\n      <td>6012</td>\n      <td>NaN</td>\n      <td>810</td>\n      <td>24OCT17:13:14:24</td>\n      <td>36562.00</td>\n      <td>C2C_OUT</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>490508</th>\n      <td>01/04/2017</td>\n      <td>10176</td>\n      <td>6011</td>\n      <td>type1</td>\n      <td>810</td>\n      <td>24APR17:14:05:26</td>\n      <td>600.00</td>\n      <td>WD_ATM_ROS</td>\n      <td>1</td>\n      <td>405.0</td>\n    </tr>\n    <tr>\n      <th>490509</th>\n      <td>01/06/2017</td>\n      <td>10171</td>\n      <td>5411</td>\n      <td>type1</td>\n      <td>810</td>\n      <td>06JUN17:00:00:00</td>\n      <td>132.00</td>\n      <td>POS</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>490510</th>\n      <td>01/02/2017</td>\n      <td>10167</td>\n      <td>5541</td>\n      <td>type1</td>\n      <td>810</td>\n      <td>03FEB17:00:00:00</td>\n      <td>1000.00</td>\n      <td>POS</td>\n      <td>1</td>\n      <td>280428.2</td>\n    </tr>\n    <tr>\n      <th>490511</th>\n      <td>01/06/2017</td>\n      <td>10163</td>\n      <td>5941</td>\n      <td>type1</td>\n      <td>810</td>\n      <td>08JUN17:00:00:00</td>\n      <td>100.00</td>\n      <td>POS</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>490512</th>\n      <td>01/06/2017</td>\n      <td>10162</td>\n      <td>5411</td>\n      <td>type1</td>\n      <td>810</td>\n      <td>15JUN17:00:00:00</td>\n      <td>441.33</td>\n      <td>POS</td>\n      <td>1</td>\n      <td>253.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>490513 rows × 10 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"target = data.groupby(by=\"cl_id\").first().reset_index()[[\"cl_id\", \"target_flag\"]]\ntarget","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:13.850827Z","iopub.execute_input":"2025-05-09T15:07:13.851104Z","iopub.status.idle":"2025-05-09T15:07:13.992465Z","shell.execute_reply.started":"2025-05-09T15:07:13.851083Z","shell.execute_reply":"2025-05-09T15:07:13.991710Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"      cl_id  target_flag\n0         0            0\n1         1            0\n2         5            1\n3         9            0\n4        10            0\n...     ...          ...\n4995  10210            1\n4996  10212            0\n4997  10213            0\n4998  10214            0\n4999  10215            0\n\n[5000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cl_id</th>\n      <th>target_flag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>10210</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>10212</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>10213</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>10214</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>10215</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"data.drop(columns=[\"PERIOD\", \"target_flag\", \"target_sum\"], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:19.801806Z","iopub.execute_input":"2025-05-09T15:07:19.802286Z","iopub.status.idle":"2025-05-09T15:07:19.832377Z","shell.execute_reply.started":"2025-05-09T15:07:19.802247Z","shell.execute_reply":"2025-05-09T15:07:19.831697Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"target_train, target_test = train_test_split(target, test_size=0.1, stratify=target[\"target_flag\"], random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:21.064511Z","iopub.execute_input":"2025-05-09T15:07:21.064836Z","iopub.status.idle":"2025-05-09T15:07:21.074063Z","shell.execute_reply.started":"2025-05-09T15:07:21.064813Z","shell.execute_reply":"2025-05-09T15:07:21.073417Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"trx_data_train = pd.merge(data, target_train[\"cl_id\"], on=\"cl_id\", how=\"inner\")\ntrx_data_test = pd.merge(data, target_test[\"cl_id\"], on=\"cl_id\", how=\"inner\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:22.380415Z","iopub.execute_input":"2025-05-09T15:07:22.380691Z","iopub.status.idle":"2025-05-09T15:07:22.470379Z","shell.execute_reply.started":"2025-05-09T15:07:22.380670Z","shell.execute_reply":"2025-05-09T15:07:22.469681Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"trx_data_train[\"channel_type\"] = trx_data_train[\"channel_type\"].fillna(\"none\")\ntrx_data_test[\"channel_type\"] = trx_data_test[\"channel_type\"].fillna(\"none\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:23.751906Z","iopub.execute_input":"2025-05-09T15:07:23.752213Z","iopub.status.idle":"2025-05-09T15:07:23.785459Z","shell.execute_reply.started":"2025-05-09T15:07:23.752190Z","shell.execute_reply":"2025-05-09T15:07:23.784427Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"month2num = {\"JAN\": \"/01/\", \"FEB\": \"/02/\", \"MAR\": \"/03/\", \"APR\": \"/04/\", \"MAY\": \"/05/\", \"JUN\": \"/06/\",\n             \"JUL\": \"/07/\", \"AUG\": \"/08/\", \"SEP\": \"/09/\", \"OCT\": \"/10/\", \"NOV\": \"/11/\", \"DEC\": \"/12/\"}\n\ntrx_data_train[\"TRDATETIME\"] = trx_data_train[\"TRDATETIME\"].map(lambda x: x[0:2] + month2num[x[2:5]] + x[5:7] + \" \" + x[8:])\ntrx_data_test[\"TRDATETIME\"] = trx_data_test[\"TRDATETIME\"].map(lambda x: x[0:2] + month2num[x[2:5]] + x[5:7] + \" \" + x[8:])\n\ntrx_data_train[\"TRDATETIME\"] = pd.to_datetime(trx_data_train[\"TRDATETIME\"],format='%d/%m/%y %H:%M:%S')\ntrx_data_test[\"TRDATETIME\"] = pd.to_datetime(trx_data_test[\"TRDATETIME\"],format='%d/%m/%y %H:%M:%S')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:25.184463Z","iopub.execute_input":"2025-05-09T15:07:25.184791Z","iopub.status.idle":"2025-05-09T15:07:25.769254Z","shell.execute_reply.started":"2025-05-09T15:07:25.184761Z","shell.execute_reply":"2025-05-09T15:07:25.768305Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"chtype2num = {\"none\": 0, \"type1\": 1, \"type2\": 2, \"type3\": 3, \"type4\": 4, \"type5\": 5}\n\ntrx_data_train[\"channel_type\"] = trx_data_train[\"channel_type\"].map(lambda x: chtype2num[x])\ntrx_data_test[\"channel_type\"] = trx_data_test[\"channel_type\"].map(lambda x: chtype2num[x])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:26.792495Z","iopub.execute_input":"2025-05-09T15:07:26.792826Z","iopub.status.idle":"2025-05-09T15:07:26.948660Z","shell.execute_reply.started":"2025-05-09T15:07:26.792798Z","shell.execute_reply":"2025-05-09T15:07:26.947774Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"trxcat2num = {\"POS\": 0, \"DEPOSIT\": 1, \"WD_ATM_ROS\": 2, \"WD_ATM_PARTNER\": 3, \n              \"C2C_IN\": 4, \"WD_ATM_OTHER\": 5, \"C2C_OUT\": 6, \"BACK_TRX\": 7,\n              \"CAT\": 8, \"CASH_ADV\": 9}\n\ntrx_data_train[\"trx_category\"] = trx_data_train[\"trx_category\"].map(lambda x: trxcat2num[x])\ntrx_data_test[\"trx_category\"] = trx_data_test[\"trx_category\"].map(lambda x: trxcat2num[x])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:28.224334Z","iopub.execute_input":"2025-05-09T15:07:28.224620Z","iopub.status.idle":"2025-05-09T15:07:28.383591Z","shell.execute_reply.started":"2025-05-09T15:07:28.224598Z","shell.execute_reply":"2025-05-09T15:07:28.382915Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Квантизация непрерывных признаков (опциональный шаг, нужен только для GPT):**","metadata":{}},{"cell_type":"code","source":"def digitize(input_array: np.array, q_count: int = 1, bins: np.array = None):\n    \"\"\"Quantile-based discretization function.\n\n    Parameters:\n    -------\n    input_array (np.array): Input array.\n    q_count (int): Amount of quantiles. Used only if input parameter `bins` is None.\n    bins (np.array):\n        If None, then calculate bins as quantiles of input array,\n        otherwise only apply bins to input_array. Default: None\n\n    Returns\n    -------\n    out_array (np.array of ints): discretized input_array\n    bins (np.array of floats):\n        Returned only if input parameter `bins` is None.\n    \"\"\"\n\n    if bins is None:\n        return_bins = True\n        bins = np.quantile(input_array, q=[i / q_count for i in range(1, q_count)], axis=0)\n    else:\n        return_bins = False\n\n    out_array = np.digitize(input_array, bins)\n\n    if return_bins:\n        return out_array, bins\n    else:\n        return out_array","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:13:22.195655Z","iopub.execute_input":"2025-04-23T11:13:22.195954Z","iopub.status.idle":"2025-04-23T11:13:22.201084Z","shell.execute_reply.started":"2025-04-23T11:13:22.195932Z","shell.execute_reply":"2025-04-23T11:13:22.200242Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"BINS_NUM = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:13:24.380883Z","iopub.execute_input":"2025-04-23T11:13:24.381238Z","iopub.status.idle":"2025-04-23T11:13:24.384940Z","shell.execute_reply.started":"2025-04-23T11:13:24.381209Z","shell.execute_reply":"2025-04-23T11:13:24.384109Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"numeric_features = [\"amount\"]\n\nfor feat in numeric_features:\n    trx_data_train[feat], bins = digitize(trx_data_train[feat], q_count=BINS_NUM)\n    trx_data_test[feat] = digitize(trx_data_test[feat], bins=bins)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:13:26.127846Z","iopub.execute_input":"2025-04-23T11:13:26.128171Z","iopub.status.idle":"2025-04-23T11:13:26.179309Z","shell.execute_reply.started":"2025-04-23T11:13:26.128145Z","shell.execute_reply":"2025-04-23T11:13:26.178681Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import gc\n\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:13:27.734349Z","iopub.execute_input":"2025-04-23T11:13:27.734672Z","iopub.status.idle":"2025-04-23T11:13:28.056924Z","shell.execute_reply.started":"2025-04-23T11:13:27.734644Z","shell.execute_reply":"2025-04-23T11:13:28.056264Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"147"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"preprocessor = PandasDataPreprocessor(\n    col_id=\"cl_id\",\n    col_event_time=\"TRDATETIME\",\n    event_time_transformation=\"dt_to_timestamp\",\n    cols_category=[\"MCC\", \"channel_type\", \"currency\", \"trx_category\"],\n    cols_numerical=[\"amount\"],\n    return_records=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:31.115497Z","iopub.execute_input":"2025-05-09T15:07:31.115791Z","iopub.status.idle":"2025-05-09T15:07:31.119894Z","shell.execute_reply.started":"2025-05-09T15:07:31.115756Z","shell.execute_reply":"2025-05-09T15:07:31.118990Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"data_train = preprocessor.fit_transform(trx_data_train)\ndata_test = preprocessor.transform(trx_data_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:33.557601Z","iopub.execute_input":"2025-05-09T15:07:33.557891Z","iopub.status.idle":"2025-05-09T15:07:37.681832Z","shell.execute_reply.started":"2025-05-09T15:07:33.557868Z","shell.execute_reply":"2025-05-09T15:07:37.681189Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"target_train.rename(columns={\"target_flag\": \"target\"}, inplace=True)\ntarget_test.rename(columns={\"target_flag\": \"target\"}, inplace=True)\ntarget_train.sort_values(by=\"cl_id\", inplace=True)\ntarget_test.sort_values(by=\"cl_id\", inplace=True)\ntarget_train = target_train[\"target\"]\ntarget_test = target_test[\"target\"]\ntarget_train.reset_index(drop=True, inplace=True)\ntarget_test.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:40.141976Z","iopub.execute_input":"2025-05-09T15:07:40.142299Z","iopub.status.idle":"2025-05-09T15:07:40.149392Z","shell.execute_reply.started":"2025-05-09T15:07:40.142270Z","shell.execute_reply":"2025-05-09T15:07:40.148638Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"data_train = data_train.to_dict(orient=\"records\")\ndata_test = data_test.to_dict(orient=\"records\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:41.484634Z","iopub.execute_input":"2025-05-09T15:07:41.484913Z","iopub.status.idle":"2025-05-09T15:07:41.516651Z","shell.execute_reply.started":"2025-05-09T15:07:41.484891Z","shell.execute_reply":"2025-05-09T15:07:41.515573Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Определение бинов для time diff'ов (в часах) (опциональный шаг, нужен только для TD-GPT):**","metadata":{}},{"cell_type":"code","source":"train_loader = inference_data_loader(data_train, num_workers=0, batch_size=128)\nSECONDS_IN_HOUR = 3600\nTIME_DIFF_BINS = 256\n\ntime_diffs = []\n\nfor batch in tqdm(train_loader):\n    timestamps = batch.payload['event_time']\n    timestamps_prev = torch.cat([timestamps[:, 0].unsqueeze(1), timestamps[:, :-1]], dim=1)\n    batch.payload['time_diff'] = (timestamps - timestamps_prev) // SECONDS_IN_HOUR\n    batch.payload['time_diff'][:, 0] = -1\n\n    mask = torch.arange(batch.payload['time_diff'].shape[1], device=batch.device)[None, :] + torch.ones((batch.seq_lens.shape[0], batch.payload['time_diff'].shape[1]), device=batch.device)\n    mask[mask > batch.seq_lens[:, None]] = 0.\n    mask[mask > 0.] = 1.\n    mask = mask.bool()\n\n    batch.payload['time_diff'][~mask] = -1\n    \n    time_diffs += [batch.payload['time_diff'][batch.payload['time_diff'] != -1].numpy()]\n    \ntime_diffs = np.concatenate(time_diffs)\n\ntime_diff_bins = np.quantile(time_diffs, q=[(i / TIME_DIFF_BINS) for i in range(1, TIME_DIFF_BINS)], axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:13:43.329839Z","iopub.execute_input":"2025-04-23T11:13:43.330177Z","iopub.status.idle":"2025-04-23T11:13:43.716914Z","shell.execute_reply.started":"2025-04-23T11:13:43.330152Z","shell.execute_reply":"2025-04-23T11:13:43.716245Z"}},"outputs":[{"name":"stderr","text":"36it [00:00, 98.54it/s] \n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"time_diff_bins","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:14:14.586984Z","iopub.execute_input":"2025-04-23T11:14:14.587344Z","iopub.status.idle":"2025-04-23T11:14:14.594089Z","shell.execute_reply.started":"2025-04-23T11:14:14.587321Z","shell.execute_reply":"2025-04-23T11:14:14.593121Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   2.,   3.,\n         4.,   5.,   6.,   7.,   7.,   8.,   9.,   9.,  10.,  10.,  11.,\n        11.,  12.,  12.,  13.,  13.,  14.,  14.,  15.,  16.,  16.,  17.,\n        18.,  20.,  22.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,\n        24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,\n        24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,\n        24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,\n        24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,\n        24.,  24.,  24.,  26.,  31.,  35.,  38.,  44.,  48.,  48.,  48.,\n        48.,  48.,  48.,  48.,  48.,  48.,  48.,  48.,  54.,  62.,  72.,\n        72.,  72.,  72.,  82.,  96.,  96., 114., 120., 144., 168., 216.,\n       300., 458.])"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"time_diff_bins = list(set(time_diff_bins.tolist()))\ntime_diff_bins.sort()\ntime_diff_bins = torch.tensor(time_diff_bins, dtype=torch.int)\ntime_diff_bins","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:14:17.680224Z","iopub.execute_input":"2025-04-23T11:14:17.680547Z","iopub.status.idle":"2025-04-23T11:14:17.690250Z","shell.execute_reply.started":"2025-04-23T11:14:17.680521Z","shell.execute_reply":"2025-04-23T11:14:17.689413Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n         14,  15,  16,  17,  18,  20,  22,  24,  26,  31,  35,  38,  44,  48,\n         54,  62,  72,  82,  96, 114, 120, 144, 168, 216, 300, 458],\n       dtype=torch.int32)"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"TIME_DIFF_BINS_NUM = len(time_diff_bins)\n\nTIME_DIFF_BINS_NUM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:14:21.214061Z","iopub.execute_input":"2025-04-23T11:14:21.214362Z","iopub.status.idle":"2025-04-23T11:14:21.219362Z","shell.execute_reply.started":"2025-04-23T11:14:21.214341Z","shell.execute_reply":"2025-04-23T11:14:21.218611Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"40"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"**Тест:**","metadata":{}},{"cell_type":"code","source":"train_loader = inference_data_loader(data_train, num_workers=0, batch_size=128)\nSECONDS_IN_HOUR = 3600\n\nfor batch in tqdm(train_loader):\n    timestamps = batch.payload['event_time']\n    timestamps_prev = torch.cat([timestamps[:, 0].unsqueeze(1), timestamps[:, :-1]], dim=1)\n    batch.payload['time_diff'] = (timestamps - timestamps_prev) // SECONDS_IN_HOUR\n    batch.payload['time_diff'][:, 0] = -1\n\n    mask = torch.arange(batch.payload['time_diff'].shape[1], device=batch.device)[None, :] + torch.ones((batch.seq_lens.shape[0], batch.payload['time_diff'].shape[1]), device=batch.device)\n    mask[mask > batch.seq_lens[:, None]] = 0.\n    mask[mask > 0.] = 1.\n    mask = mask.bool()\n\n    batch.payload['time_diff'][~mask] = -1\n\n    print(torch.bucketize(batch.payload['time_diff'], time_diff_bins, right=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:14:27.741899Z","iopub.execute_input":"2025-04-23T11:14:27.742207Z","iopub.status.idle":"2025-04-23T11:14:28.127693Z","shell.execute_reply.started":"2025-04-23T11:14:27.742185Z","shell.execute_reply":"2025-04-23T11:14:28.126828Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"20it [00:00, 93.69it/s]","output_type":"stream"},{"name":"stdout","text":"tensor([[ 0, 37,  1,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        [ 0, 12, 37,  ...,  0,  0,  0],\n        ...,\n        [ 0, 22,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0]])\ntensor([[ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        ...,\n        [ 0,  2,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 39,  ...,  0,  0,  0]])\ntensor([[ 0, 22,  1,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        [ 0, 22,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0, 11, 21,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        [ 0,  1, 32,  ...,  0,  0,  0]])\ntensor([[ 0, 22, 17,  ...,  0,  0,  0],\n        [ 0, 40, 39,  ...,  0,  0,  0],\n        [ 0, 28,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1, 31,  ...,  0,  0,  0],\n        [ 0, 40, 39,  ...,  0,  0,  0],\n        [ 0, 19,  6,  ...,  0,  0,  0]])\ntensor([[ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 38, 22,  ...,  0,  0,  0],\n        [ 0, 24,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0, 22,  1,  ...,  0,  0,  0],\n        [ 0, 19,  6,  ...,  0,  0,  0],\n        [ 0,  9, 12,  ...,  0,  0,  0]])\ntensor([[ 0, 40,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 28,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 33,  1,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0]])\ntensor([[ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 31,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0, 40, 38,  ...,  0,  0,  0],\n        [ 0, 31,  1,  ...,  0,  0,  0],\n        [ 0, 22, 15,  ...,  0,  0,  0]])\ntensor([[0, 1, 1,  ..., 0, 0, 0],\n        [0, 6, 1,  ..., 0, 0, 0],\n        [0, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [0, 1, 1,  ..., 0, 0, 0],\n        [0, 1, 1,  ..., 0, 0, 0],\n        [0, 8, 1,  ..., 0, 0, 0]])\ntensor([[ 0, 22,  1,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        [ 0, 12, 31,  ...,  0,  0,  0],\n        ...,\n        [ 0, 22, 22,  ...,  0,  0,  0],\n        [ 0, 37, 22,  ...,  0,  0,  0],\n        [ 0, 14, 24,  ...,  0,  0,  0]])\ntensor([[ 0, 30,  1,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        [ 0, 15, 10,  ...,  0,  0,  0],\n        ...,\n        [ 0, 35,  1,  ...,  0,  0,  0],\n        [ 0, 22, 22,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0]])\ntensor([[ 0, 37,  9,  ...,  0,  0,  0],\n        [ 0, 22, 22,  ...,  0,  0,  0],\n        [ 0, 38, 40,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 22, 35,  ...,  0,  0,  0],\n        [ 0, 21,  1,  ...,  0,  0,  0]])\ntensor([[ 0, 27, 26,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  0,  0,  ...,  0,  0,  0],\n        [ 0, 31,  1,  ...,  0,  0,  0]])\ntensor([[ 0, 22, 28,  ...,  0,  0,  0],\n        [ 0, 24,  1,  ...,  0,  0,  0],\n        [ 0, 28, 28,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0]])\ntensor([[ 0,  1, 22,  ...,  0,  0,  0],\n        [ 0, 31,  1,  ...,  0,  0,  0],\n        [ 0, 22,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1, 15,  ...,  0,  0,  0],\n        [ 0, 22,  1,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0]])\ntensor([[ 0,  1, 28,  ...,  0,  0,  0],\n        [ 0,  8, 22,  ...,  0,  0,  0],\n        [ 0, 29,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1, 32,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 22, 22,  ...,  0,  0,  0]])\ntensor([[ 0,  1,  7,  ...,  0,  0,  0],\n        [ 0,  0,  0,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 25,  1,  ...,  0,  0,  0],\n        [ 0, 22, 22,  ...,  0,  0,  0]])\ntensor([[ 0, 19, 20,  ...,  0,  0,  0],\n        [ 0, 38, 22,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 40,  1,  ...,  0,  0,  0],\n        [ 0, 22,  1,  ...,  0,  0,  0]])\ntensor([[ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 19,  ...,  0,  0,  0],\n        [ 0,  3,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0, 22, 22,  ...,  0,  0,  0],\n        [ 0,  1, 13,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0]])\ntensor([[ 0,  1, 31,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        [ 0,  1, 23,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1, 17,  ...,  0,  0,  0],\n        [ 0, 37, 22,  ...,  0,  0,  0],\n        [ 0, 12, 13,  ...,  0,  0,  0]])\ntensor([[ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        [ 0, 22, 28,  ...,  0,  0,  0],\n        ...,\n        [ 0, 28,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 22,  1,  ...,  0,  0,  0]])\ntensor([[ 0, 22,  1,  ...,  0,  0,  0],\n        [ 0, 32,  1,  ...,  0,  0,  0],\n        [ 0, 11, 22,  ...,  0,  0,  0],\n        ...,\n        [ 0, 40, 40,  ...,  0,  0,  0],\n        [ 0, 28, 12,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0]])\n","output_type":"stream"},{"name":"stderr","text":"36it [00:00, 95.76it/s]","output_type":"stream"},{"name":"stdout","text":"tensor([[ 0,  1, 28,  ...,  0,  0,  0],\n        [ 0,  1, 15,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1, 11,  ...,  0,  0,  0],\n        [ 0, 14, 40,  ...,  0,  0,  0],\n        [ 0,  1, 12,  ...,  0,  0,  0]])\ntensor([[ 0,  8,  1,  ...,  0,  0,  0],\n        [ 0, 21, 39,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 15,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0]])\ntensor([[ 0, 17,  8,  ...,  0,  0,  0],\n        [ 0, 22, 28,  ...,  0,  0,  0],\n        [ 0,  1, 10,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 18,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0]])\ntensor([[ 0, 22,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1, 10,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 17,  ...,  0,  0,  0]])\ntensor([[ 0,  1,  4,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 12,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0, 22, 22,  ...,  0,  0,  0],\n        [ 0,  1, 16,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ..., 22, 14, 11]])\ntensor([[ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 15,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0]])\ntensor([[ 0,  1, 10,  ...,  0,  0,  0],\n        [ 0,  6,  1,  ...,  0,  0,  0],\n        [ 0, 11, 14,  ...,  0,  0,  0],\n        ...,\n        [ 0, 22,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 21, 36,  ...,  0,  0,  0]])\ntensor([[ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 36,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0, 37,  1,  ...,  0,  0,  0],\n        [ 0, 39, 10,  ...,  0,  0,  0],\n        [ 0,  8, 12,  ...,  0,  0,  0]])\ntensor([[ 0, 22,  1,  ...,  0,  0,  0],\n        [ 0, 13,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0, 21, 23,  ...,  0,  0,  0],\n        [ 0, 29, 33,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0]])\ntensor([[ 0, 40, 38,  ...,  0,  0,  0],\n        [ 0,  1, 14,  ...,  0,  0,  0],\n        [ 0,  0,  0,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  8,  1,  ...,  0,  0,  0],\n        [ 0,  1, 17,  ...,  0,  0,  0]])\ntensor([[ 0, 22, 28,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 13,  ...,  0,  0,  0],\n        ...,\n        [ 0, 22, 28,  ...,  0,  0,  0],\n        [ 0, 15,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0]])\ntensor([[ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 28, 19,  ...,  0,  0,  0],\n        [ 0,  7,  7,  ...,  0,  0,  0],\n        ...,\n        [ 0, 22, 13,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 36, 28,  ...,  0,  0,  0]])\ntensor([[ 0, 18, 23,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        ...,\n        [ 0, 15, 24,  ...,  0,  0,  0],\n        [ 0, 37,  1,  ...,  0,  0,  0],\n        [ 0,  1, 15,  ...,  0,  0,  0]])\ntensor([[ 0, 18,  7,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0, 14, 24,  ...,  0,  0,  0],\n        ...,\n        [ 0,  1,  5,  ...,  0,  0,  0],\n        [ 0, 33, 22,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0]])\ntensor([[ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        [ 0,  1,  1,  ...,  0,  0,  0],\n        ...,\n        [ 0,  4, 22,  ...,  0,  0,  0],\n        [ 0,  1, 22,  ...,  0,  0,  0],\n        [ 0, 14,  1,  ...,  0,  0,  0]])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**SWIN-RNN Seq Encoder:**","metadata":{}},{"cell_type":"code","source":"from ptls.nn.seq_encoder.rnn_encoder import RnnEncoder\nfrom ptls.nn.seq_encoder.containers import SeqEncoderContainer\n\n\nclass SWIN_RNN_SeqEncoder(SeqEncoderContainer):\n    \"\"\"SeqEncoderContainer with SWIN transformer backbone for features hierarchic fusion and RnnEncoder for feature aggregation.\n    \n    Parameters\n        trx_encoder:\n            TrxEncoder object\n        input_size:\n            input_size parameter for RnnEncoder\n            If None: input_size = trx_encoder.output_size\n            Set input_size explicitly or use None if your trx_encoder object has output_size attribute\n        is_reduce_sequence:\n            False - returns PaddedBatch with all transactions embeddings\n            True - returns one embedding for sequence based on CLS token\n        swin_depths: Numbers of blocks in stages (SWIN backbone).\n        swin_num_heads: Number of attention heads in W-MSA layers (SWIN backbone).\n        swin_start_window_size: Local window size of stage 1 (SWIN backbone).\n        swin_window_size_mult (int): the number by which the `window_size` is being multiplied when moving to another stage (SWIN backbone).\n        swin_drop: Dropout rate (SWIN backbone). Default: 0.0\n        swin_attn_drop: Attention dropout rate (SWIN backbone). Default: 0.0\n        swin_drop_path: Stochastic depth rate (SWIN backbone). Default: 0.0\n        swin_decoder: Flag that shows whether blocks in SWIN backbone are decoder-like. True => decoder-like; False => encoder-like. Default: False\n        swin_start_end_fusion: Flag that shows if the last and the first half-windows should merge (True) or not (False). Must be False for CPC and GPT.\n        **rnn_seq_encoder_params:\n            RnnEncoder params\n    \"\"\"\n    def __init__(self,\n                 trx_encoder=None,\n                 input_size=None,\n                 is_reduce_sequence=True,\n                 swin_depths=[],\n                 swin_num_heads=4,\n                 swin_start_window_size=4,\n                 swin_window_size_mult=1,\n                 swin_drop=0.,\n                 swin_attn_drop=0.,\n                 swin_drop_path=0.,\n                 swin_decoder=False,\n                 swin_start_end_fusion=True,\n                 **rnn_seq_encoder_params\n                 ):\n        super().__init__(\n            trx_encoder=trx_encoder,\n            seq_encoder_cls=RnnEncoder,\n            input_size=input_size,\n            seq_encoder_params=rnn_seq_encoder_params,\n            is_reduce_sequence=is_reduce_sequence,\n        )\n        self.swin_fusion = SwinTransformerBackbone(\n                               dim=trx_encoder.output_size,\n                               depths=swin_depths,\n                               num_heads=swin_num_heads,\n                               start_window_size=swin_start_window_size,\n                               window_size_mult=swin_window_size_mult,\n                               drop=swin_drop,\n                               attn_drop=swin_attn_drop,\n                               drop_path=swin_drop_path,\n                               decoder=swin_decoder,\n                               start_end_fusion=swin_start_end_fusion \n                              )\n\n    def forward(self, x, names=None, seq_len=None, h_0=None):\n        x = self.trx_encoder(x)\n        x = self.swin_fusion(x)\n        x = self.seq_encoder(x, h_0)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:45.049044Z","iopub.execute_input":"2025-05-09T15:07:45.049374Z","iopub.status.idle":"2025-05-09T15:07:45.056059Z","shell.execute_reply.started":"2025-05-09T15:07:45.049345Z","shell.execute_reply":"2025-05-09T15:07:45.055198Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**SWIN Seq Encoder:**","metadata":{}},{"cell_type":"code","source":"from ptls.data_load.padded_batch import PaddedBatch\nfrom ptls.nn.seq_encoder.abs_seq_encoder import AbsSeqEncoder\nfrom ptls.nn.seq_encoder.containers import SeqEncoderContainer\n\n\nclass SWIN_Encoder(AbsSeqEncoder):\n    def __init__(self,\n                 dim=0,\n                 depths=[],\n                 num_heads=4,\n                 start_window_size=4,\n                 window_size_mult=1,\n                 drop=0.,\n                 attn_drop=0.,\n                 drop_path=0.,\n                 decoder=False,\n                 start_end_fusion=True,\n                 is_reduce_sequence=False\n                 ):\n        super().__init__(is_reduce_sequence=is_reduce_sequence)\n        self.dim = dim\n        self.swin_fusion = SwinTransformerBackbone(\n                               dim=dim,\n                               depths=depths,\n                               num_heads=num_heads,\n                               start_window_size=start_window_size,\n                               window_size_mult=window_size_mult,\n                               drop=drop,\n                               attn_drop=attn_drop,\n                               drop_path=drop_path,\n                               decoder=decoder,\n                               start_end_fusion=start_end_fusion \n                              )\n\n    @property\n    def embedding_size(self):\n        return self.dim\n\n    def forward(self, x):\n        x = self.swin_fusion(x)\n\n        if self.is_reduce_sequence:\n            x = x.payload.sum(dim=1) / x.seq_lens.unsqueeze(-1)\n        \n        return x\n\n\nclass SWIN_SeqEncoder(torch.nn.Module):\n    def __init__(self,\n                 trx_encoder,\n                 depths=[],\n                 num_heads=4,\n                 start_window_size=4,\n                 window_size_mult=1,\n                 drop=0.,\n                 attn_drop=0.,\n                 drop_path=0.,\n                 decoder=False,\n                 start_end_fusion=True,\n                 is_reduce_sequence=False\n                 ):\n        super().__init__()\n        self.trx_encoder = trx_encoder\n        self.seq_encoder = SWIN_Encoder(\n            dim=trx_encoder.output_size,\n            depths=depths,\n            num_heads=num_heads,\n            start_window_size=start_window_size,\n            window_size_mult=window_size_mult,\n            drop=drop,\n            attn_drop=attn_drop,\n            drop_path=drop_path,\n            decoder=decoder,\n            start_end_fusion=start_end_fusion,\n            is_reduce_sequence=is_reduce_sequence\n        )\n        \n    def forward(self, x):\n        x = self.trx_encoder(x)\n        x = self.seq_encoder(x)\n        return x\n\n    @property\n    def embedding_size(self):\n        return self.seq_encoder.embedding_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:48.237687Z","iopub.execute_input":"2025-05-09T15:07:48.237980Z","iopub.status.idle":"2025-05-09T15:07:48.247153Z","shell.execute_reply.started":"2025-05-09T15:07:48.237958Z","shell.execute_reply":"2025-05-09T15:07:48.246201Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from ptls.data_load.padded_batch import PaddedBatch\nimport torch.nn as nn\n\n\nclass ConvAggregator(TrxEncoderT2V):\n    \"\"\"The NN layer, a combination of TrxEncoder and Conv Layer (a window of #`agg_samples` transactions) \n       (works like nn.Sequential([TrxEncoder, Conv Window Aggregation])).\n       \n       The types of the input and output are `PaddedBatch` of shapes (B, L, T) and (B, L', T) respectively, where \n       B means batch_size,\n       L/L' means the max length of a sequence of transactions in a batch (the length is the same as #trx)\n       T means the dimension of a single transaction.\n\n       Parameters\n        agg_samples (int):\n            The number of transactions in a sliding aggregation window (conv layer).\n\n        use_window_attention (bool):\n            If True, the attention layer will be applied to transactions in a sliding window before pooling.\n\n        k (int):\n            Number of periodic components in T2V time embeddings\n\n        time_col (str):\n            Name of the time column in data\n            \n        embeddings:\n            You can find info about this param in TrxEncoder desc.\n        \n        numeric_values:\n            You can find info about this param in TrxEncoder desc.\n\n        embeddings_noise:\n            You can find info about this param in TrxEncoder desc.\n            \n        emb_dropout:\n            You can find info about this param in TrxEncoder desc.\n            \n        spatial_dropout:\n            You can find info about this param in TrxEncoder desc.\n\n        use_batch_norm:\n            You can find info about this param in TrxEncoder desc.\n\n        orthogonal_init:\n            You can find info about this param in TrxEncoder desc.\n            \n        linear_projection_size:\n            You can find info about this param in TrxEncoder desc.\n\n        out_of_index:\n            You can find info about this param in TrxEncoder desc.\n\n        norm_embeddings:\n            Keep default value for this parameter\n        \n        clip_replace_value:\n            Not used. Keep default value for this parameter\n        \n        positions: \n            Not used. Keep default value for this parameter\n       \"\"\"\n\n    def __init__(self,\n                 agg_samples=3,\n                 use_window_attention=False,\n                 embeddings=None,\n                 numeric_values=None,\n                 custom_embeddings=None,\n                 time_values=None,\n                 embeddings_noise: float = 0,\n                 norm_embeddings=None,\n                 use_batch_norm=False,\n                 use_batch_norm_with_lens=False,\n                 clip_replace_value=None,\n                 positions=None,\n                 emb_dropout=0,\n                 spatial_dropout=False,\n                 orthogonal_init=False,\n                 linear_projection_size=0,\n                 out_of_index: str = 'clip',\n                 k=2,\n                 time_col='event_time'\n                ):\n        \n        super().__init__(\n            embeddings=embeddings,\n            numeric_values=numeric_values,\n            custom_embeddings=custom_embeddings,\n            embeddings_noise=embeddings_noise,\n            norm_embeddings=norm_embeddings,\n            use_batch_norm=use_batch_norm,\n            use_batch_norm_with_lens=use_batch_norm_with_lens,\n            clip_replace_value=clip_replace_value,\n            positions=positions,\n            emb_dropout=emb_dropout,\n            spatial_dropout=spatial_dropout,\n            orthogonal_init=orthogonal_init,\n            linear_projection_size=linear_projection_size,\n            out_of_index=out_of_index,\n            k=k,\n            time_col=time_col\n        )\n\n        self.agg_samples = agg_samples\n\n        channels = super().output_size\n\n        self.conv = nn.Conv1d(in_channels=channels, out_channels=channels, kernel_size=self.agg_samples, padding=(self.agg_samples - 1), bias=False) # (B, T, L)\n\n        self.use_window_attention = use_window_attention\n        if self.use_window_attention:\n            pass # Not Implemented\n\n    def forward(self, pb: PaddedBatch):\n        embeds = super().forward(pb)\n\n        mask = torch.arange(embeds.payload.shape[1], device=embeds.device)[None, :] + torch.ones((embeds.seq_lens.shape[0], embeds.payload.shape[1]), device=embeds.device)\n        mask[mask > embeds.seq_lens[:, None]] = 0.\n        mask[mask > 0.] = 1.\n        mask = mask[:, :, None]\n    \n        masked_embeds = embeds.payload * mask\n    \n        if self.use_window_attention:\n            pass # Not Implemented\n    \n        agg_embeds = torch.transpose(self.conv(torch.transpose(masked_embeds, 1, 2)), 1, 2)\n\n        new_seq_lens = embeds.seq_lens + self.agg_samples - 1\n\n        return PaddedBatch(agg_embeds, new_seq_lens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:07:50.427519Z","iopub.execute_input":"2025-05-09T15:07:50.427809Z","iopub.status.idle":"2025-05-09T15:07:50.436480Z","shell.execute_reply.started":"2025-05-09T15:07:50.427786Z","shell.execute_reply":"2025-05-09T15:07:50.435616Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"**Test:**","metadata":{}},{"cell_type":"code","source":"seed_everything(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:45:08.987718Z","iopub.execute_input":"2025-05-09T11:45:08.988032Z","iopub.status.idle":"2025-05-09T11:45:08.996612Z","shell.execute_reply.started":"2025-05-09T11:45:08.988008Z","shell.execute_reply":"2025-05-09T11:45:08.995874Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"device = \"cuda:0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:45:10.184332Z","iopub.execute_input":"2025-05-09T11:45:10.184617Z","iopub.status.idle":"2025-05-09T11:45:10.188097Z","shell.execute_reply.started":"2025-05-09T11:45:10.184596Z","shell.execute_reply":"2025-05-09T11:45:10.187213Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"trx_encoder_params = dict(\n    embeddings={\n        \"MCC\": {\"in\": 342, \"out\": 8},\n        \"channel_type\": {\"in\": 7, \"out\": 8},\n        \"currency\": {\"in\": 60, \"out\": 8},\n        \"trx_category\": {\"in\": 11, \"out\": 8}            \n    },\n    numeric_values={\"amount\": \"log\"},\n    embeddings_noise=0.003,\n    linear_projection_size=64,\n    k=7,\n    time_col=\"event_time\"\n)\n\ntrx_encoder = TrxEncoderT2V(**trx_encoder_params).to(device)\n\nseq_encoder = SWIN_RNN_SeqEncoder(\n    trx_encoder=trx_encoder,\n    swin_depths=[2, 2, 6, 2],\n    swin_num_heads=[2, 4, 8, 16],\n    swin_start_window_size=4,\n    swin_window_size_mult=2,\n    swin_drop=0.1,\n    swin_attn_drop=0.1,\n    swin_drop_path=0.1,\n    swin_decoder=True,\n    swin_start_end_fusion=False,\n    hidden_size=512,\n    type=\"gru\").to(device)\n\n# seq_encoder = SWIN_SeqEncoder(\n#     trx_encoder=trx_encoder,\n#     depths=[2, 2, 6, 2],\n#     num_heads=[2, 4, 8, 16],\n#     start_window_size=4,\n#     window_size_mult=2,\n#     drop=0.1,\n#     attn_drop=0.1,\n#     drop_path=0.1,\n#     decoder=True,\n#     start_end_fusion=False,\n#     is_reduce_sequence=True).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:46:05.627437Z","iopub.execute_input":"2025-05-09T11:46:05.627757Z","iopub.status.idle":"2025-05-09T11:46:05.710898Z","shell.execute_reply.started":"2025-05-09T11:46:05.627735Z","shell.execute_reply":"2025-05-09T11:46:05.709988Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"from ptls.data_load.padded_batch import PaddedBatch\n\ntrx_encoder.eval()\n\ntrain_loader = inference_data_loader(data_train, num_workers=0, batch_size=32)\n\nfor i, batch in tqdm(enumerate(train_loader)):\n    batch = batch.to(device)\n    embeds = seq_encoder(batch)\n    \n    if i == 0:\n        #print(batch.payload)\n        #print(batch.seq_lens)\n        #print()\n        #print(masked_embeds.payload[0, 4])\n        #print()\n        #print(embeds_batch.seq_lens)\n        print(embeds)\n        #print(embeds.shape)\n        #print(embeds.seq_lens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:46:22.922544Z","iopub.execute_input":"2025-05-09T11:46:22.922833Z","iopub.status.idle":"2025-05-09T11:46:28.506980Z","shell.execute_reply.started":"2025-05-09T11:46:22.922812Z","shell.execute_reply":"2025-05-09T11:46:28.506033Z"}},"outputs":[{"name":"stderr","text":"5it [00:00, 24.26it/s]","output_type":"stream"},{"name":"stdout","text":"tensor([[ 0.3058,  0.0266, -0.4723,  ..., -0.2322, -0.5250, -0.6277],\n        [ 0.4279,  0.6304, -0.9828,  ..., -0.9140, -0.9946, -0.8880],\n        [ 0.2657,  0.6070, -0.9835,  ..., -0.8823, -0.9938, -0.9012],\n        ...,\n        [ 0.3871,  0.7265, -0.9873,  ..., -0.9603, -0.9937, -0.8249],\n        [ 0.1095, -0.0019,  0.1778,  ...,  0.1959, -0.1310, -0.1935],\n        [ 0.1517,  0.0067,  0.1989,  ..., -0.2095, -0.4819, -0.4876]],\n       device='cuda:0', grad_fn=<IndexBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"141it [00:05, 25.29it/s]\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# import gc\n# #swin_backbone.cpu()\n# #del swin_backbone\n# del embeds\n# del batch\n# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:46:39.597136Z","iopub.execute_input":"2025-05-09T11:46:39.597460Z","iopub.status.idle":"2025-05-09T11:46:40.089292Z","shell.execute_reply.started":"2025-05-09T11:46:39.597433Z","shell.execute_reply":"2025-05-09T11:46:40.088358Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Train sequences lengths check:**","metadata":{}},{"cell_type":"code","source":"trx_encoder_params = dict(\n    embeddings={\n        \"MCC\": {\"in\": 342, \"out\": 8},\n        \"channel_type\": {\"in\": 7, \"out\": 8},\n        \"currency\": {\"in\": 60, \"out\": 8},\n        \"trx_category\": {\"in\": 11, \"out\": 8}            \n    },\n    numeric_values={\"amount\": \"log\"},\n    embeddings_noise=0.003,\n    linear_projection_size=64,\n    k=7,\n    time_col=\"event_time\"\n)\n\ntrx_encoder = TrxEncoderT2V(**trx_encoder_params)\ntrx_encoder.to(\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = inference_data_loader(data_train, num_workers=0, batch_size=128)\n\ntrx_encoder.eval()\n\nseq_lens = []\n\nfor batch in tqdm(train_loader):\n    embeds_batch = trx_encoder(batch.to(\"cuda\"))\n    seq_lens += [embeds_batch.seq_lens.detach().cpu().numpy()]\n\nseq_lens = np.concatenate(seq_lens)\n\nthreshold = int(np.quantile(seq_lens, 0.75) * 0.7)\n\nprint(\"Max Length:\", threshold)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T16:42:12.676361Z","iopub.execute_input":"2025-04-29T16:42:12.676674Z","iopub.status.idle":"2025-04-29T16:42:13.438525Z","shell.execute_reply.started":"2025-04-29T16:42:12.676652Z","shell.execute_reply":"2025-04-29T16:42:13.437692Z"}},"outputs":[{"name":"stderr","text":"36it [00:00, 48.00it/s]","output_type":"stream"},{"name":"stdout","text":"Max Length: 100\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# SWIN Aggregation ","metadata":{}},{"cell_type":"markdown","source":"- **COLES:**","metadata":{}},{"cell_type":"code","source":"seed_everything(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:40:50.681320Z","iopub.execute_input":"2025-05-09T14:40:50.681631Z","iopub.status.idle":"2025-05-09T14:40:50.686844Z","shell.execute_reply.started":"2025-05-09T14:40:50.681607Z","shell.execute_reply":"2025-05-09T14:40:50.686131Z"}},"outputs":[],"execution_count":202},{"cell_type":"markdown","source":"**DataLoaders:**","metadata":{}},{"cell_type":"code","source":"data = PtlsDataModule(\n    train_data=ColesDataset(\n        MemoryMapDataset(\n            data=data_train,\n            i_filters=[SeqLenFilter(min_seq_len=10)],\n        ),\n        splitter=SampleSlices(\n            split_count=5,\n            cnt_min=5,\n            cnt_max=100,\n        ),\n    ),\n    train_num_workers=4,\n    train_batch_size=128,\n    valid_data=ColesDataset(\n        MemoryMapDataset(\n            data=data_test,\n            i_filters=[SeqLenFilter(min_seq_len=10)],\n        ),\n        splitter=SampleSlices(\n            split_count=5,\n            cnt_min=5,\n            cnt_max=100,\n        ),\n    ),\n    valid_num_workers=4,\n    valid_batch_size=128\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:40:54.416364Z","iopub.execute_input":"2025-05-09T14:40:54.416662Z","iopub.status.idle":"2025-05-09T14:40:54.430969Z","shell.execute_reply.started":"2025-05-09T14:40:54.416640Z","shell.execute_reply":"2025-05-09T14:40:54.429962Z"}},"outputs":[],"execution_count":203},{"cell_type":"markdown","source":"**Модель:**","metadata":{}},{"cell_type":"code","source":"N_EPOCHS = 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:40:56.019466Z","iopub.execute_input":"2025-05-09T14:40:56.019758Z","iopub.status.idle":"2025-05-09T14:40:56.023552Z","shell.execute_reply.started":"2025-05-09T14:40:56.019736Z","shell.execute_reply":"2025-05-09T14:40:56.022717Z"}},"outputs":[],"execution_count":204},{"cell_type":"code","source":"trx_encoder_params = dict(\n    embeddings={\n        \"MCC\": {\"in\": 342, \"out\": 8},\n        \"channel_type\": {\"in\": 7, \"out\": 8},\n        \"currency\": {\"in\": 60, \"out\": 8},\n        \"trx_category\": {\"in\": 11, \"out\": 8}            \n    },\n    numeric_values={\"amount\": \"log\"},\n    embeddings_noise=0.003,\n    linear_projection_size=64,\n    k=7,\n    time_col=\"event_time\",\n    agg_samples=3,\n    use_window_attention=False\n)\n\n#trx_encoder = TrxEncoderT2V(**trx_encoder_params)\ntrx_encoder = ConvAggregator(**trx_encoder_params)\n\n# seq_encoder = SWIN_SeqEncoder(\n#     trx_encoder=trx_encoder,\n#     depths=[2, 2, 6, 2],\n#     num_heads=4,\n#     start_window_size=4,\n#     window_size_mult=2,\n#     drop=0.1,\n#     attn_drop=0.1,\n#     drop_path=0.1,\n#     decoder=False,\n#     start_end_fusion=True,\n#     is_reduce_sequence=True\n# )\n\nseq_encoder = SWIN_RNN_SeqEncoder(\n    trx_encoder=trx_encoder,\n    swin_depths=[2, 2, 6, 2],\n    swin_num_heads=[2, 4, 8, 16],\n    swin_start_window_size=4, # 2, 4\n    swin_window_size_mult=2,\n    swin_drop=0.1,\n    swin_attn_drop=0.1,\n    swin_drop_path=0.1,\n    swin_decoder=False,\n    swin_start_end_fusion=False,\n    hidden_size=512,\n    type=\"gru\"\n)\n\ncoles = CoLESModule(\n    seq_encoder=seq_encoder,\n    #loss=SoftmaxLoss(),\n    optimizer_partial=partial(torch.optim.Adam, lr=1e-3, weight_decay=0.),\n    lr_scheduler_partial=partial(torch.optim.lr_scheduler.CosineAnnealingLR, T_max=N_EPOCHS, eta_min=1e-6)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:41:25.656919Z","iopub.execute_input":"2025-05-09T14:41:25.657280Z","iopub.status.idle":"2025-05-09T14:41:25.693964Z","shell.execute_reply.started":"2025-05-09T14:41:25.657251Z","shell.execute_reply":"2025-05-09T14:41:25.693087Z"}},"outputs":[],"execution_count":205},{"cell_type":"markdown","source":"**Обучение:**","metadata":{}},{"cell_type":"code","source":"logger = CometLogger(project_name=\"evs-ssl-rb\", experiment_name=\"CoLES_SWIN_agg (w/ conv_agg, 5trx)\")\n\ntrainer = pl.Trainer(\n    logger=logger,\n    max_epochs=N_EPOCHS,\n    accelerator=\"gpu\",\n    devices=1,\n    enable_progress_bar=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:41:49.489015Z","iopub.execute_input":"2025-05-09T14:41:49.489388Z","iopub.status.idle":"2025-05-09T14:41:49.533496Z","shell.execute_reply.started":"2025-05-09T14:41:49.489359Z","shell.execute_reply":"2025-05-09T14:41:49.532843Z"}},"outputs":[],"execution_count":206},{"cell_type":"code","source":"trainer.fit(coles, data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:41:50.940673Z","iopub.execute_input":"2025-05-09T14:41:50.940962Z","iopub.status.idle":"2025-05-09T14:47:35.640286Z","shell.execute_reply.started":"2025-05-09T14:41:50.940942Z","shell.execute_reply":"2025-05-09T14:47:35.639498Z"}},"outputs":[{"name":"stderr","text":"\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/askoro/evs-ssl-rb/632953b300fc4d5389bb5e4e2768aaae\n\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/kaggle/working' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:310: PossibleUserWarning:\n\nThe number of training batches (33) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39f306e6097f444faef694ef4ca2dd26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : CoLES_SWIN_agg (w/ conv_agg, 5trx)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/askoro/evs-ssl-rb/632953b300fc4d5389bb5e4e2768aaae\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [79]               : (80.70063781738281, 808.0537109375)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seq_len [13]            : (36.43281173706055, 41.673439025878906)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid/recall_top_k [20] : (0.08885176479816437, 0.6555304527282715)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : CoLES_SWIN_agg (w/ conv_agg, 5trx)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_batch_size   : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_drop_last    : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_num_workers  : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_batch_size  : 128\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_drop_last   : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_num_workers : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid_batch_size  : 128\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid_drop_last   : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid_num_workers : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph         : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n","output_type":"stream"}],"execution_count":207},{"cell_type":"code","source":"trainer.logged_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:47:46.897992Z","iopub.execute_input":"2025-05-09T14:47:46.898326Z","iopub.status.idle":"2025-05-09T14:47:46.905686Z","shell.execute_reply.started":"2025-05-09T14:47:46.898299Z","shell.execute_reply":"2025-05-09T14:47:46.904762Z"}},"outputs":[{"execution_count":208,"output_type":"execute_result","data":{"text/plain":"{'loss': tensor(76.2719),\n 'seq_len': tensor(34.2066),\n 'valid/recall_top_k': tensor(0.6555)}"},"metadata":{}}],"execution_count":208},{"cell_type":"code","source":"torch.save(seq_encoder.state_dict(), \"coles_enc_baseline_rosbank.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T01:34:02.719311Z","iopub.execute_input":"2025-02-26T01:34:02.719608Z","iopub.status.idle":"2025-02-26T01:34:02.729366Z","shell.execute_reply.started":"2025-02-26T01:34:02.719586Z","shell.execute_reply":"2025-02-26T01:34:02.728654Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"**Измерим качество на тесте (catboost поверх эмбеддингов):**","metadata":{}},{"cell_type":"code","source":"# !wget \"https://drive.google.com/uc?export=download&id=1Mn8o9IPT4Zzg3946orbw1MVZwpkrBoNb\" -O \"coles_enc_baseline.pt\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder = coles.seq_encoder\n\n# state_dict = torch.load(\"./coles_enc_baseline.pt\")\n# encoder.load_state_dict(state_dict)\n\ndevice = \"cuda:0\"\n\nencoder.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:47:49.370940Z","iopub.execute_input":"2025-05-09T14:47:49.371303Z","iopub.status.idle":"2025-05-09T14:47:49.388383Z","shell.execute_reply.started":"2025-05-09T14:47:49.371265Z","shell.execute_reply":"2025-05-09T14:47:49.387561Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":209,"output_type":"execute_result","data":{"text/plain":"SWIN_RNN_SeqEncoder(\n  (trx_encoder): ConvAggregator(\n    (embeddings): ModuleDict(\n      (MCC): NoisyEmbedding(\n        342, 8, padding_idx=0\n        (dropout): Dropout(p=0, inplace=False)\n      )\n      (channel_type): NoisyEmbedding(\n        7, 8, padding_idx=0\n        (dropout): Dropout(p=0, inplace=False)\n      )\n      (currency): NoisyEmbedding(\n        60, 8, padding_idx=0\n        (dropout): Dropout(p=0, inplace=False)\n      )\n      (trx_category): NoisyEmbedding(\n        11, 8, padding_idx=0\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (custom_embeddings): ModuleDict(\n      (amount): LogScaler()\n    )\n    (time2vec_days): Time2Vec()\n    (linear_projection_head): Linear(in_features=41, out_features=64, bias=True)\n    (conv): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(4,), bias=False)\n  )\n  (seq_encoder): RnnEncoder(\n    (rnn): GRU(64, 512, batch_first=True)\n    (reducer): LastStepEncoder()\n  )\n  (swin_fusion): SwinTransformerBackbone(\n    (backbone): ModuleList(\n      (0): SwinTransformerLayer(\n        dim=64, depth=2, num_heads=2, window_size=4\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=64, num_heads=2, window_size=4, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=4, num_heads=2\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=64, num_heads=2, window_size=4, shift_size=2, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=4, num_heads=2\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (1): SwinTransformerLayer(\n        dim=64, depth=2, num_heads=4, window_size=8\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=64, num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=8, num_heads=4\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=64, num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=8, num_heads=4\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (2): SwinTransformerLayer(\n        dim=64, depth=6, num_heads=8, window_size=16\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=64, num_heads=8, window_size=16, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=16, num_heads=8\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=64, num_heads=8, window_size=16, shift_size=8, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=16, num_heads=8\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): SwinTransformerBlock(\n            dim=64, num_heads=8, window_size=16, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=16, num_heads=8\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): SwinTransformerBlock(\n            dim=64, num_heads=8, window_size=16, shift_size=8, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=16, num_heads=8\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): SwinTransformerBlock(\n            dim=64, num_heads=8, window_size=16, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=16, num_heads=8\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): SwinTransformerBlock(\n            dim=64, num_heads=8, window_size=16, shift_size=8, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=16, num_heads=8\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (3): SwinTransformerLayer(\n        dim=64, depth=2, num_heads=16, window_size=32\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=64, num_heads=16, window_size=32, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=32, num_heads=16\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=64, num_heads=16, window_size=32, shift_size=16, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=32, num_heads=16\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":209},{"cell_type":"code","source":"# change_to_enc(encoder.swin_fusion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:25:43.984888Z","iopub.execute_input":"2025-05-02T14:25:43.985183Z","iopub.status.idle":"2025-05-02T14:25:43.989248Z","shell.execute_reply.started":"2025-05-02T14:25:43.985161Z","shell.execute_reply":"2025-05-02T14:25:43.988304Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"from tqdm import tqdm\n\nseed_everything(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:47:52.693981Z","iopub.execute_input":"2025-05-09T14:47:52.694355Z","iopub.status.idle":"2025-05-09T14:47:52.699713Z","shell.execute_reply.started":"2025-05-09T14:47:52.694326Z","shell.execute_reply":"2025-05-09T14:47:52.698853Z"}},"outputs":[],"execution_count":210},{"cell_type":"code","source":"train_loader = inference_data_loader(data_train, num_workers=0, batch_size=128)\nencoder.eval()\ntrain_embeds = None\n\nfor i, batch in tqdm(enumerate(train_loader)):\n    train_embeds_batch = encoder(batch.to(device))\n    if i == 0:\n        train_embeds = train_embeds_batch.detach().cpu().numpy()\n    else:\n        train_embeds = np.concatenate([train_embeds, train_embeds_batch.detach().cpu().numpy()], axis=0)\n    \ntrain_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:47:55.713604Z","iopub.execute_input":"2025-05-09T14:47:55.713894Z","iopub.status.idle":"2025-05-09T14:48:00.339021Z","shell.execute_reply.started":"2025-05-09T14:47:55.713871Z","shell.execute_reply":"2025-05-09T14:48:00.338280Z"}},"outputs":[{"name":"stderr","text":"36it [00:04,  7.80it/s]\n","output_type":"stream"},{"execution_count":211,"output_type":"execute_result","data":{"text/plain":"array([[ 0.3654307 , -0.9336183 ,  0.9923521 , ..., -0.79270506,\n        -0.9202323 , -0.846506  ],\n       [ 0.22435653, -0.9190185 ,  0.9950493 , ..., -0.4321019 ,\n        -0.9257525 , -0.80878735],\n       [ 0.7760851 , -0.9392842 ,  0.99143946, ..., -0.63314885,\n        -0.90790373, -0.81972027],\n       ...,\n       [-0.0429655 , -0.9298562 ,  0.9901741 , ..., -0.23477188,\n        -0.90675217, -0.70827776],\n       [ 0.21107441, -0.93539536,  0.9918051 , ..., -0.08189818,\n        -0.91077983, -0.6599331 ],\n       [ 0.07144204, -0.94521785,  0.9964512 , ..., -0.01008347,\n        -0.9235658 , -0.559582  ]], dtype=float32)"},"metadata":{}}],"execution_count":211},{"cell_type":"code","source":"test_loader = inference_data_loader(data_test, num_workers=0, batch_size=128)\nencoder.eval()\ntest_embeds = None\n\nfor i, batch in tqdm(enumerate(test_loader)):\n    test_embeds_batch = encoder(batch.to(device))\n    if i == 0:\n        test_embeds = test_embeds_batch.detach().cpu().numpy()\n    else:\n        test_embeds = np.concatenate([test_embeds, test_embeds_batch.detach().cpu().numpy()], axis=0)\n    \ntest_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:48:01.987593Z","iopub.execute_input":"2025-05-09T14:48:01.987916Z","iopub.status.idle":"2025-05-09T14:48:02.499508Z","shell.execute_reply.started":"2025-05-09T14:48:01.987886Z","shell.execute_reply":"2025-05-09T14:48:02.498778Z"}},"outputs":[{"name":"stderr","text":"4it [00:00,  8.00it/s]\n","output_type":"stream"},{"execution_count":212,"output_type":"execute_result","data":{"text/plain":"array([[-0.06601859, -0.9046768 ,  0.9926913 , ..., -0.41738456,\n        -0.89603454, -0.70903504],\n       [ 0.52761394, -0.9205212 ,  0.9918173 , ..., -0.76959485,\n        -0.857632  , -0.8636213 ],\n       [-0.03815325, -0.91488475,  0.9963867 , ..., -0.21135972,\n        -0.89220965, -0.69404054],\n       ...,\n       [ 0.7193384 , -0.9260903 ,  0.9931525 , ..., -0.20386115,\n        -0.91107005, -0.7313736 ],\n       [ 0.01250206, -0.92826676,  0.9942206 , ..., -0.39163008,\n        -0.89881516, -0.63099337],\n       [-0.1448749 , -0.9290574 ,  0.9948817 , ..., -0.11638625,\n        -0.88336426, -0.5805926 ]], dtype=float32)"},"metadata":{}}],"execution_count":212},{"cell_type":"code","source":"import gc\n\ngc.collect()\n\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:48:04.598017Z","iopub.execute_input":"2025-05-09T14:48:04.598381Z","iopub.status.idle":"2025-05-09T14:48:05.234002Z","shell.execute_reply.started":"2025-05-09T14:48:04.598357Z","shell.execute_reply":"2025-05-09T14:48:05.233340Z"}},"outputs":[],"execution_count":213},{"cell_type":"code","source":"clf = CatBoostClassifier(loss_function='MultiClass', task_type=\"GPU\", devices='0', random_state=0)\n\nclf.fit(train_embeds, target_train, plot_file=\"catboost_log.html\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:48:06.555647Z","iopub.execute_input":"2025-05-09T14:48:06.555948Z","iopub.status.idle":"2025-05-09T14:48:13.056604Z","shell.execute_reply.started":"2025-05-09T14:48:06.555927Z","shell.execute_reply":"2025-05-09T14:48:13.055745Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 10573.125 Total: 16269.25\n","output_type":"stream"},{"name":"stdout","text":"Learning rate set to 0.088214\n0:\tlearn: 0.6656387\ttotal: 10.1ms\tremaining: 10.1s\n1:\tlearn: 0.6426715\ttotal: 16.6ms\tremaining: 8.3s\n2:\tlearn: 0.6224552\ttotal: 23.1ms\tremaining: 7.67s\n3:\tlearn: 0.6051181\ttotal: 29.6ms\tremaining: 7.38s\n4:\tlearn: 0.5890547\ttotal: 36.1ms\tremaining: 7.19s\n5:\tlearn: 0.5758434\ttotal: 42.5ms\tremaining: 7.04s\n6:\tlearn: 0.5633223\ttotal: 49.1ms\tremaining: 6.96s\n7:\tlearn: 0.5520632\ttotal: 55.7ms\tremaining: 6.91s\n8:\tlearn: 0.5426978\ttotal: 62.2ms\tremaining: 6.85s\n9:\tlearn: 0.5331597\ttotal: 68.8ms\tremaining: 6.82s\n10:\tlearn: 0.5244493\ttotal: 75.2ms\tremaining: 6.76s\n11:\tlearn: 0.5172855\ttotal: 81.8ms\tremaining: 6.73s\n12:\tlearn: 0.5101961\ttotal: 88.4ms\tremaining: 6.71s\n13:\tlearn: 0.5043178\ttotal: 94.7ms\tremaining: 6.67s\n14:\tlearn: 0.4987792\ttotal: 101ms\tremaining: 6.63s\n15:\tlearn: 0.4940862\ttotal: 107ms\tremaining: 6.61s\n16:\tlearn: 0.4893876\ttotal: 114ms\tremaining: 6.58s\n17:\tlearn: 0.4850416\ttotal: 120ms\tremaining: 6.54s\n18:\tlearn: 0.4808889\ttotal: 127ms\tremaining: 6.56s\n19:\tlearn: 0.4769748\ttotal: 135ms\tremaining: 6.6s\n20:\tlearn: 0.4734784\ttotal: 141ms\tremaining: 6.59s\n21:\tlearn: 0.4701234\ttotal: 147ms\tremaining: 6.55s\n22:\tlearn: 0.4664367\ttotal: 153ms\tremaining: 6.51s\n23:\tlearn: 0.4636854\ttotal: 159ms\tremaining: 6.48s\n24:\tlearn: 0.4604033\ttotal: 166ms\tremaining: 6.46s\n25:\tlearn: 0.4577567\ttotal: 172ms\tremaining: 6.45s\n26:\tlearn: 0.4550822\ttotal: 179ms\tremaining: 6.44s\n27:\tlearn: 0.4521526\ttotal: 185ms\tremaining: 6.43s\n28:\tlearn: 0.4495663\ttotal: 191ms\tremaining: 6.41s\n29:\tlearn: 0.4475791\ttotal: 198ms\tremaining: 6.39s\n30:\tlearn: 0.4456808\ttotal: 204ms\tremaining: 6.38s\n31:\tlearn: 0.4432780\ttotal: 210ms\tremaining: 6.37s\n32:\tlearn: 0.4411986\ttotal: 217ms\tremaining: 6.34s\n33:\tlearn: 0.4394652\ttotal: 223ms\tremaining: 6.33s\n34:\tlearn: 0.4377814\ttotal: 229ms\tremaining: 6.32s\n35:\tlearn: 0.4361618\ttotal: 236ms\tremaining: 6.31s\n36:\tlearn: 0.4344537\ttotal: 243ms\tremaining: 6.32s\n37:\tlearn: 0.4327069\ttotal: 249ms\tremaining: 6.31s\n38:\tlearn: 0.4307769\ttotal: 256ms\tremaining: 6.3s\n39:\tlearn: 0.4293188\ttotal: 262ms\tremaining: 6.29s\n40:\tlearn: 0.4275355\ttotal: 269ms\tremaining: 6.29s\n41:\tlearn: 0.4260304\ttotal: 275ms\tremaining: 6.28s\n42:\tlearn: 0.4248207\ttotal: 283ms\tremaining: 6.3s\n43:\tlearn: 0.4231957\ttotal: 290ms\tremaining: 6.31s\n44:\tlearn: 0.4215314\ttotal: 297ms\tremaining: 6.3s\n45:\tlearn: 0.4200881\ttotal: 305ms\tremaining: 6.32s\n46:\tlearn: 0.4190785\ttotal: 312ms\tremaining: 6.33s\n47:\tlearn: 0.4174944\ttotal: 322ms\tremaining: 6.38s\n48:\tlearn: 0.4157633\ttotal: 329ms\tremaining: 6.38s\n49:\tlearn: 0.4148178\ttotal: 335ms\tremaining: 6.37s\n50:\tlearn: 0.4137808\ttotal: 341ms\tremaining: 6.35s\n51:\tlearn: 0.4125130\ttotal: 348ms\tremaining: 6.34s\n52:\tlearn: 0.4112156\ttotal: 354ms\tremaining: 6.33s\n53:\tlearn: 0.4097741\ttotal: 361ms\tremaining: 6.32s\n54:\tlearn: 0.4085765\ttotal: 366ms\tremaining: 6.3s\n55:\tlearn: 0.4076275\ttotal: 372ms\tremaining: 6.27s\n56:\tlearn: 0.4064818\ttotal: 378ms\tremaining: 6.25s\n57:\tlearn: 0.4051195\ttotal: 384ms\tremaining: 6.23s\n58:\tlearn: 0.4041847\ttotal: 389ms\tremaining: 6.21s\n59:\tlearn: 0.4033817\ttotal: 395ms\tremaining: 6.19s\n60:\tlearn: 0.4023184\ttotal: 401ms\tremaining: 6.17s\n61:\tlearn: 0.4011231\ttotal: 407ms\tremaining: 6.15s\n62:\tlearn: 0.3999491\ttotal: 412ms\tremaining: 6.13s\n63:\tlearn: 0.3990498\ttotal: 418ms\tremaining: 6.11s\n64:\tlearn: 0.3979066\ttotal: 424ms\tremaining: 6.1s\n65:\tlearn: 0.3968040\ttotal: 430ms\tremaining: 6.08s\n66:\tlearn: 0.3953773\ttotal: 436ms\tremaining: 6.07s\n67:\tlearn: 0.3947268\ttotal: 441ms\tremaining: 6.05s\n68:\tlearn: 0.3932327\ttotal: 447ms\tremaining: 6.04s\n69:\tlearn: 0.3925010\ttotal: 453ms\tremaining: 6.02s\n70:\tlearn: 0.3917636\ttotal: 459ms\tremaining: 6s\n71:\tlearn: 0.3909257\ttotal: 464ms\tremaining: 5.99s\n72:\tlearn: 0.3897426\ttotal: 470ms\tremaining: 5.97s\n73:\tlearn: 0.3887756\ttotal: 476ms\tremaining: 5.96s\n74:\tlearn: 0.3879135\ttotal: 482ms\tremaining: 5.94s\n75:\tlearn: 0.3870746\ttotal: 488ms\tremaining: 5.93s\n76:\tlearn: 0.3856943\ttotal: 494ms\tremaining: 5.92s\n77:\tlearn: 0.3848421\ttotal: 500ms\tremaining: 5.91s\n78:\tlearn: 0.3837514\ttotal: 506ms\tremaining: 5.89s\n79:\tlearn: 0.3832983\ttotal: 511ms\tremaining: 5.88s\n80:\tlearn: 0.3824801\ttotal: 517ms\tremaining: 5.86s\n81:\tlearn: 0.3812856\ttotal: 523ms\tremaining: 5.85s\n82:\tlearn: 0.3803459\ttotal: 529ms\tremaining: 5.84s\n83:\tlearn: 0.3793592\ttotal: 535ms\tremaining: 5.84s\n84:\tlearn: 0.3781111\ttotal: 542ms\tremaining: 5.83s\n85:\tlearn: 0.3775114\ttotal: 548ms\tremaining: 5.83s\n86:\tlearn: 0.3766455\ttotal: 555ms\tremaining: 5.82s\n87:\tlearn: 0.3756065\ttotal: 562ms\tremaining: 5.82s\n88:\tlearn: 0.3746630\ttotal: 568ms\tremaining: 5.82s\n89:\tlearn: 0.3735833\ttotal: 575ms\tremaining: 5.81s\n90:\tlearn: 0.3726581\ttotal: 582ms\tremaining: 5.81s\n91:\tlearn: 0.3716116\ttotal: 589ms\tremaining: 5.81s\n92:\tlearn: 0.3710330\ttotal: 595ms\tremaining: 5.8s\n93:\tlearn: 0.3699241\ttotal: 601ms\tremaining: 5.8s\n94:\tlearn: 0.3686259\ttotal: 608ms\tremaining: 5.79s\n95:\tlearn: 0.3680760\ttotal: 615ms\tremaining: 5.79s\n96:\tlearn: 0.3674261\ttotal: 621ms\tremaining: 5.78s\n97:\tlearn: 0.3665143\ttotal: 628ms\tremaining: 5.78s\n98:\tlearn: 0.3657976\ttotal: 634ms\tremaining: 5.77s\n99:\tlearn: 0.3647931\ttotal: 641ms\tremaining: 5.77s\n100:\tlearn: 0.3639352\ttotal: 647ms\tremaining: 5.76s\n101:\tlearn: 0.3632198\ttotal: 654ms\tremaining: 5.76s\n102:\tlearn: 0.3623120\ttotal: 660ms\tremaining: 5.75s\n103:\tlearn: 0.3617032\ttotal: 667ms\tremaining: 5.75s\n104:\tlearn: 0.3609647\ttotal: 673ms\tremaining: 5.74s\n105:\tlearn: 0.3599800\ttotal: 680ms\tremaining: 5.74s\n106:\tlearn: 0.3588966\ttotal: 687ms\tremaining: 5.73s\n107:\tlearn: 0.3581099\ttotal: 693ms\tremaining: 5.73s\n108:\tlearn: 0.3574395\ttotal: 700ms\tremaining: 5.72s\n109:\tlearn: 0.3564658\ttotal: 706ms\tremaining: 5.71s\n110:\tlearn: 0.3556653\ttotal: 713ms\tremaining: 5.71s\n111:\tlearn: 0.3545951\ttotal: 719ms\tremaining: 5.7s\n112:\tlearn: 0.3540891\ttotal: 726ms\tremaining: 5.7s\n113:\tlearn: 0.3531905\ttotal: 732ms\tremaining: 5.69s\n114:\tlearn: 0.3525522\ttotal: 739ms\tremaining: 5.68s\n115:\tlearn: 0.3516592\ttotal: 745ms\tremaining: 5.68s\n116:\tlearn: 0.3506292\ttotal: 752ms\tremaining: 5.67s\n117:\tlearn: 0.3499515\ttotal: 758ms\tremaining: 5.67s\n118:\tlearn: 0.3488925\ttotal: 765ms\tremaining: 5.67s\n119:\tlearn: 0.3484183\ttotal: 771ms\tremaining: 5.65s\n120:\tlearn: 0.3477615\ttotal: 777ms\tremaining: 5.64s\n121:\tlearn: 0.3467865\ttotal: 782ms\tremaining: 5.63s\n122:\tlearn: 0.3463514\ttotal: 788ms\tremaining: 5.62s\n123:\tlearn: 0.3457916\ttotal: 794ms\tremaining: 5.61s\n124:\tlearn: 0.3452110\ttotal: 799ms\tremaining: 5.6s\n125:\tlearn: 0.3441284\ttotal: 805ms\tremaining: 5.59s\n126:\tlearn: 0.3435438\ttotal: 811ms\tremaining: 5.58s\n127:\tlearn: 0.3428678\ttotal: 817ms\tremaining: 5.57s\n128:\tlearn: 0.3419469\ttotal: 824ms\tremaining: 5.56s\n129:\tlearn: 0.3412283\ttotal: 830ms\tremaining: 5.55s\n130:\tlearn: 0.3401901\ttotal: 836ms\tremaining: 5.54s\n131:\tlearn: 0.3393685\ttotal: 842ms\tremaining: 5.53s\n132:\tlearn: 0.3388457\ttotal: 847ms\tremaining: 5.52s\n133:\tlearn: 0.3378436\ttotal: 853ms\tremaining: 5.51s\n134:\tlearn: 0.3370420\ttotal: 859ms\tremaining: 5.5s\n135:\tlearn: 0.3361179\ttotal: 865ms\tremaining: 5.5s\n136:\tlearn: 0.3353820\ttotal: 871ms\tremaining: 5.49s\n137:\tlearn: 0.3349071\ttotal: 876ms\tremaining: 5.47s\n138:\tlearn: 0.3343317\ttotal: 882ms\tremaining: 5.46s\n139:\tlearn: 0.3337773\ttotal: 888ms\tremaining: 5.45s\n140:\tlearn: 0.3331590\ttotal: 894ms\tremaining: 5.44s\n141:\tlearn: 0.3324806\ttotal: 899ms\tremaining: 5.43s\n142:\tlearn: 0.3318110\ttotal: 905ms\tremaining: 5.42s\n143:\tlearn: 0.3311132\ttotal: 910ms\tremaining: 5.41s\n144:\tlearn: 0.3303722\ttotal: 917ms\tremaining: 5.41s\n145:\tlearn: 0.3297716\ttotal: 923ms\tremaining: 5.4s\n146:\tlearn: 0.3292153\ttotal: 930ms\tremaining: 5.39s\n147:\tlearn: 0.3280646\ttotal: 937ms\tremaining: 5.39s\n148:\tlearn: 0.3272600\ttotal: 943ms\tremaining: 5.39s\n149:\tlearn: 0.3265212\ttotal: 950ms\tremaining: 5.38s\n150:\tlearn: 0.3255489\ttotal: 957ms\tremaining: 5.38s\n151:\tlearn: 0.3250197\ttotal: 964ms\tremaining: 5.38s\n152:\tlearn: 0.3241429\ttotal: 971ms\tremaining: 5.37s\n153:\tlearn: 0.3231659\ttotal: 977ms\tremaining: 5.37s\n154:\tlearn: 0.3226751\ttotal: 984ms\tremaining: 5.36s\n155:\tlearn: 0.3220119\ttotal: 991ms\tremaining: 5.36s\n156:\tlearn: 0.3212953\ttotal: 997ms\tremaining: 5.35s\n157:\tlearn: 0.3207663\ttotal: 1s\tremaining: 5.35s\n158:\tlearn: 0.3201858\ttotal: 1.01s\tremaining: 5.34s\n159:\tlearn: 0.3196297\ttotal: 1.01s\tremaining: 5.33s\n160:\tlearn: 0.3189766\ttotal: 1.02s\tremaining: 5.32s\n161:\tlearn: 0.3182838\ttotal: 1.03s\tremaining: 5.31s\n162:\tlearn: 0.3173477\ttotal: 1.03s\tremaining: 5.31s\n163:\tlearn: 0.3166586\ttotal: 1.04s\tremaining: 5.3s\n164:\tlearn: 0.3156597\ttotal: 1.04s\tremaining: 5.29s\n165:\tlearn: 0.3149747\ttotal: 1.05s\tremaining: 5.28s\n166:\tlearn: 0.3143684\ttotal: 1.06s\tremaining: 5.28s\n167:\tlearn: 0.3138207\ttotal: 1.06s\tremaining: 5.27s\n168:\tlearn: 0.3131305\ttotal: 1.07s\tremaining: 5.26s\n169:\tlearn: 0.3123102\ttotal: 1.07s\tremaining: 5.25s\n170:\tlearn: 0.3120076\ttotal: 1.08s\tremaining: 5.24s\n171:\tlearn: 0.3115498\ttotal: 1.09s\tremaining: 5.23s\n172:\tlearn: 0.3108920\ttotal: 1.09s\tremaining: 5.22s\n173:\tlearn: 0.3100913\ttotal: 1.1s\tremaining: 5.21s\n174:\tlearn: 0.3096984\ttotal: 1.1s\tremaining: 5.21s\n175:\tlearn: 0.3091965\ttotal: 1.11s\tremaining: 5.2s\n176:\tlearn: 0.3086328\ttotal: 1.12s\tremaining: 5.19s\n177:\tlearn: 0.3081323\ttotal: 1.12s\tremaining: 5.18s\n178:\tlearn: 0.3069316\ttotal: 1.13s\tremaining: 5.17s\n179:\tlearn: 0.3063101\ttotal: 1.13s\tremaining: 5.16s\n180:\tlearn: 0.3059593\ttotal: 1.14s\tremaining: 5.16s\n181:\tlearn: 0.3054155\ttotal: 1.15s\tremaining: 5.15s\n182:\tlearn: 0.3048420\ttotal: 1.15s\tremaining: 5.14s\n183:\tlearn: 0.3043539\ttotal: 1.16s\tremaining: 5.13s\n184:\tlearn: 0.3035487\ttotal: 1.16s\tremaining: 5.12s\n185:\tlearn: 0.3028014\ttotal: 1.17s\tremaining: 5.12s\n186:\tlearn: 0.3019586\ttotal: 1.18s\tremaining: 5.11s\n187:\tlearn: 0.3013095\ttotal: 1.18s\tremaining: 5.11s\n188:\tlearn: 0.3007789\ttotal: 1.19s\tremaining: 5.1s\n189:\tlearn: 0.3001102\ttotal: 1.2s\tremaining: 5.1s\n190:\tlearn: 0.2995592\ttotal: 1.2s\tremaining: 5.09s\n191:\tlearn: 0.2990514\ttotal: 1.21s\tremaining: 5.08s\n192:\tlearn: 0.2983287\ttotal: 1.21s\tremaining: 5.08s\n193:\tlearn: 0.2979152\ttotal: 1.22s\tremaining: 5.07s\n194:\tlearn: 0.2972835\ttotal: 1.23s\tremaining: 5.06s\n195:\tlearn: 0.2970065\ttotal: 1.23s\tremaining: 5.05s\n196:\tlearn: 0.2964796\ttotal: 1.24s\tremaining: 5.04s\n197:\tlearn: 0.2957460\ttotal: 1.24s\tremaining: 5.04s\n198:\tlearn: 0.2951350\ttotal: 1.25s\tremaining: 5.03s\n199:\tlearn: 0.2944078\ttotal: 1.26s\tremaining: 5.03s\n200:\tlearn: 0.2935613\ttotal: 1.26s\tremaining: 5.02s\n201:\tlearn: 0.2931955\ttotal: 1.27s\tremaining: 5.01s\n202:\tlearn: 0.2927745\ttotal: 1.27s\tremaining: 5s\n203:\tlearn: 0.2922809\ttotal: 1.28s\tremaining: 4.99s\n204:\tlearn: 0.2916616\ttotal: 1.28s\tremaining: 4.98s\n205:\tlearn: 0.2911688\ttotal: 1.29s\tremaining: 4.97s\n206:\tlearn: 0.2901957\ttotal: 1.3s\tremaining: 4.97s\n207:\tlearn: 0.2896284\ttotal: 1.3s\tremaining: 4.96s\n208:\tlearn: 0.2887812\ttotal: 1.31s\tremaining: 4.95s\n209:\tlearn: 0.2882006\ttotal: 1.31s\tremaining: 4.95s\n210:\tlearn: 0.2875545\ttotal: 1.32s\tremaining: 4.94s\n211:\tlearn: 0.2867159\ttotal: 1.33s\tremaining: 4.94s\n212:\tlearn: 0.2861143\ttotal: 1.33s\tremaining: 4.93s\n213:\tlearn: 0.2857000\ttotal: 1.34s\tremaining: 4.92s\n214:\tlearn: 0.2852767\ttotal: 1.34s\tremaining: 4.91s\n215:\tlearn: 0.2846231\ttotal: 1.35s\tremaining: 4.91s\n216:\tlearn: 0.2841522\ttotal: 1.36s\tremaining: 4.9s\n217:\tlearn: 0.2833282\ttotal: 1.36s\tremaining: 4.89s\n218:\tlearn: 0.2829230\ttotal: 1.37s\tremaining: 4.88s\n219:\tlearn: 0.2824411\ttotal: 1.38s\tremaining: 4.88s\n220:\tlearn: 0.2819312\ttotal: 1.38s\tremaining: 4.87s\n221:\tlearn: 0.2811163\ttotal: 1.39s\tremaining: 4.87s\n222:\tlearn: 0.2803372\ttotal: 1.39s\tremaining: 4.86s\n223:\tlearn: 0.2796974\ttotal: 1.4s\tremaining: 4.85s\n224:\tlearn: 0.2792327\ttotal: 1.41s\tremaining: 4.85s\n225:\tlearn: 0.2788023\ttotal: 1.41s\tremaining: 4.84s\n226:\tlearn: 0.2782930\ttotal: 1.42s\tremaining: 4.83s\n227:\tlearn: 0.2778554\ttotal: 1.43s\tremaining: 4.83s\n228:\tlearn: 0.2773650\ttotal: 1.43s\tremaining: 4.82s\n229:\tlearn: 0.2767383\ttotal: 1.44s\tremaining: 4.81s\n230:\tlearn: 0.2761860\ttotal: 1.44s\tremaining: 4.8s\n231:\tlearn: 0.2758022\ttotal: 1.45s\tremaining: 4.79s\n232:\tlearn: 0.2751444\ttotal: 1.45s\tremaining: 4.79s\n233:\tlearn: 0.2746590\ttotal: 1.46s\tremaining: 4.78s\n234:\tlearn: 0.2739840\ttotal: 1.47s\tremaining: 4.77s\n235:\tlearn: 0.2735727\ttotal: 1.47s\tremaining: 4.76s\n236:\tlearn: 0.2732052\ttotal: 1.48s\tremaining: 4.76s\n237:\tlearn: 0.2727250\ttotal: 1.48s\tremaining: 4.75s\n238:\tlearn: 0.2723490\ttotal: 1.49s\tremaining: 4.74s\n239:\tlearn: 0.2719305\ttotal: 1.5s\tremaining: 4.74s\n240:\tlearn: 0.2714315\ttotal: 1.5s\tremaining: 4.73s\n241:\tlearn: 0.2708959\ttotal: 1.51s\tremaining: 4.72s\n242:\tlearn: 0.2703808\ttotal: 1.51s\tremaining: 4.71s\n243:\tlearn: 0.2701140\ttotal: 1.52s\tremaining: 4.7s\n244:\tlearn: 0.2697343\ttotal: 1.52s\tremaining: 4.7s\n245:\tlearn: 0.2690562\ttotal: 1.53s\tremaining: 4.69s\n246:\tlearn: 0.2683787\ttotal: 1.53s\tremaining: 4.68s\n247:\tlearn: 0.2678424\ttotal: 1.54s\tremaining: 4.67s\n248:\tlearn: 0.2673719\ttotal: 1.55s\tremaining: 4.67s\n249:\tlearn: 0.2669557\ttotal: 1.55s\tremaining: 4.66s\n250:\tlearn: 0.2663019\ttotal: 1.56s\tremaining: 4.65s\n251:\tlearn: 0.2658137\ttotal: 1.56s\tremaining: 4.65s\n252:\tlearn: 0.2652820\ttotal: 1.57s\tremaining: 4.64s\n253:\tlearn: 0.2647543\ttotal: 1.58s\tremaining: 4.63s\n254:\tlearn: 0.2644019\ttotal: 1.58s\tremaining: 4.63s\n255:\tlearn: 0.2639100\ttotal: 1.59s\tremaining: 4.62s\n256:\tlearn: 0.2634300\ttotal: 1.59s\tremaining: 4.61s\n257:\tlearn: 0.2628811\ttotal: 1.6s\tremaining: 4.6s\n258:\tlearn: 0.2622527\ttotal: 1.61s\tremaining: 4.59s\n259:\tlearn: 0.2617588\ttotal: 1.61s\tremaining: 4.59s\n260:\tlearn: 0.2611046\ttotal: 1.62s\tremaining: 4.58s\n261:\tlearn: 0.2604688\ttotal: 1.62s\tremaining: 4.57s\n262:\tlearn: 0.2600040\ttotal: 1.63s\tremaining: 4.57s\n263:\tlearn: 0.2594096\ttotal: 1.64s\tremaining: 4.56s\n264:\tlearn: 0.2587389\ttotal: 1.64s\tremaining: 4.55s\n265:\tlearn: 0.2583930\ttotal: 1.65s\tremaining: 4.54s\n266:\tlearn: 0.2579006\ttotal: 1.65s\tremaining: 4.54s\n267:\tlearn: 0.2575818\ttotal: 1.66s\tremaining: 4.53s\n268:\tlearn: 0.2571406\ttotal: 1.66s\tremaining: 4.52s\n269:\tlearn: 0.2564734\ttotal: 1.67s\tremaining: 4.52s\n270:\tlearn: 0.2561351\ttotal: 1.68s\tremaining: 4.51s\n271:\tlearn: 0.2557651\ttotal: 1.68s\tremaining: 4.5s\n272:\tlearn: 0.2550451\ttotal: 1.69s\tremaining: 4.49s\n273:\tlearn: 0.2547322\ttotal: 1.69s\tremaining: 4.49s\n274:\tlearn: 0.2544178\ttotal: 1.7s\tremaining: 4.48s\n275:\tlearn: 0.2540740\ttotal: 1.7s\tremaining: 4.47s\n276:\tlearn: 0.2536185\ttotal: 1.71s\tremaining: 4.46s\n277:\tlearn: 0.2532495\ttotal: 1.72s\tremaining: 4.46s\n278:\tlearn: 0.2527230\ttotal: 1.72s\tremaining: 4.45s\n279:\tlearn: 0.2524836\ttotal: 1.73s\tremaining: 4.44s\n280:\tlearn: 0.2520502\ttotal: 1.73s\tremaining: 4.43s\n281:\tlearn: 0.2514759\ttotal: 1.74s\tremaining: 4.43s\n282:\tlearn: 0.2506771\ttotal: 1.75s\tremaining: 4.42s\n283:\tlearn: 0.2503006\ttotal: 1.75s\tremaining: 4.41s\n284:\tlearn: 0.2497516\ttotal: 1.76s\tremaining: 4.41s\n285:\tlearn: 0.2491702\ttotal: 1.76s\tremaining: 4.4s\n286:\tlearn: 0.2485611\ttotal: 1.77s\tremaining: 4.39s\n287:\tlearn: 0.2482227\ttotal: 1.77s\tremaining: 4.39s\n288:\tlearn: 0.2476860\ttotal: 1.78s\tremaining: 4.38s\n289:\tlearn: 0.2472753\ttotal: 1.79s\tremaining: 4.38s\n290:\tlearn: 0.2467720\ttotal: 1.79s\tremaining: 4.37s\n291:\tlearn: 0.2463715\ttotal: 1.8s\tremaining: 4.37s\n292:\tlearn: 0.2458376\ttotal: 1.81s\tremaining: 4.36s\n293:\tlearn: 0.2453602\ttotal: 1.81s\tremaining: 4.36s\n294:\tlearn: 0.2450699\ttotal: 1.82s\tremaining: 4.35s\n295:\tlearn: 0.2444321\ttotal: 1.83s\tremaining: 4.35s\n296:\tlearn: 0.2438715\ttotal: 1.83s\tremaining: 4.34s\n297:\tlearn: 0.2435815\ttotal: 1.84s\tremaining: 4.33s\n298:\tlearn: 0.2432931\ttotal: 1.84s\tremaining: 4.33s\n299:\tlearn: 0.2429626\ttotal: 1.85s\tremaining: 4.32s\n300:\tlearn: 0.2425900\ttotal: 1.86s\tremaining: 4.31s\n301:\tlearn: 0.2421646\ttotal: 1.86s\tremaining: 4.3s\n302:\tlearn: 0.2417106\ttotal: 1.87s\tremaining: 4.3s\n303:\tlearn: 0.2413763\ttotal: 1.87s\tremaining: 4.29s\n304:\tlearn: 0.2408944\ttotal: 1.88s\tremaining: 4.28s\n305:\tlearn: 0.2403540\ttotal: 1.89s\tremaining: 4.28s\n306:\tlearn: 0.2398777\ttotal: 1.89s\tremaining: 4.27s\n307:\tlearn: 0.2395968\ttotal: 1.9s\tremaining: 4.26s\n308:\tlearn: 0.2391986\ttotal: 1.9s\tremaining: 4.25s\n309:\tlearn: 0.2388132\ttotal: 1.91s\tremaining: 4.25s\n310:\tlearn: 0.2384443\ttotal: 1.91s\tremaining: 4.24s\n311:\tlearn: 0.2380030\ttotal: 1.92s\tremaining: 4.24s\n312:\tlearn: 0.2375537\ttotal: 1.93s\tremaining: 4.23s\n313:\tlearn: 0.2370049\ttotal: 1.93s\tremaining: 4.22s\n314:\tlearn: 0.2363338\ttotal: 1.94s\tremaining: 4.21s\n315:\tlearn: 0.2359241\ttotal: 1.94s\tremaining: 4.21s\n316:\tlearn: 0.2354973\ttotal: 1.95s\tremaining: 4.2s\n317:\tlearn: 0.2352425\ttotal: 1.96s\tremaining: 4.2s\n318:\tlearn: 0.2348798\ttotal: 1.96s\tremaining: 4.19s\n319:\tlearn: 0.2342299\ttotal: 1.97s\tremaining: 4.18s\n320:\tlearn: 0.2337972\ttotal: 1.97s\tremaining: 4.18s\n321:\tlearn: 0.2334677\ttotal: 1.98s\tremaining: 4.17s\n322:\tlearn: 0.2329728\ttotal: 1.99s\tremaining: 4.16s\n323:\tlearn: 0.2326792\ttotal: 1.99s\tremaining: 4.16s\n324:\tlearn: 0.2324202\ttotal: 2s\tremaining: 4.15s\n325:\tlearn: 0.2319798\ttotal: 2s\tremaining: 4.14s\n326:\tlearn: 0.2317128\ttotal: 2.01s\tremaining: 4.13s\n327:\tlearn: 0.2312133\ttotal: 2.01s\tremaining: 4.13s\n328:\tlearn: 0.2307024\ttotal: 2.02s\tremaining: 4.12s\n329:\tlearn: 0.2304628\ttotal: 2.03s\tremaining: 4.11s\n330:\tlearn: 0.2301328\ttotal: 2.03s\tremaining: 4.11s\n331:\tlearn: 0.2296334\ttotal: 2.04s\tremaining: 4.1s\n332:\tlearn: 0.2289786\ttotal: 2.04s\tremaining: 4.09s\n333:\tlearn: 0.2281334\ttotal: 2.05s\tremaining: 4.09s\n334:\tlearn: 0.2278596\ttotal: 2.06s\tremaining: 4.08s\n335:\tlearn: 0.2275971\ttotal: 2.06s\tremaining: 4.07s\n336:\tlearn: 0.2272396\ttotal: 2.07s\tremaining: 4.07s\n337:\tlearn: 0.2268698\ttotal: 2.07s\tremaining: 4.06s\n338:\tlearn: 0.2265183\ttotal: 2.08s\tremaining: 4.05s\n339:\tlearn: 0.2261779\ttotal: 2.08s\tremaining: 4.04s\n340:\tlearn: 0.2256242\ttotal: 2.09s\tremaining: 4.04s\n341:\tlearn: 0.2253261\ttotal: 2.1s\tremaining: 4.03s\n342:\tlearn: 0.2248872\ttotal: 2.1s\tremaining: 4.03s\n343:\tlearn: 0.2244514\ttotal: 2.11s\tremaining: 4.02s\n344:\tlearn: 0.2241236\ttotal: 2.11s\tremaining: 4.01s\n345:\tlearn: 0.2236367\ttotal: 2.12s\tremaining: 4s\n346:\tlearn: 0.2232235\ttotal: 2.13s\tremaining: 4s\n347:\tlearn: 0.2227585\ttotal: 2.13s\tremaining: 3.99s\n348:\tlearn: 0.2223444\ttotal: 2.14s\tremaining: 3.99s\n349:\tlearn: 0.2216630\ttotal: 2.14s\tremaining: 3.98s\n350:\tlearn: 0.2210997\ttotal: 2.15s\tremaining: 3.97s\n351:\tlearn: 0.2206364\ttotal: 2.15s\tremaining: 3.97s\n352:\tlearn: 0.2202836\ttotal: 2.16s\tremaining: 3.96s\n353:\tlearn: 0.2198732\ttotal: 2.17s\tremaining: 3.95s\n354:\tlearn: 0.2193612\ttotal: 2.17s\tremaining: 3.95s\n355:\tlearn: 0.2191000\ttotal: 2.18s\tremaining: 3.94s\n356:\tlearn: 0.2185494\ttotal: 2.18s\tremaining: 3.94s\n357:\tlearn: 0.2181501\ttotal: 2.19s\tremaining: 3.93s\n358:\tlearn: 0.2175247\ttotal: 2.2s\tremaining: 3.92s\n359:\tlearn: 0.2169684\ttotal: 2.2s\tremaining: 3.92s\n360:\tlearn: 0.2165363\ttotal: 2.21s\tremaining: 3.91s\n361:\tlearn: 0.2162131\ttotal: 2.21s\tremaining: 3.9s\n362:\tlearn: 0.2159727\ttotal: 2.22s\tremaining: 3.9s\n363:\tlearn: 0.2157349\ttotal: 2.23s\tremaining: 3.89s\n364:\tlearn: 0.2153624\ttotal: 2.23s\tremaining: 3.88s\n365:\tlearn: 0.2149229\ttotal: 2.24s\tremaining: 3.88s\n366:\tlearn: 0.2145355\ttotal: 2.24s\tremaining: 3.87s\n367:\tlearn: 0.2140158\ttotal: 2.25s\tremaining: 3.86s\n368:\tlearn: 0.2136854\ttotal: 2.26s\tremaining: 3.86s\n369:\tlearn: 0.2133445\ttotal: 2.26s\tremaining: 3.85s\n370:\tlearn: 0.2130551\ttotal: 2.27s\tremaining: 3.84s\n371:\tlearn: 0.2128221\ttotal: 2.27s\tremaining: 3.84s\n372:\tlearn: 0.2125015\ttotal: 2.28s\tremaining: 3.83s\n373:\tlearn: 0.2121491\ttotal: 2.28s\tremaining: 3.82s\n374:\tlearn: 0.2117439\ttotal: 2.29s\tremaining: 3.82s\n375:\tlearn: 0.2112904\ttotal: 2.3s\tremaining: 3.81s\n376:\tlearn: 0.2107699\ttotal: 2.3s\tremaining: 3.81s\n377:\tlearn: 0.2100380\ttotal: 2.31s\tremaining: 3.8s\n378:\tlearn: 0.2097452\ttotal: 2.31s\tremaining: 3.79s\n379:\tlearn: 0.2093704\ttotal: 2.32s\tremaining: 3.79s\n380:\tlearn: 0.2089286\ttotal: 2.33s\tremaining: 3.78s\n381:\tlearn: 0.2086264\ttotal: 2.33s\tremaining: 3.77s\n382:\tlearn: 0.2083546\ttotal: 2.34s\tremaining: 3.77s\n383:\tlearn: 0.2080846\ttotal: 2.34s\tremaining: 3.76s\n384:\tlearn: 0.2077707\ttotal: 2.35s\tremaining: 3.75s\n385:\tlearn: 0.2073093\ttotal: 2.35s\tremaining: 3.75s\n386:\tlearn: 0.2068147\ttotal: 2.36s\tremaining: 3.74s\n387:\tlearn: 0.2065303\ttotal: 2.37s\tremaining: 3.73s\n388:\tlearn: 0.2061651\ttotal: 2.37s\tremaining: 3.73s\n389:\tlearn: 0.2058038\ttotal: 2.38s\tremaining: 3.72s\n390:\tlearn: 0.2055200\ttotal: 2.38s\tremaining: 3.71s\n391:\tlearn: 0.2050486\ttotal: 2.39s\tremaining: 3.71s\n392:\tlearn: 0.2046745\ttotal: 2.4s\tremaining: 3.7s\n393:\tlearn: 0.2044232\ttotal: 2.4s\tremaining: 3.7s\n394:\tlearn: 0.2041171\ttotal: 2.41s\tremaining: 3.69s\n395:\tlearn: 0.2036867\ttotal: 2.42s\tremaining: 3.69s\n396:\tlearn: 0.2034398\ttotal: 2.42s\tremaining: 3.68s\n397:\tlearn: 0.2028952\ttotal: 2.43s\tremaining: 3.67s\n398:\tlearn: 0.2025658\ttotal: 2.44s\tremaining: 3.67s\n399:\tlearn: 0.2021153\ttotal: 2.44s\tremaining: 3.66s\n400:\tlearn: 0.2016568\ttotal: 2.45s\tremaining: 3.66s\n401:\tlearn: 0.2012604\ttotal: 2.46s\tremaining: 3.65s\n402:\tlearn: 0.2010576\ttotal: 2.46s\tremaining: 3.65s\n403:\tlearn: 0.2005648\ttotal: 2.47s\tremaining: 3.64s\n404:\tlearn: 0.2001568\ttotal: 2.48s\tremaining: 3.64s\n405:\tlearn: 0.1997065\ttotal: 2.48s\tremaining: 3.63s\n406:\tlearn: 0.1993148\ttotal: 2.49s\tremaining: 3.63s\n407:\tlearn: 0.1990108\ttotal: 2.49s\tremaining: 3.62s\n408:\tlearn: 0.1985072\ttotal: 2.5s\tremaining: 3.61s\n409:\tlearn: 0.1980146\ttotal: 2.51s\tremaining: 3.61s\n410:\tlearn: 0.1977787\ttotal: 2.51s\tremaining: 3.6s\n411:\tlearn: 0.1974042\ttotal: 2.52s\tremaining: 3.6s\n412:\tlearn: 0.1969256\ttotal: 2.53s\tremaining: 3.59s\n413:\tlearn: 0.1966383\ttotal: 2.53s\tremaining: 3.58s\n414:\tlearn: 0.1962264\ttotal: 2.54s\tremaining: 3.58s\n415:\tlearn: 0.1955694\ttotal: 2.55s\tremaining: 3.57s\n416:\tlearn: 0.1952085\ttotal: 2.55s\tremaining: 3.57s\n417:\tlearn: 0.1947731\ttotal: 2.56s\tremaining: 3.56s\n418:\tlearn: 0.1942846\ttotal: 2.56s\tremaining: 3.56s\n419:\tlearn: 0.1937189\ttotal: 2.57s\tremaining: 3.55s\n420:\tlearn: 0.1933190\ttotal: 2.58s\tremaining: 3.55s\n421:\tlearn: 0.1929202\ttotal: 2.58s\tremaining: 3.54s\n422:\tlearn: 0.1924916\ttotal: 2.59s\tremaining: 3.54s\n423:\tlearn: 0.1922732\ttotal: 2.6s\tremaining: 3.53s\n424:\tlearn: 0.1919104\ttotal: 2.6s\tremaining: 3.52s\n425:\tlearn: 0.1916633\ttotal: 2.61s\tremaining: 3.52s\n426:\tlearn: 0.1913761\ttotal: 2.62s\tremaining: 3.51s\n427:\tlearn: 0.1908297\ttotal: 2.62s\tremaining: 3.5s\n428:\tlearn: 0.1906100\ttotal: 2.63s\tremaining: 3.5s\n429:\tlearn: 0.1901761\ttotal: 2.63s\tremaining: 3.49s\n430:\tlearn: 0.1899410\ttotal: 2.64s\tremaining: 3.48s\n431:\tlearn: 0.1897535\ttotal: 2.64s\tremaining: 3.48s\n432:\tlearn: 0.1893986\ttotal: 2.65s\tremaining: 3.47s\n433:\tlearn: 0.1891348\ttotal: 2.66s\tremaining: 3.46s\n434:\tlearn: 0.1887521\ttotal: 2.66s\tremaining: 3.46s\n435:\tlearn: 0.1882769\ttotal: 2.67s\tremaining: 3.45s\n436:\tlearn: 0.1879890\ttotal: 2.67s\tremaining: 3.44s\n437:\tlearn: 0.1875250\ttotal: 2.68s\tremaining: 3.44s\n438:\tlearn: 0.1872849\ttotal: 2.69s\tremaining: 3.43s\n439:\tlearn: 0.1869666\ttotal: 2.69s\tremaining: 3.42s\n440:\tlearn: 0.1866917\ttotal: 2.7s\tremaining: 3.42s\n441:\tlearn: 0.1863802\ttotal: 2.7s\tremaining: 3.41s\n442:\tlearn: 0.1860884\ttotal: 2.71s\tremaining: 3.41s\n443:\tlearn: 0.1857935\ttotal: 2.71s\tremaining: 3.4s\n444:\tlearn: 0.1853680\ttotal: 2.72s\tremaining: 3.39s\n445:\tlearn: 0.1850699\ttotal: 2.73s\tremaining: 3.39s\n446:\tlearn: 0.1847901\ttotal: 2.73s\tremaining: 3.38s\n447:\tlearn: 0.1846162\ttotal: 2.74s\tremaining: 3.37s\n448:\tlearn: 0.1844013\ttotal: 2.74s\tremaining: 3.37s\n449:\tlearn: 0.1840315\ttotal: 2.75s\tremaining: 3.36s\n450:\tlearn: 0.1838431\ttotal: 2.75s\tremaining: 3.35s\n451:\tlearn: 0.1836420\ttotal: 2.76s\tremaining: 3.35s\n452:\tlearn: 0.1833382\ttotal: 2.77s\tremaining: 3.34s\n453:\tlearn: 0.1827866\ttotal: 2.77s\tremaining: 3.34s\n454:\tlearn: 0.1825309\ttotal: 2.78s\tremaining: 3.33s\n455:\tlearn: 0.1821210\ttotal: 2.79s\tremaining: 3.33s\n456:\tlearn: 0.1815822\ttotal: 2.79s\tremaining: 3.32s\n457:\tlearn: 0.1813320\ttotal: 2.8s\tremaining: 3.31s\n458:\tlearn: 0.1810897\ttotal: 2.81s\tremaining: 3.31s\n459:\tlearn: 0.1807177\ttotal: 2.81s\tremaining: 3.3s\n460:\tlearn: 0.1802656\ttotal: 2.82s\tremaining: 3.3s\n461:\tlearn: 0.1800748\ttotal: 2.83s\tremaining: 3.29s\n462:\tlearn: 0.1798850\ttotal: 2.83s\tremaining: 3.28s\n463:\tlearn: 0.1797049\ttotal: 2.84s\tremaining: 3.28s\n464:\tlearn: 0.1793586\ttotal: 2.84s\tremaining: 3.27s\n465:\tlearn: 0.1789453\ttotal: 2.85s\tremaining: 3.26s\n466:\tlearn: 0.1785777\ttotal: 2.85s\tremaining: 3.26s\n467:\tlearn: 0.1782453\ttotal: 2.86s\tremaining: 3.25s\n468:\tlearn: 0.1779761\ttotal: 2.87s\tremaining: 3.25s\n469:\tlearn: 0.1776983\ttotal: 2.87s\tremaining: 3.24s\n470:\tlearn: 0.1773838\ttotal: 2.88s\tremaining: 3.23s\n471:\tlearn: 0.1771262\ttotal: 2.88s\tremaining: 3.23s\n472:\tlearn: 0.1768892\ttotal: 2.89s\tremaining: 3.22s\n473:\tlearn: 0.1765831\ttotal: 2.9s\tremaining: 3.21s\n474:\tlearn: 0.1764915\ttotal: 2.9s\tremaining: 3.21s\n475:\tlearn: 0.1761996\ttotal: 2.91s\tremaining: 3.2s\n476:\tlearn: 0.1758620\ttotal: 2.91s\tremaining: 3.19s\n477:\tlearn: 0.1754779\ttotal: 2.92s\tremaining: 3.19s\n478:\tlearn: 0.1752503\ttotal: 2.92s\tremaining: 3.18s\n479:\tlearn: 0.1748005\ttotal: 2.93s\tremaining: 3.17s\n480:\tlearn: 0.1745288\ttotal: 2.94s\tremaining: 3.17s\n481:\tlearn: 0.1741521\ttotal: 2.94s\tremaining: 3.16s\n482:\tlearn: 0.1738805\ttotal: 2.95s\tremaining: 3.16s\n483:\tlearn: 0.1735335\ttotal: 2.96s\tremaining: 3.15s\n484:\tlearn: 0.1732418\ttotal: 2.96s\tremaining: 3.14s\n485:\tlearn: 0.1729865\ttotal: 2.97s\tremaining: 3.14s\n486:\tlearn: 0.1727113\ttotal: 2.97s\tremaining: 3.13s\n487:\tlearn: 0.1725133\ttotal: 2.98s\tremaining: 3.12s\n488:\tlearn: 0.1722833\ttotal: 2.98s\tremaining: 3.12s\n489:\tlearn: 0.1719834\ttotal: 2.99s\tremaining: 3.11s\n490:\tlearn: 0.1717092\ttotal: 3s\tremaining: 3.1s\n491:\tlearn: 0.1715037\ttotal: 3s\tremaining: 3.1s\n492:\tlearn: 0.1710896\ttotal: 3.01s\tremaining: 3.09s\n493:\tlearn: 0.1708940\ttotal: 3.01s\tremaining: 3.09s\n494:\tlearn: 0.1705728\ttotal: 3.02s\tremaining: 3.08s\n495:\tlearn: 0.1703901\ttotal: 3.02s\tremaining: 3.07s\n496:\tlearn: 0.1701781\ttotal: 3.03s\tremaining: 3.07s\n497:\tlearn: 0.1700114\ttotal: 3.04s\tremaining: 3.06s\n498:\tlearn: 0.1695561\ttotal: 3.04s\tremaining: 3.06s\n499:\tlearn: 0.1693664\ttotal: 3.05s\tremaining: 3.05s\n500:\tlearn: 0.1691711\ttotal: 3.05s\tremaining: 3.04s\n501:\tlearn: 0.1687531\ttotal: 3.06s\tremaining: 3.04s\n502:\tlearn: 0.1685273\ttotal: 3.06s\tremaining: 3.03s\n503:\tlearn: 0.1682885\ttotal: 3.07s\tremaining: 3.02s\n504:\tlearn: 0.1679680\ttotal: 3.08s\tremaining: 3.02s\n505:\tlearn: 0.1677206\ttotal: 3.08s\tremaining: 3.01s\n506:\tlearn: 0.1673793\ttotal: 3.09s\tremaining: 3s\n507:\tlearn: 0.1672174\ttotal: 3.09s\tremaining: 3s\n508:\tlearn: 0.1670078\ttotal: 3.1s\tremaining: 2.99s\n509:\tlearn: 0.1666842\ttotal: 3.11s\tremaining: 2.98s\n510:\tlearn: 0.1664967\ttotal: 3.11s\tremaining: 2.98s\n511:\tlearn: 0.1661892\ttotal: 3.12s\tremaining: 2.97s\n512:\tlearn: 0.1660046\ttotal: 3.13s\tremaining: 2.97s\n513:\tlearn: 0.1657077\ttotal: 3.13s\tremaining: 2.96s\n514:\tlearn: 0.1654257\ttotal: 3.14s\tremaining: 2.95s\n515:\tlearn: 0.1651985\ttotal: 3.14s\tremaining: 2.95s\n516:\tlearn: 0.1650427\ttotal: 3.15s\tremaining: 2.94s\n517:\tlearn: 0.1648611\ttotal: 3.15s\tremaining: 2.94s\n518:\tlearn: 0.1645618\ttotal: 3.16s\tremaining: 2.93s\n519:\tlearn: 0.1643813\ttotal: 3.17s\tremaining: 2.92s\n520:\tlearn: 0.1640422\ttotal: 3.17s\tremaining: 2.92s\n521:\tlearn: 0.1638363\ttotal: 3.18s\tremaining: 2.91s\n522:\tlearn: 0.1635342\ttotal: 3.19s\tremaining: 2.9s\n523:\tlearn: 0.1633723\ttotal: 3.19s\tremaining: 2.9s\n524:\tlearn: 0.1630793\ttotal: 3.2s\tremaining: 2.89s\n525:\tlearn: 0.1628528\ttotal: 3.2s\tremaining: 2.89s\n526:\tlearn: 0.1626477\ttotal: 3.21s\tremaining: 2.88s\n527:\tlearn: 0.1624496\ttotal: 3.22s\tremaining: 2.88s\n528:\tlearn: 0.1621564\ttotal: 3.22s\tremaining: 2.87s\n529:\tlearn: 0.1618364\ttotal: 3.23s\tremaining: 2.86s\n530:\tlearn: 0.1615734\ttotal: 3.24s\tremaining: 2.86s\n531:\tlearn: 0.1613594\ttotal: 3.24s\tremaining: 2.85s\n532:\tlearn: 0.1610400\ttotal: 3.25s\tremaining: 2.85s\n533:\tlearn: 0.1608585\ttotal: 3.25s\tremaining: 2.84s\n534:\tlearn: 0.1606658\ttotal: 3.26s\tremaining: 2.83s\n535:\tlearn: 0.1603893\ttotal: 3.27s\tremaining: 2.83s\n536:\tlearn: 0.1602185\ttotal: 3.27s\tremaining: 2.82s\n537:\tlearn: 0.1599835\ttotal: 3.28s\tremaining: 2.81s\n538:\tlearn: 0.1596484\ttotal: 3.28s\tremaining: 2.81s\n539:\tlearn: 0.1592498\ttotal: 3.29s\tremaining: 2.8s\n540:\tlearn: 0.1590179\ttotal: 3.29s\tremaining: 2.79s\n541:\tlearn: 0.1588298\ttotal: 3.3s\tremaining: 2.79s\n542:\tlearn: 0.1586360\ttotal: 3.31s\tremaining: 2.78s\n543:\tlearn: 0.1584481\ttotal: 3.31s\tremaining: 2.78s\n544:\tlearn: 0.1582415\ttotal: 3.32s\tremaining: 2.77s\n545:\tlearn: 0.1579575\ttotal: 3.32s\tremaining: 2.76s\n546:\tlearn: 0.1577103\ttotal: 3.33s\tremaining: 2.76s\n547:\tlearn: 0.1574966\ttotal: 3.33s\tremaining: 2.75s\n548:\tlearn: 0.1572925\ttotal: 3.34s\tremaining: 2.74s\n549:\tlearn: 0.1570546\ttotal: 3.35s\tremaining: 2.74s\n550:\tlearn: 0.1566920\ttotal: 3.35s\tremaining: 2.73s\n551:\tlearn: 0.1565542\ttotal: 3.36s\tremaining: 2.73s\n552:\tlearn: 0.1562139\ttotal: 3.37s\tremaining: 2.72s\n553:\tlearn: 0.1559360\ttotal: 3.37s\tremaining: 2.71s\n554:\tlearn: 0.1556627\ttotal: 3.38s\tremaining: 2.71s\n555:\tlearn: 0.1553163\ttotal: 3.38s\tremaining: 2.7s\n556:\tlearn: 0.1551136\ttotal: 3.39s\tremaining: 2.69s\n557:\tlearn: 0.1546408\ttotal: 3.4s\tremaining: 2.69s\n558:\tlearn: 0.1544266\ttotal: 3.4s\tremaining: 2.68s\n559:\tlearn: 0.1540121\ttotal: 3.41s\tremaining: 2.68s\n560:\tlearn: 0.1538575\ttotal: 3.41s\tremaining: 2.67s\n561:\tlearn: 0.1536653\ttotal: 3.42s\tremaining: 2.66s\n562:\tlearn: 0.1531937\ttotal: 3.42s\tremaining: 2.66s\n563:\tlearn: 0.1530222\ttotal: 3.43s\tremaining: 2.65s\n564:\tlearn: 0.1526681\ttotal: 3.44s\tremaining: 2.65s\n565:\tlearn: 0.1522966\ttotal: 3.44s\tremaining: 2.64s\n566:\tlearn: 0.1520808\ttotal: 3.45s\tremaining: 2.63s\n567:\tlearn: 0.1518955\ttotal: 3.45s\tremaining: 2.63s\n568:\tlearn: 0.1517073\ttotal: 3.46s\tremaining: 2.62s\n569:\tlearn: 0.1515500\ttotal: 3.46s\tremaining: 2.61s\n570:\tlearn: 0.1512833\ttotal: 3.47s\tremaining: 2.61s\n571:\tlearn: 0.1508183\ttotal: 3.48s\tremaining: 2.6s\n572:\tlearn: 0.1505652\ttotal: 3.48s\tremaining: 2.59s\n573:\tlearn: 0.1503204\ttotal: 3.49s\tremaining: 2.59s\n574:\tlearn: 0.1498780\ttotal: 3.49s\tremaining: 2.58s\n575:\tlearn: 0.1496460\ttotal: 3.5s\tremaining: 2.58s\n576:\tlearn: 0.1494496\ttotal: 3.5s\tremaining: 2.57s\n577:\tlearn: 0.1493260\ttotal: 3.51s\tremaining: 2.56s\n578:\tlearn: 0.1490220\ttotal: 3.52s\tremaining: 2.56s\n579:\tlearn: 0.1487793\ttotal: 3.52s\tremaining: 2.55s\n580:\tlearn: 0.1485878\ttotal: 3.53s\tremaining: 2.54s\n581:\tlearn: 0.1481532\ttotal: 3.53s\tremaining: 2.54s\n582:\tlearn: 0.1478165\ttotal: 3.54s\tremaining: 2.53s\n583:\tlearn: 0.1476576\ttotal: 3.54s\tremaining: 2.52s\n584:\tlearn: 0.1475024\ttotal: 3.55s\tremaining: 2.52s\n585:\tlearn: 0.1471972\ttotal: 3.56s\tremaining: 2.51s\n586:\tlearn: 0.1469023\ttotal: 3.56s\tremaining: 2.51s\n587:\tlearn: 0.1467427\ttotal: 3.57s\tremaining: 2.5s\n588:\tlearn: 0.1464342\ttotal: 3.58s\tremaining: 2.49s\n589:\tlearn: 0.1461691\ttotal: 3.58s\tremaining: 2.49s\n590:\tlearn: 0.1457571\ttotal: 3.59s\tremaining: 2.48s\n591:\tlearn: 0.1455330\ttotal: 3.59s\tremaining: 2.48s\n592:\tlearn: 0.1453104\ttotal: 3.6s\tremaining: 2.47s\n593:\tlearn: 0.1451226\ttotal: 3.61s\tremaining: 2.46s\n594:\tlearn: 0.1449471\ttotal: 3.61s\tremaining: 2.46s\n595:\tlearn: 0.1447780\ttotal: 3.62s\tremaining: 2.45s\n596:\tlearn: 0.1445938\ttotal: 3.62s\tremaining: 2.45s\n597:\tlearn: 0.1445168\ttotal: 3.63s\tremaining: 2.44s\n598:\tlearn: 0.1444068\ttotal: 3.63s\tremaining: 2.43s\n599:\tlearn: 0.1441224\ttotal: 3.64s\tremaining: 2.43s\n600:\tlearn: 0.1439672\ttotal: 3.65s\tremaining: 2.42s\n601:\tlearn: 0.1437617\ttotal: 3.65s\tremaining: 2.41s\n602:\tlearn: 0.1435024\ttotal: 3.66s\tremaining: 2.41s\n603:\tlearn: 0.1432433\ttotal: 3.66s\tremaining: 2.4s\n604:\tlearn: 0.1430920\ttotal: 3.67s\tremaining: 2.4s\n605:\tlearn: 0.1429715\ttotal: 3.67s\tremaining: 2.39s\n606:\tlearn: 0.1427775\ttotal: 3.68s\tremaining: 2.38s\n607:\tlearn: 0.1425216\ttotal: 3.69s\tremaining: 2.38s\n608:\tlearn: 0.1424276\ttotal: 3.69s\tremaining: 2.37s\n609:\tlearn: 0.1422905\ttotal: 3.7s\tremaining: 2.36s\n610:\tlearn: 0.1421111\ttotal: 3.7s\tremaining: 2.36s\n611:\tlearn: 0.1418316\ttotal: 3.71s\tremaining: 2.35s\n612:\tlearn: 0.1415831\ttotal: 3.71s\tremaining: 2.34s\n613:\tlearn: 0.1413381\ttotal: 3.72s\tremaining: 2.34s\n614:\tlearn: 0.1412182\ttotal: 3.73s\tremaining: 2.33s\n615:\tlearn: 0.1408913\ttotal: 3.73s\tremaining: 2.33s\n616:\tlearn: 0.1406739\ttotal: 3.74s\tremaining: 2.32s\n617:\tlearn: 0.1405327\ttotal: 3.74s\tremaining: 2.31s\n618:\tlearn: 0.1403330\ttotal: 3.75s\tremaining: 2.31s\n619:\tlearn: 0.1401096\ttotal: 3.75s\tremaining: 2.3s\n620:\tlearn: 0.1399204\ttotal: 3.76s\tremaining: 2.29s\n621:\tlearn: 0.1396435\ttotal: 3.76s\tremaining: 2.29s\n622:\tlearn: 0.1392927\ttotal: 3.77s\tremaining: 2.28s\n623:\tlearn: 0.1392189\ttotal: 3.78s\tremaining: 2.27s\n624:\tlearn: 0.1390910\ttotal: 3.78s\tremaining: 2.27s\n625:\tlearn: 0.1388518\ttotal: 3.79s\tremaining: 2.26s\n626:\tlearn: 0.1387129\ttotal: 3.79s\tremaining: 2.26s\n627:\tlearn: 0.1385453\ttotal: 3.8s\tremaining: 2.25s\n628:\tlearn: 0.1383772\ttotal: 3.81s\tremaining: 2.25s\n629:\tlearn: 0.1381560\ttotal: 3.81s\tremaining: 2.24s\n630:\tlearn: 0.1379178\ttotal: 3.82s\tremaining: 2.23s\n631:\tlearn: 0.1375693\ttotal: 3.82s\tremaining: 2.23s\n632:\tlearn: 0.1373950\ttotal: 3.83s\tremaining: 2.22s\n633:\tlearn: 0.1371386\ttotal: 3.84s\tremaining: 2.21s\n634:\tlearn: 0.1369030\ttotal: 3.84s\tremaining: 2.21s\n635:\tlearn: 0.1367359\ttotal: 3.85s\tremaining: 2.2s\n636:\tlearn: 0.1365734\ttotal: 3.85s\tremaining: 2.19s\n637:\tlearn: 0.1363348\ttotal: 3.86s\tremaining: 2.19s\n638:\tlearn: 0.1362225\ttotal: 3.86s\tremaining: 2.18s\n639:\tlearn: 0.1361103\ttotal: 3.87s\tremaining: 2.18s\n640:\tlearn: 0.1359259\ttotal: 3.88s\tremaining: 2.17s\n641:\tlearn: 0.1358163\ttotal: 3.88s\tremaining: 2.16s\n642:\tlearn: 0.1356088\ttotal: 3.89s\tremaining: 2.16s\n643:\tlearn: 0.1354751\ttotal: 3.89s\tremaining: 2.15s\n644:\tlearn: 0.1353625\ttotal: 3.9s\tremaining: 2.15s\n645:\tlearn: 0.1352311\ttotal: 3.9s\tremaining: 2.14s\n646:\tlearn: 0.1351083\ttotal: 3.91s\tremaining: 2.13s\n647:\tlearn: 0.1349289\ttotal: 3.92s\tremaining: 2.13s\n648:\tlearn: 0.1347752\ttotal: 3.92s\tremaining: 2.12s\n649:\tlearn: 0.1345498\ttotal: 3.93s\tremaining: 2.11s\n650:\tlearn: 0.1344398\ttotal: 3.93s\tremaining: 2.11s\n651:\tlearn: 0.1342620\ttotal: 3.94s\tremaining: 2.1s\n652:\tlearn: 0.1340524\ttotal: 3.94s\tremaining: 2.1s\n653:\tlearn: 0.1338608\ttotal: 3.95s\tremaining: 2.09s\n654:\tlearn: 0.1337625\ttotal: 3.96s\tremaining: 2.08s\n655:\tlearn: 0.1335769\ttotal: 3.96s\tremaining: 2.08s\n656:\tlearn: 0.1332959\ttotal: 3.97s\tremaining: 2.07s\n657:\tlearn: 0.1331146\ttotal: 3.97s\tremaining: 2.06s\n658:\tlearn: 0.1328833\ttotal: 3.98s\tremaining: 2.06s\n659:\tlearn: 0.1325575\ttotal: 3.98s\tremaining: 2.05s\n660:\tlearn: 0.1323868\ttotal: 3.99s\tremaining: 2.05s\n661:\tlearn: 0.1320708\ttotal: 4s\tremaining: 2.04s\n662:\tlearn: 0.1319641\ttotal: 4s\tremaining: 2.03s\n663:\tlearn: 0.1317407\ttotal: 4.01s\tremaining: 2.03s\n664:\tlearn: 0.1314997\ttotal: 4.01s\tremaining: 2.02s\n665:\tlearn: 0.1313338\ttotal: 4.02s\tremaining: 2.02s\n666:\tlearn: 0.1311970\ttotal: 4.03s\tremaining: 2.01s\n667:\tlearn: 0.1309960\ttotal: 4.03s\tremaining: 2s\n668:\tlearn: 0.1307873\ttotal: 4.04s\tremaining: 2s\n669:\tlearn: 0.1306462\ttotal: 4.04s\tremaining: 1.99s\n670:\tlearn: 0.1304848\ttotal: 4.05s\tremaining: 1.98s\n671:\tlearn: 0.1302901\ttotal: 4.05s\tremaining: 1.98s\n672:\tlearn: 0.1300708\ttotal: 4.06s\tremaining: 1.97s\n673:\tlearn: 0.1298053\ttotal: 4.07s\tremaining: 1.97s\n674:\tlearn: 0.1295607\ttotal: 4.07s\tremaining: 1.96s\n675:\tlearn: 0.1294510\ttotal: 4.08s\tremaining: 1.95s\n676:\tlearn: 0.1291752\ttotal: 4.08s\tremaining: 1.95s\n677:\tlearn: 0.1289685\ttotal: 4.09s\tremaining: 1.94s\n678:\tlearn: 0.1286734\ttotal: 4.09s\tremaining: 1.94s\n679:\tlearn: 0.1283981\ttotal: 4.1s\tremaining: 1.93s\n680:\tlearn: 0.1281216\ttotal: 4.11s\tremaining: 1.92s\n681:\tlearn: 0.1279006\ttotal: 4.11s\tremaining: 1.92s\n682:\tlearn: 0.1275957\ttotal: 4.12s\tremaining: 1.91s\n683:\tlearn: 0.1273345\ttotal: 4.12s\tremaining: 1.9s\n684:\tlearn: 0.1272565\ttotal: 4.13s\tremaining: 1.9s\n685:\tlearn: 0.1270295\ttotal: 4.13s\tremaining: 1.89s\n686:\tlearn: 0.1268788\ttotal: 4.14s\tremaining: 1.89s\n687:\tlearn: 0.1267315\ttotal: 4.14s\tremaining: 1.88s\n688:\tlearn: 0.1265523\ttotal: 4.15s\tremaining: 1.87s\n689:\tlearn: 0.1263742\ttotal: 4.16s\tremaining: 1.87s\n690:\tlearn: 0.1260504\ttotal: 4.16s\tremaining: 1.86s\n691:\tlearn: 0.1257694\ttotal: 4.17s\tremaining: 1.85s\n692:\tlearn: 0.1255828\ttotal: 4.17s\tremaining: 1.85s\n693:\tlearn: 0.1254269\ttotal: 4.18s\tremaining: 1.84s\n694:\tlearn: 0.1253530\ttotal: 4.18s\tremaining: 1.84s\n695:\tlearn: 0.1252669\ttotal: 4.19s\tremaining: 1.83s\n696:\tlearn: 0.1250344\ttotal: 4.2s\tremaining: 1.82s\n697:\tlearn: 0.1248545\ttotal: 4.2s\tremaining: 1.82s\n698:\tlearn: 0.1246331\ttotal: 4.21s\tremaining: 1.81s\n699:\tlearn: 0.1244319\ttotal: 4.21s\tremaining: 1.8s\n700:\tlearn: 0.1242776\ttotal: 4.22s\tremaining: 1.8s\n701:\tlearn: 0.1241313\ttotal: 4.22s\tremaining: 1.79s\n702:\tlearn: 0.1240140\ttotal: 4.23s\tremaining: 1.79s\n703:\tlearn: 0.1239015\ttotal: 4.24s\tremaining: 1.78s\n704:\tlearn: 0.1237181\ttotal: 4.24s\tremaining: 1.77s\n705:\tlearn: 0.1234614\ttotal: 4.25s\tremaining: 1.77s\n706:\tlearn: 0.1232978\ttotal: 4.25s\tremaining: 1.76s\n707:\tlearn: 0.1231108\ttotal: 4.26s\tremaining: 1.76s\n708:\tlearn: 0.1229960\ttotal: 4.26s\tremaining: 1.75s\n709:\tlearn: 0.1228597\ttotal: 4.27s\tremaining: 1.74s\n710:\tlearn: 0.1226379\ttotal: 4.28s\tremaining: 1.74s\n711:\tlearn: 0.1225903\ttotal: 4.28s\tremaining: 1.73s\n712:\tlearn: 0.1224560\ttotal: 4.29s\tremaining: 1.73s\n713:\tlearn: 0.1221976\ttotal: 4.29s\tremaining: 1.72s\n714:\tlearn: 0.1218859\ttotal: 4.3s\tremaining: 1.71s\n715:\tlearn: 0.1216605\ttotal: 4.3s\tremaining: 1.71s\n716:\tlearn: 0.1213627\ttotal: 4.31s\tremaining: 1.7s\n717:\tlearn: 0.1211084\ttotal: 4.32s\tremaining: 1.7s\n718:\tlearn: 0.1208178\ttotal: 4.32s\tremaining: 1.69s\n719:\tlearn: 0.1206863\ttotal: 4.33s\tremaining: 1.68s\n720:\tlearn: 0.1204828\ttotal: 4.33s\tremaining: 1.68s\n721:\tlearn: 0.1202502\ttotal: 4.34s\tremaining: 1.67s\n722:\tlearn: 0.1201550\ttotal: 4.35s\tremaining: 1.67s\n723:\tlearn: 0.1198920\ttotal: 4.35s\tremaining: 1.66s\n724:\tlearn: 0.1196545\ttotal: 4.36s\tremaining: 1.65s\n725:\tlearn: 0.1194878\ttotal: 4.36s\tremaining: 1.65s\n726:\tlearn: 0.1193096\ttotal: 4.37s\tremaining: 1.64s\n727:\tlearn: 0.1191695\ttotal: 4.38s\tremaining: 1.63s\n728:\tlearn: 0.1189911\ttotal: 4.38s\tremaining: 1.63s\n729:\tlearn: 0.1187639\ttotal: 4.39s\tremaining: 1.62s\n730:\tlearn: 0.1186514\ttotal: 4.39s\tremaining: 1.62s\n731:\tlearn: 0.1184047\ttotal: 4.4s\tremaining: 1.61s\n732:\tlearn: 0.1182083\ttotal: 4.4s\tremaining: 1.6s\n733:\tlearn: 0.1180091\ttotal: 4.41s\tremaining: 1.6s\n734:\tlearn: 0.1177937\ttotal: 4.42s\tremaining: 1.59s\n735:\tlearn: 0.1176369\ttotal: 4.42s\tremaining: 1.59s\n736:\tlearn: 0.1174485\ttotal: 4.43s\tremaining: 1.58s\n737:\tlearn: 0.1171844\ttotal: 4.43s\tremaining: 1.57s\n738:\tlearn: 0.1170397\ttotal: 4.44s\tremaining: 1.57s\n739:\tlearn: 0.1168760\ttotal: 4.45s\tremaining: 1.56s\n740:\tlearn: 0.1166704\ttotal: 4.45s\tremaining: 1.55s\n741:\tlearn: 0.1163753\ttotal: 4.46s\tremaining: 1.55s\n742:\tlearn: 0.1161795\ttotal: 4.46s\tremaining: 1.54s\n743:\tlearn: 0.1158803\ttotal: 4.47s\tremaining: 1.54s\n744:\tlearn: 0.1156823\ttotal: 4.47s\tremaining: 1.53s\n745:\tlearn: 0.1154744\ttotal: 4.48s\tremaining: 1.52s\n746:\tlearn: 0.1153819\ttotal: 4.49s\tremaining: 1.52s\n747:\tlearn: 0.1151800\ttotal: 4.49s\tremaining: 1.51s\n748:\tlearn: 0.1149593\ttotal: 4.5s\tremaining: 1.51s\n749:\tlearn: 0.1148588\ttotal: 4.5s\tremaining: 1.5s\n750:\tlearn: 0.1147577\ttotal: 4.51s\tremaining: 1.5s\n751:\tlearn: 0.1145811\ttotal: 4.51s\tremaining: 1.49s\n752:\tlearn: 0.1144351\ttotal: 4.52s\tremaining: 1.48s\n753:\tlearn: 0.1141992\ttotal: 4.53s\tremaining: 1.48s\n754:\tlearn: 0.1140231\ttotal: 4.53s\tremaining: 1.47s\n755:\tlearn: 0.1138975\ttotal: 4.54s\tremaining: 1.46s\n756:\tlearn: 0.1137155\ttotal: 4.54s\tremaining: 1.46s\n757:\tlearn: 0.1134974\ttotal: 4.55s\tremaining: 1.45s\n758:\tlearn: 0.1132480\ttotal: 4.56s\tremaining: 1.45s\n759:\tlearn: 0.1130730\ttotal: 4.56s\tremaining: 1.44s\n760:\tlearn: 0.1129775\ttotal: 4.57s\tremaining: 1.43s\n761:\tlearn: 0.1128722\ttotal: 4.57s\tremaining: 1.43s\n762:\tlearn: 0.1127867\ttotal: 4.58s\tremaining: 1.42s\n763:\tlearn: 0.1124765\ttotal: 4.59s\tremaining: 1.42s\n764:\tlearn: 0.1122147\ttotal: 4.59s\tremaining: 1.41s\n765:\tlearn: 0.1119962\ttotal: 4.6s\tremaining: 1.4s\n766:\tlearn: 0.1119027\ttotal: 4.6s\tremaining: 1.4s\n767:\tlearn: 0.1116763\ttotal: 4.61s\tremaining: 1.39s\n768:\tlearn: 0.1115831\ttotal: 4.61s\tremaining: 1.39s\n769:\tlearn: 0.1114138\ttotal: 4.62s\tremaining: 1.38s\n770:\tlearn: 0.1112337\ttotal: 4.63s\tremaining: 1.37s\n771:\tlearn: 0.1110265\ttotal: 4.63s\tremaining: 1.37s\n772:\tlearn: 0.1109205\ttotal: 4.64s\tremaining: 1.36s\n773:\tlearn: 0.1108457\ttotal: 4.64s\tremaining: 1.35s\n774:\tlearn: 0.1106433\ttotal: 4.65s\tremaining: 1.35s\n775:\tlearn: 0.1105453\ttotal: 4.66s\tremaining: 1.34s\n776:\tlearn: 0.1103394\ttotal: 4.66s\tremaining: 1.34s\n777:\tlearn: 0.1102169\ttotal: 4.67s\tremaining: 1.33s\n778:\tlearn: 0.1100647\ttotal: 4.67s\tremaining: 1.32s\n779:\tlearn: 0.1099293\ttotal: 4.68s\tremaining: 1.32s\n780:\tlearn: 0.1098036\ttotal: 4.68s\tremaining: 1.31s\n781:\tlearn: 0.1096093\ttotal: 4.69s\tremaining: 1.31s\n782:\tlearn: 0.1094749\ttotal: 4.7s\tremaining: 1.3s\n783:\tlearn: 0.1092731\ttotal: 4.7s\tremaining: 1.29s\n784:\tlearn: 0.1091493\ttotal: 4.71s\tremaining: 1.29s\n785:\tlearn: 0.1090159\ttotal: 4.71s\tremaining: 1.28s\n786:\tlearn: 0.1089445\ttotal: 4.72s\tremaining: 1.28s\n787:\tlearn: 0.1087926\ttotal: 4.73s\tremaining: 1.27s\n788:\tlearn: 0.1086406\ttotal: 4.73s\tremaining: 1.26s\n789:\tlearn: 0.1084989\ttotal: 4.74s\tremaining: 1.26s\n790:\tlearn: 0.1082692\ttotal: 4.74s\tremaining: 1.25s\n791:\tlearn: 0.1081390\ttotal: 4.75s\tremaining: 1.25s\n792:\tlearn: 0.1079986\ttotal: 4.76s\tremaining: 1.24s\n793:\tlearn: 0.1077885\ttotal: 4.76s\tremaining: 1.24s\n794:\tlearn: 0.1076758\ttotal: 4.77s\tremaining: 1.23s\n795:\tlearn: 0.1074680\ttotal: 4.77s\tremaining: 1.22s\n796:\tlearn: 0.1072443\ttotal: 4.78s\tremaining: 1.22s\n797:\tlearn: 0.1071400\ttotal: 4.79s\tremaining: 1.21s\n798:\tlearn: 0.1069839\ttotal: 4.79s\tremaining: 1.21s\n799:\tlearn: 0.1068534\ttotal: 4.8s\tremaining: 1.2s\n800:\tlearn: 0.1067881\ttotal: 4.8s\tremaining: 1.19s\n801:\tlearn: 0.1067015\ttotal: 4.81s\tremaining: 1.19s\n802:\tlearn: 0.1065809\ttotal: 4.82s\tremaining: 1.18s\n803:\tlearn: 0.1064346\ttotal: 4.82s\tremaining: 1.18s\n804:\tlearn: 0.1062805\ttotal: 4.83s\tremaining: 1.17s\n805:\tlearn: 0.1060878\ttotal: 4.83s\tremaining: 1.16s\n806:\tlearn: 0.1058732\ttotal: 4.84s\tremaining: 1.16s\n807:\tlearn: 0.1057269\ttotal: 4.85s\tremaining: 1.15s\n808:\tlearn: 0.1055883\ttotal: 4.85s\tremaining: 1.15s\n809:\tlearn: 0.1054442\ttotal: 4.86s\tremaining: 1.14s\n810:\tlearn: 0.1052317\ttotal: 4.86s\tremaining: 1.13s\n811:\tlearn: 0.1050488\ttotal: 4.87s\tremaining: 1.13s\n812:\tlearn: 0.1048285\ttotal: 4.88s\tremaining: 1.12s\n813:\tlearn: 0.1046510\ttotal: 4.88s\tremaining: 1.11s\n814:\tlearn: 0.1044088\ttotal: 4.89s\tremaining: 1.11s\n815:\tlearn: 0.1042150\ttotal: 4.89s\tremaining: 1.1s\n816:\tlearn: 0.1040651\ttotal: 4.9s\tremaining: 1.1s\n817:\tlearn: 0.1039486\ttotal: 4.91s\tremaining: 1.09s\n818:\tlearn: 0.1038200\ttotal: 4.91s\tremaining: 1.09s\n819:\tlearn: 0.1036652\ttotal: 4.92s\tremaining: 1.08s\n820:\tlearn: 0.1035880\ttotal: 4.93s\tremaining: 1.07s\n821:\tlearn: 0.1034453\ttotal: 4.93s\tremaining: 1.07s\n822:\tlearn: 0.1033603\ttotal: 4.94s\tremaining: 1.06s\n823:\tlearn: 0.1032199\ttotal: 4.95s\tremaining: 1.06s\n824:\tlearn: 0.1030732\ttotal: 4.95s\tremaining: 1.05s\n825:\tlearn: 0.1029424\ttotal: 4.96s\tremaining: 1.04s\n826:\tlearn: 0.1027913\ttotal: 4.96s\tremaining: 1.04s\n827:\tlearn: 0.1026139\ttotal: 4.97s\tremaining: 1.03s\n828:\tlearn: 0.1024975\ttotal: 4.97s\tremaining: 1.03s\n829:\tlearn: 0.1024234\ttotal: 4.98s\tremaining: 1.02s\n830:\tlearn: 0.1023109\ttotal: 4.99s\tremaining: 1.01s\n831:\tlearn: 0.1020644\ttotal: 4.99s\tremaining: 1.01s\n832:\tlearn: 0.1019077\ttotal: 5s\tremaining: 1s\n833:\tlearn: 0.1018185\ttotal: 5.01s\tremaining: 996ms\n834:\tlearn: 0.1016643\ttotal: 5.01s\tremaining: 990ms\n835:\tlearn: 0.1015449\ttotal: 5.02s\tremaining: 984ms\n836:\tlearn: 0.1014280\ttotal: 5.02s\tremaining: 979ms\n837:\tlearn: 0.1012465\ttotal: 5.03s\tremaining: 973ms\n838:\tlearn: 0.1011800\ttotal: 5.04s\tremaining: 966ms\n839:\tlearn: 0.1010343\ttotal: 5.04s\tremaining: 961ms\n840:\tlearn: 0.1008789\ttotal: 5.05s\tremaining: 955ms\n841:\tlearn: 0.1007671\ttotal: 5.05s\tremaining: 949ms\n842:\tlearn: 0.1006468\ttotal: 5.06s\tremaining: 943ms\n843:\tlearn: 0.1005006\ttotal: 5.07s\tremaining: 937ms\n844:\tlearn: 0.1003218\ttotal: 5.07s\tremaining: 931ms\n845:\tlearn: 0.1001515\ttotal: 5.08s\tremaining: 925ms\n846:\tlearn: 0.1000389\ttotal: 5.08s\tremaining: 919ms\n847:\tlearn: 0.0998365\ttotal: 5.09s\tremaining: 913ms\n848:\tlearn: 0.0996518\ttotal: 5.1s\tremaining: 907ms\n849:\tlearn: 0.0995170\ttotal: 5.1s\tremaining: 901ms\n850:\tlearn: 0.0994427\ttotal: 5.11s\tremaining: 895ms\n851:\tlearn: 0.0993414\ttotal: 5.12s\tremaining: 889ms\n852:\tlearn: 0.0992132\ttotal: 5.12s\tremaining: 883ms\n853:\tlearn: 0.0991172\ttotal: 5.13s\tremaining: 877ms\n854:\tlearn: 0.0990337\ttotal: 5.13s\tremaining: 870ms\n855:\tlearn: 0.0987517\ttotal: 5.14s\tremaining: 864ms\n856:\tlearn: 0.0985898\ttotal: 5.14s\tremaining: 858ms\n857:\tlearn: 0.0984855\ttotal: 5.15s\tremaining: 852ms\n858:\tlearn: 0.0983777\ttotal: 5.16s\tremaining: 846ms\n859:\tlearn: 0.0982191\ttotal: 5.16s\tremaining: 840ms\n860:\tlearn: 0.0981180\ttotal: 5.17s\tremaining: 834ms\n861:\tlearn: 0.0980253\ttotal: 5.17s\tremaining: 828ms\n862:\tlearn: 0.0978304\ttotal: 5.18s\tremaining: 822ms\n863:\tlearn: 0.0976602\ttotal: 5.18s\tremaining: 816ms\n864:\tlearn: 0.0975747\ttotal: 5.19s\tremaining: 810ms\n865:\tlearn: 0.0974884\ttotal: 5.2s\tremaining: 804ms\n866:\tlearn: 0.0973485\ttotal: 5.2s\tremaining: 798ms\n867:\tlearn: 0.0972457\ttotal: 5.21s\tremaining: 792ms\n868:\tlearn: 0.0971292\ttotal: 5.21s\tremaining: 786ms\n869:\tlearn: 0.0970284\ttotal: 5.22s\tremaining: 780ms\n870:\tlearn: 0.0968623\ttotal: 5.22s\tremaining: 774ms\n871:\tlearn: 0.0967502\ttotal: 5.23s\tremaining: 768ms\n872:\tlearn: 0.0966657\ttotal: 5.24s\tremaining: 762ms\n873:\tlearn: 0.0965524\ttotal: 5.24s\tremaining: 756ms\n874:\tlearn: 0.0964225\ttotal: 5.25s\tremaining: 750ms\n875:\tlearn: 0.0963643\ttotal: 5.25s\tremaining: 744ms\n876:\tlearn: 0.0962480\ttotal: 5.26s\tremaining: 738ms\n877:\tlearn: 0.0961432\ttotal: 5.27s\tremaining: 732ms\n878:\tlearn: 0.0959706\ttotal: 5.27s\tremaining: 726ms\n879:\tlearn: 0.0958752\ttotal: 5.28s\tremaining: 720ms\n880:\tlearn: 0.0957152\ttotal: 5.28s\tremaining: 714ms\n881:\tlearn: 0.0956303\ttotal: 5.29s\tremaining: 708ms\n882:\tlearn: 0.0954649\ttotal: 5.29s\tremaining: 702ms\n883:\tlearn: 0.0953729\ttotal: 5.3s\tremaining: 696ms\n884:\tlearn: 0.0952580\ttotal: 5.31s\tremaining: 690ms\n885:\tlearn: 0.0950235\ttotal: 5.31s\tremaining: 684ms\n886:\tlearn: 0.0949177\ttotal: 5.32s\tremaining: 677ms\n887:\tlearn: 0.0947375\ttotal: 5.32s\tremaining: 671ms\n888:\tlearn: 0.0944990\ttotal: 5.33s\tremaining: 666ms\n889:\tlearn: 0.0944316\ttotal: 5.34s\tremaining: 660ms\n890:\tlearn: 0.0943307\ttotal: 5.34s\tremaining: 654ms\n891:\tlearn: 0.0941815\ttotal: 5.35s\tremaining: 648ms\n892:\tlearn: 0.0940066\ttotal: 5.35s\tremaining: 642ms\n893:\tlearn: 0.0939262\ttotal: 5.36s\tremaining: 636ms\n894:\tlearn: 0.0937619\ttotal: 5.37s\tremaining: 630ms\n895:\tlearn: 0.0936782\ttotal: 5.37s\tremaining: 624ms\n896:\tlearn: 0.0935837\ttotal: 5.38s\tremaining: 618ms\n897:\tlearn: 0.0933587\ttotal: 5.38s\tremaining: 612ms\n898:\tlearn: 0.0932116\ttotal: 5.39s\tremaining: 606ms\n899:\tlearn: 0.0930559\ttotal: 5.4s\tremaining: 600ms\n900:\tlearn: 0.0929009\ttotal: 5.4s\tremaining: 594ms\n901:\tlearn: 0.0927835\ttotal: 5.41s\tremaining: 588ms\n902:\tlearn: 0.0927108\ttotal: 5.41s\tremaining: 582ms\n903:\tlearn: 0.0926461\ttotal: 5.42s\tremaining: 576ms\n904:\tlearn: 0.0925464\ttotal: 5.42s\tremaining: 570ms\n905:\tlearn: 0.0924641\ttotal: 5.43s\tremaining: 564ms\n906:\tlearn: 0.0923819\ttotal: 5.44s\tremaining: 558ms\n907:\tlearn: 0.0922761\ttotal: 5.44s\tremaining: 552ms\n908:\tlearn: 0.0921265\ttotal: 5.45s\tremaining: 546ms\n909:\tlearn: 0.0920154\ttotal: 5.46s\tremaining: 540ms\n910:\tlearn: 0.0918129\ttotal: 5.46s\tremaining: 534ms\n911:\tlearn: 0.0916687\ttotal: 5.47s\tremaining: 528ms\n912:\tlearn: 0.0915164\ttotal: 5.47s\tremaining: 522ms\n913:\tlearn: 0.0914125\ttotal: 5.48s\tremaining: 516ms\n914:\tlearn: 0.0913537\ttotal: 5.49s\tremaining: 510ms\n915:\tlearn: 0.0912829\ttotal: 5.49s\tremaining: 504ms\n916:\tlearn: 0.0911982\ttotal: 5.5s\tremaining: 498ms\n917:\tlearn: 0.0911152\ttotal: 5.5s\tremaining: 492ms\n918:\tlearn: 0.0910177\ttotal: 5.51s\tremaining: 486ms\n919:\tlearn: 0.0908505\ttotal: 5.52s\tremaining: 480ms\n920:\tlearn: 0.0907768\ttotal: 5.52s\tremaining: 474ms\n921:\tlearn: 0.0906902\ttotal: 5.53s\tremaining: 468ms\n922:\tlearn: 0.0905212\ttotal: 5.54s\tremaining: 462ms\n923:\tlearn: 0.0903998\ttotal: 5.54s\tremaining: 456ms\n924:\tlearn: 0.0903562\ttotal: 5.55s\tremaining: 450ms\n925:\tlearn: 0.0902144\ttotal: 5.55s\tremaining: 444ms\n926:\tlearn: 0.0900995\ttotal: 5.56s\tremaining: 438ms\n927:\tlearn: 0.0900163\ttotal: 5.57s\tremaining: 432ms\n928:\tlearn: 0.0899291\ttotal: 5.57s\tremaining: 426ms\n929:\tlearn: 0.0898278\ttotal: 5.58s\tremaining: 420ms\n930:\tlearn: 0.0897081\ttotal: 5.59s\tremaining: 414ms\n931:\tlearn: 0.0894797\ttotal: 5.59s\tremaining: 408ms\n932:\tlearn: 0.0893045\ttotal: 5.6s\tremaining: 402ms\n933:\tlearn: 0.0892255\ttotal: 5.61s\tremaining: 396ms\n934:\tlearn: 0.0891450\ttotal: 5.61s\tremaining: 390ms\n935:\tlearn: 0.0890999\ttotal: 5.62s\tremaining: 384ms\n936:\tlearn: 0.0890104\ttotal: 5.62s\tremaining: 378ms\n937:\tlearn: 0.0889130\ttotal: 5.63s\tremaining: 372ms\n938:\tlearn: 0.0887468\ttotal: 5.64s\tremaining: 366ms\n939:\tlearn: 0.0886062\ttotal: 5.64s\tremaining: 360ms\n940:\tlearn: 0.0884413\ttotal: 5.65s\tremaining: 354ms\n941:\tlearn: 0.0883147\ttotal: 5.66s\tremaining: 348ms\n942:\tlearn: 0.0882185\ttotal: 5.66s\tremaining: 342ms\n943:\tlearn: 0.0881256\ttotal: 5.67s\tremaining: 336ms\n944:\tlearn: 0.0880315\ttotal: 5.67s\tremaining: 330ms\n945:\tlearn: 0.0879208\ttotal: 5.68s\tremaining: 324ms\n946:\tlearn: 0.0877949\ttotal: 5.68s\tremaining: 318ms\n947:\tlearn: 0.0877354\ttotal: 5.69s\tremaining: 312ms\n948:\tlearn: 0.0875958\ttotal: 5.7s\tremaining: 306ms\n949:\tlearn: 0.0874640\ttotal: 5.7s\tremaining: 300ms\n950:\tlearn: 0.0872879\ttotal: 5.71s\tremaining: 294ms\n951:\tlearn: 0.0871704\ttotal: 5.72s\tremaining: 288ms\n952:\tlearn: 0.0870772\ttotal: 5.72s\tremaining: 282ms\n953:\tlearn: 0.0870282\ttotal: 5.73s\tremaining: 276ms\n954:\tlearn: 0.0868971\ttotal: 5.74s\tremaining: 270ms\n955:\tlearn: 0.0867852\ttotal: 5.74s\tremaining: 264ms\n956:\tlearn: 0.0867188\ttotal: 5.75s\tremaining: 258ms\n957:\tlearn: 0.0865586\ttotal: 5.75s\tremaining: 252ms\n958:\tlearn: 0.0864495\ttotal: 5.76s\tremaining: 246ms\n959:\tlearn: 0.0863380\ttotal: 5.77s\tremaining: 240ms\n960:\tlearn: 0.0861860\ttotal: 5.77s\tremaining: 234ms\n961:\tlearn: 0.0860800\ttotal: 5.78s\tremaining: 228ms\n962:\tlearn: 0.0860172\ttotal: 5.79s\tremaining: 222ms\n963:\tlearn: 0.0858291\ttotal: 5.79s\tremaining: 216ms\n964:\tlearn: 0.0857470\ttotal: 5.8s\tremaining: 210ms\n965:\tlearn: 0.0855913\ttotal: 5.8s\tremaining: 204ms\n966:\tlearn: 0.0854851\ttotal: 5.81s\tremaining: 198ms\n967:\tlearn: 0.0853757\ttotal: 5.82s\tremaining: 192ms\n968:\tlearn: 0.0852030\ttotal: 5.82s\tremaining: 186ms\n969:\tlearn: 0.0850695\ttotal: 5.83s\tremaining: 180ms\n970:\tlearn: 0.0849399\ttotal: 5.83s\tremaining: 174ms\n971:\tlearn: 0.0847871\ttotal: 5.84s\tremaining: 168ms\n972:\tlearn: 0.0846759\ttotal: 5.85s\tremaining: 162ms\n973:\tlearn: 0.0845210\ttotal: 5.85s\tremaining: 156ms\n974:\tlearn: 0.0844404\ttotal: 5.86s\tremaining: 150ms\n975:\tlearn: 0.0843867\ttotal: 5.86s\tremaining: 144ms\n976:\tlearn: 0.0842348\ttotal: 5.87s\tremaining: 138ms\n977:\tlearn: 0.0841463\ttotal: 5.88s\tremaining: 132ms\n978:\tlearn: 0.0840486\ttotal: 5.88s\tremaining: 126ms\n979:\tlearn: 0.0839616\ttotal: 5.89s\tremaining: 120ms\n980:\tlearn: 0.0837947\ttotal: 5.89s\tremaining: 114ms\n981:\tlearn: 0.0836552\ttotal: 5.9s\tremaining: 108ms\n982:\tlearn: 0.0835240\ttotal: 5.91s\tremaining: 102ms\n983:\tlearn: 0.0833683\ttotal: 5.91s\tremaining: 96.2ms\n984:\tlearn: 0.0832619\ttotal: 5.92s\tremaining: 90.1ms\n985:\tlearn: 0.0831229\ttotal: 5.92s\tremaining: 84.1ms\n986:\tlearn: 0.0830003\ttotal: 5.93s\tremaining: 78.1ms\n987:\tlearn: 0.0829024\ttotal: 5.94s\tremaining: 72.1ms\n988:\tlearn: 0.0827982\ttotal: 5.94s\tremaining: 66.1ms\n989:\tlearn: 0.0827111\ttotal: 5.95s\tremaining: 60.1ms\n990:\tlearn: 0.0826027\ttotal: 5.95s\tremaining: 54.1ms\n991:\tlearn: 0.0824705\ttotal: 5.96s\tremaining: 48.1ms\n992:\tlearn: 0.0823497\ttotal: 5.96s\tremaining: 42ms\n993:\tlearn: 0.0821833\ttotal: 5.97s\tremaining: 36ms\n994:\tlearn: 0.0820395\ttotal: 5.98s\tremaining: 30ms\n995:\tlearn: 0.0819313\ttotal: 5.98s\tremaining: 24ms\n996:\tlearn: 0.0818303\ttotal: 5.99s\tremaining: 18ms\n997:\tlearn: 0.0817154\ttotal: 6s\tremaining: 12ms\n998:\tlearn: 0.0815660\ttotal: 6s\tremaining: 6.01ms\n999:\tlearn: 0.0814675\ttotal: 6.01s\tremaining: 0us\n","output_type":"stream"},{"execution_count":214,"output_type":"execute_result","data":{"text/plain":"<catboost.core.CatBoostClassifier at 0x7ea95632cbb0>"},"metadata":{}}],"execution_count":214},{"cell_type":"code","source":"test_pred = clf.predict(test_embeds)\ntest_proba = clf.predict_proba(test_embeds)[:, 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:48:15.478415Z","iopub.execute_input":"2025-05-09T14:48:15.478752Z","iopub.status.idle":"2025-05-09T14:48:15.494967Z","shell.execute_reply.started":"2025-05-09T14:48:15.478724Z","shell.execute_reply":"2025-05-09T14:48:15.493951Z"}},"outputs":[],"execution_count":215},{"cell_type":"code","source":"print(\"Accuracy:\", accuracy_score(target_test, test_pred))\nprint(\"ROC-AUC:\", roc_auc_score(target_test, test_proba))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"arr = np.array([0.8147836363342021, 0.7974777808356672, 0.8094089459455085])\n\narr.mean(), arr.std()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:27:51.987784Z","iopub.execute_input":"2025-05-02T14:27:51.988106Z","iopub.status.idle":"2025-05-02T14:27:51.994016Z","shell.execute_reply.started":"2025-05-02T14:27:51.988083Z","shell.execute_reply":"2025-05-02T14:27:51.993107Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"(0.8072234543717927, 0.00723212457019052)"},"metadata":{}}],"execution_count":77},{"cell_type":"markdown","source":"<!-- - COLES embeds + Catboost:\n  - `Accuracy: 0.736`, `0.72`, `0.722`, avg: `0.726 +- 0.0071` \n  -  `ROC-AUC: 0.8099107995661394`, `0.8041475773421184`, `0.8088423370189894`, avg: `0.8076 +- 0.0025`\n\n---\n\n- COLES embeds w/ SWIN_Agg seq_enc + Catboost:\n  - Accuracy: `0.746`, `0.744`, `0.734`, avg: `0.7413 +- 0.0052`\n  - ROC-AUC: `0.8197050395816807`, `0.8041961438215345`, `0.8093603794660925`, avg: `0.8111 +- 0.0064`\n\n---\n\n- COLES embeds w/ SWIN_Agg seq_enc (trained on InfoNCE loss) + Catboost:\n  - Accuracy: `0.728`, `0.736`, `0.704`, avg: `0.7227 +- 0.0136`\n  - ROC-AUC: `0.803888556118567`, `0.8046494309627494`, `0.7928153988117401`, avg: `0.8005 +- 0.0054`\n\n--- -->\n\n<!-- - COLES embeds w/ SWIN_Agg seq_enc + w/ ConvAgg (3 trx) + Catboost:\n  - Accuracy: `0.762`, `0.716`, `0.74`, avg: `0.7393 +- 0.0188`\n  - ROC-AUC: `0.8183128005050913`, `0.7948875686001521`, `0.8069158666688252`, avg: `0.8067 +- 0.0096`\n\n---\n\n- COLES embeds w/ SWIN_Agg seq_enc + w/ ConvAgg (5 trx) + Catboost:\n  - Accuracy: `0.742`, `0.724`, `0.746`, avg: `0.7373 +- 0.0096`\n  - ROC-AUC: `0.8101698207896909`, `0.7846724190963398`, `0.8073691538100403`, avg: `0.8007 +- 0.0114`\n\n---\n\n- COLES embeds w/ SWIN_Agg seq_enc + Catboost (no start-end fusion):\n  - Accuracy: `0.736`, `0.734`, `0.726`, avg: `0.732 +- 0.0043`\n  - ROC-AUC: `0.8147350698547863`, `0.8079519515630311`, `0.8073043985041525`, avg: `0.81 +- 0.0034`\n\n---\n\n- COLES embeds w/ SWIN_Agg seq_enc (No RNN for feat agg) + Catboost:\n  - Accuracy: `0.694`\n  - ROC-AUC: `0.7710576160334137` -->\n\n<!-- **Вывод:** Неудовлетворительные результаты, SWIN стоит использовать только в связке с RNN (усреднение эмбеддингов не работает). -->\n\n<!-- - COLES embeds w/ SWIN_Agg seq_enc (trained on InfoNCE loss) + Catboost:\n  - Accuracy: `0.718`, `0.726`, `0.702`, avg: `0.7153 +- 0.01`\n  - ROC-AUC: `0.8016383092389634`, `0.7930744200352916`, `0.8027229606125851`, avg: `0.7991 +- 0.0043`\n\n---\n\n- COLES embeds w/ SWIN_Agg seq_enc (decoder-like training) + Catboost:\n  - Accuracy: `0.734`, `0.734`, `0.722`, avg: `0.73 +- 0.0057`\n  - ROC-AUC: `0.8147836363342021`, `0.7974777808356672`, `0.8094089459455085`, avg: `0.8072 +- 0.0072` -->\n\n<!-- ---\n\n**Вывод:** для CoLES агрегация свёртками в целом приводит к повышению качества, причём больше это проявляется по accuracy. Лучший результат достигается при агрегации свёрточным слоем с ядром свёртки размера 7, после чего результат становится лишь хуже.\n\n**Лучший результат:**  \n\n- COLES embeds + ConvAgg (7 trx) + Catboost:\n  - Accuracy: `0.758`, `0.728`, `0.732`, avg: `0.7393 +- 0.0133`\n  - ROC-AUC: `0.8269576338411229`, `0.7956646322708065`, `0.8140389503164915`, avg: `0.8122 +- 0.0128` -->","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"- COLES embeds + Catboost:\n  - `Accuracy: 0.736`, `0.72`, `0.722`, avg: `0.726 +- 0.0071` \n  -  `ROC-AUC: 0.8099107995661394`, `0.8041475773421184`, `0.8088423370189894`, avg: `0.8076 +- 0.0025`\n\n---\n\n- COLES embeds w/ SWIN_Agg seq_enc (num_heads is multiplied by factor 2 on each stage) + Catboost:\n  - Accuracy: `0.756`, `0.738`, `0.734`, avg: `0.7427 +- 0.0096`\n  - ROC-AUC: `0.8205468585582232`, `0.8086156934483819`, `0.8079195739100873`, avg: `0.8124 +- 0.0058`\n\n---\n\n- COLES embeds w/ SWIN_Agg seq_enc (smaller window sizes) + Catboost:\n  - Accuracy: `0.746`, `0.732`, `0.724`, avg: `0.734 +- 0.0091`\n  - ROC-AUC: `0.8117725146104158`, `0.8022372958184262`, `0.795000890385456`, avg: `0.803 +- 0.0069`\n\n---\n\n- COLES embeds w/ SWIN_Agg seq_enc (trained on InfoNCE loss) + Catboost:\n  - Accuracy: `0.72`, `0.722`, `0.708`, avg: `0.7167 +- 0.0062`\n  - ROC-AUC: `0.8057340823363714`, `0.7915202926939827`, `0.7877321072995418`, avg: `0.795 +- 0.0077`\n\n---\n\n- COLES embeds w/ SWIN_Agg seq_enc & ConvAgg (3 trx) + Catboost:\n  - Accuracy: `0.756`, `0.762`, `0.74`, avg: `0.7527 +- 0.0093`\n  - ROC-AUC: `0.8149131469459778`, `0.812209612925159`, `0.8027553382655291`, avg: `0.81 +- 0.0052`\n\n---\n\n- COLES embeds w/ SWIN_Agg seq_enc (smaller window sizes) & ConvAgg (3 trx) + Catboost:\n  - Accuracy: `0.764`, `0.726`, `0.76`, avg: `0.75 +- 0.017`\n  - ROC-AUC: `0.8225380842142753`, `0.7952113451295915`, `0.8167586731637823`, avg: `0.8115 +- 0.0118`\n\n**Вывод:** CoLES с SWIN-трансформером в качестве seq_encoder'а демонстрирует значительно лучшее качество, чем обычный CoLES (с RNN), - как по accuracy, так и по ROC-AUC.\n\nВ качестве лоссов пробовались ContrastiveLoss и InfoNCE Loss. Обучать такой CoLES нужно всё же на стандартный Contrastive Loss, так как при обучении на InfoNCE Loss качество становится даже хуже, чем у бейзлайна.  \n\nПо умолчанию стартовое окно покрывает 4 транзакции, также пробовался сетап с размером окна в 2 транзакции, он демонстрировал качество хуже, чем при большом размере окна.\n\nТакже пробовалась архитектура (Conv Aggregator в качестве trx_enc +  SWIN + RNN SeqEncoder). Размер свёртки Conv Aggregator'а брался как минимальный (3) из рассматриваемых, чтобы не увеличивать слишком сильно за счёт свёртки - и далее - SWIN-трансформера - receptive field. Такой вариант продемонстрировал качество, значительно лучшее, чем у SWIN-трансформера с обычным trx_encoder'ом, как в случае маленьких окон, так и в случае размера окна по умолчанию. Сетап COLES embeds w/ SWIN_Agg seq_enc & ConvAgg (3 trx) + Catboost оказался лучшим подходом по accuracy в целом.\n\n**Лучший результат:**\n\n- COLES embeds w/ SWIN_Agg seq_enc & ConvAgg (3 trx) + Catboost:\n  - Accuracy: `0.756`, `0.762`, `0.74`, avg: `0.7527 +- 0.0093`\n  - ROC-AUC: `0.8149131469459778`, `0.812209612925159`, `0.8027553382655291`, avg: `0.81 +- 0.0052`","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Train sequences lengths check:**","metadata":{}},{"cell_type":"code","source":"trx_encoder_params = dict(\n    embeddings={\n        \"MCC\": {\"in\": 342, \"out\": 32},\n        \"channel_type\": {\"in\": 7, \"out\": 32},\n        \"currency\": {\"in\": 60, \"out\": 32},\n        \"trx_category\": {\"in\": 11, \"out\": 32}            \n    },\n    numeric_values={\"amount\": \"log\"},\n    embeddings_noise=0.003,\n    linear_projection_size=192,\n    k=31,\n    time_col=\"event_time\"\n)\n\ntrx_encoder = TrxEncoderT2V(**trx_encoder_params)\ntrx_encoder.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T21:20:50.449017Z","iopub.execute_input":"2025-05-01T21:20:50.449536Z","iopub.status.idle":"2025-05-01T21:20:50.465046Z","shell.execute_reply.started":"2025-05-01T21:20:50.449496Z","shell.execute_reply":"2025-05-01T21:20:50.463966Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"TrxEncoderT2V(\n  (embeddings): ModuleDict(\n    (MCC): NoisyEmbedding(\n      342, 32, padding_idx=0\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (channel_type): NoisyEmbedding(\n      7, 32, padding_idx=0\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (currency): NoisyEmbedding(\n      60, 32, padding_idx=0\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (trx_category): NoisyEmbedding(\n      11, 32, padding_idx=0\n      (dropout): Dropout(p=0, inplace=False)\n    )\n  )\n  (custom_embeddings): ModuleDict(\n    (amount): LogScaler()\n  )\n  (custom_embedding_batch_norm): RBatchNorm(\n    (bn): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (time2vec_days): Time2Vec()\n  (linear_projection_head): Linear(in_features=161, out_features=192, bias=True)\n)"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"train_loader = inference_data_loader(data_train, num_workers=0, batch_size=128)\n\ntrx_encoder.eval()\n\nseq_lens = []\n\nfor batch in tqdm(train_loader):\n    embeds_batch = trx_encoder(batch.to(\"cuda\"))\n    seq_lens += [embeds_batch.seq_lens.detach().cpu().numpy()]\n\nseq_lens = np.concatenate(seq_lens)\n\nthreshold = int(np.quantile(seq_lens, 0.6))\n\nprint(\"Max Length:\", threshold)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T21:21:41.076841Z","iopub.execute_input":"2025-05-01T21:21:41.077248Z","iopub.status.idle":"2025-05-01T21:21:41.522109Z","shell.execute_reply.started":"2025-05-01T21:21:41.077203Z","shell.execute_reply":"2025-05-01T21:21:41.521031Z"}},"outputs":[{"name":"stderr","text":"36it [00:00, 82.96it/s]","output_type":"stream"},{"name":"stdout","text":"Max Length: 83\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"- **CPC modeling:**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Скорректируем класс CpcModule так, чтобы при работе CPC не было даталиков:**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\nfrom ptls.data_load.padded_batch import PaddedBatch\n\n\nclass CPC_ShiftedLoss(nn.Module):\n    def __init__(self, n_negatives=None, n_forward_steps=None, shift=0):\n        super().__init__()\n        self.n_negatives = n_negatives\n        self.n_forward_steps = n_forward_steps\n        self.shift = shift\n\n    def _get_preds(self, base_embeddings, mapped_ctx_embeddings):\n        batch_size, max_seq_len, emb_size = base_embeddings.payload.shape\n        _, _, _, n_forward_steps = mapped_ctx_embeddings.payload.shape\n        seq_lens = mapped_ctx_embeddings.seq_lens\n        device = mapped_ctx_embeddings.payload.device\n\n        # num_additional_samples = mapped_ctx_embeddings.payload.shape[1] - max_seq_len\n        # if num_additional_samples > 0:\n        #     additional_samples = torch.zeros((batch_size, num_additional_samples, emb_size), device=device)\n        #     base_embeddings = PaddedBatch(torch.cat((base_embeddings.payload, additional_samples), dim=1), base_embeddings.seq_lens)\n        #     max_seq_len += num_additional_samples               \n        \n        #mapped_ctx_embeddings = mapped_ctx_embeddings.payload\n            \n        mapped_ctx_embeddings = mapped_ctx_embeddings.payload[:, :max_seq_len, :, :]\n\n        len_mask = torch.arange(max_seq_len).unsqueeze(0).expand(batch_size, -1).to(device)\n        len_mask = (len_mask < seq_lens.unsqueeze(1).expand(-1, max_seq_len)).float()\n        \n        possible_negatives = base_embeddings.payload.reshape(batch_size * max_seq_len, emb_size)\n\n        mask = len_mask.unsqueeze(0).expand(batch_size, *len_mask.shape).clone()\n\n        mask = mask.reshape(batch_size, -1)\n        sample_ids = torch.multinomial(mask, self.n_negatives)\n        neg_samples = possible_negatives[sample_ids]\n\n        positive_preds, neg_preds = [], []\n        len_mask_exp = len_mask.unsqueeze(-1).unsqueeze(-1).to(device).expand(-1, -1, emb_size, n_forward_steps)\n        trimmed_mce = mapped_ctx_embeddings.mul(len_mask_exp)  # zero context vectors by sequence lengths\n        for i in range(1, n_forward_steps + 1):\n            ce_i = trimmed_mce[:, 0:(max_seq_len - i - self.shift), :, i - 1]\n            be_i = base_embeddings.payload[:, (i + self.shift):max_seq_len]\n\n            positive_pred_i = ce_i.mul(be_i).sum(axis=-1)\n            positive_preds.append(positive_pred_i)\n\n            neg_pred_i = ce_i.matmul(neg_samples.transpose(-2, -1))\n            neg_preds.append(neg_pred_i)\n\n        return positive_preds, neg_preds\n\n    def forward(self, embeddings, _):\n        base_embeddings, _, mapped_ctx_embeddings = embeddings\n        device = mapped_ctx_embeddings.payload.device\n        positive_preds, neg_preds = self._get_preds(base_embeddings, mapped_ctx_embeddings)\n\n        step_losses = []\n        for positive_pred_i, neg_pred_i in zip(positive_preds, neg_preds):\n            step_loss = -F.log_softmax(torch.cat([positive_pred_i.unsqueeze(-1), neg_pred_i], dim=-1), dim=-1)[:, :, 0].mean()\n            step_losses.append(step_loss)\n\n        loss = torch.stack(step_losses).mean()\n        return loss\n\n    def cpc_accuracy(self, embeddings, _):\n        base_embeddings, _, mapped_ctx_embeddings = embeddings\n        positive_preds, neg_preds = self._get_preds(base_embeddings, mapped_ctx_embeddings)\n\n        batch_size, max_seq_len, emb_size = base_embeddings.payload.shape\n        #max_seq_len = mapped_ctx_embeddings.payload.shape[1]\n        seq_lens = mapped_ctx_embeddings.seq_lens\n        device = mapped_ctx_embeddings.payload.device\n\n        len_mask = torch.arange(max_seq_len).unsqueeze(0).expand(batch_size, -1).to(device)\n        len_mask = (len_mask < seq_lens.unsqueeze(1).expand(-1, max_seq_len)).float()\n\n        total, accurate = 0, 0\n        \n        for i, (positive_pred_i, neg_pred_i) in enumerate(zip(positive_preds, neg_preds)):\n            i_mask = len_mask[:, (self.shift + i + 1):max_seq_len].to(device)\n            total += i_mask.sum().item()\n            accurate += (((positive_pred_i.unsqueeze(-1).expand(*neg_pred_i.shape) > neg_pred_i) \\\n                          .sum(dim=-1) == self.n_negatives) * i_mask).sum().item()\n        return accurate / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:08:05.229934Z","iopub.execute_input":"2025-05-09T15:08:05.230280Z","iopub.status.idle":"2025-05-09T15:08:05.242226Z","shell.execute_reply.started":"2025-05-09T15:08:05.230253Z","shell.execute_reply":"2025-05-09T15:08:05.241382Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import torch\n\nfrom ptls.frames.abs_module import ABSModule\nfrom ptls.frames.cpc.metrics.cpc_accuracy import CpcAccuracy\nfrom ptls.nn.seq_encoder import RnnSeqEncoder\nfrom ptls.data_load.padded_batch import PaddedBatch\n\n\nclass CpcModule(ABSModule):\n    \"\"\"Contrastive Predictive Coding ([CPC](https://arxiv.org/abs/1807.03748))\n\n    Original sequence are encoded by `TrxEncoder`.\n    Hidden representation `z` is an embedding for each individual transaction.\n    Next `RnnEncoder` used for `context` calculation from `z`.\n    Linear predictors are used to predict next trx embedding by context.\n    The loss function tends to make future trx embedding and they predict closer.\n    Negative sampling are used to avoid trivial solution.\n\n    Parameters\n        seq_encoder:\n            Model which calculate embeddings for original raw transaction sequences\n            `seq_encoder` is trained by `CoLESModule` to get better representations of input sequences\n        head:\n            Not used\n        loss:\n            Keep None. CPCLoss used by default\n        validation_metric:\n            Keep None. CPCAccuracy used by default\n        optimizer_partial:\n            optimizer init partial. Network parameters are missed.\n        lr_scheduler_partial:\n            scheduler init partial. Optimizer are missed.\n\n    \"\"\"\n    def __init__(self, validation_metric=None,\n                       seq_encoder=None,\n                       head=None,\n                       n_negatives=40, n_forward_steps=6, shift='none',\n                       optimizer_partial=None,\n                       lr_scheduler_partial=None):\n\n        self.save_hyperparameters('n_negatives', 'n_forward_steps')\n\n        if shift == 'add':\n            loss = CPC_ShiftedLoss(n_negatives=n_negatives, n_forward_steps=n_forward_steps, shift=(seq_encoder.trx_encoder.agg_samples - 1))\n        else:\n            loss = CPC_ShiftedLoss(n_negatives=n_negatives, n_forward_steps=n_forward_steps, shift=0)\n\n        if validation_metric is None:\n            validation_metric = CpcAccuracy(loss)\n\n        seq_encoder.seq_encoder.is_reduce_sequence = False\n\n        super().__init__(validation_metric,\n                         seq_encoder,\n                         loss,\n                         optimizer_partial,\n                         lr_scheduler_partial)\n\n        linear_size = self.seq_encoder.trx_encoder.output_size\n        embedding_size = self.seq_encoder.embedding_size\n        self._linears = torch.nn.ModuleList([torch.nn.Linear(embedding_size, linear_size)\n                                             for _ in range(loss.n_forward_steps)])\n\n    @property\n    def metric_name(self):\n        return 'cpc_accuracy'\n\n    @property\n    def is_requires_reduced_sequence(self):\n        return False\n\n    def shared_step(self, x, y):\n        trx_encoder = self._seq_encoder.trx_encoder\n        swin_fusion = self._seq_encoder.swin_fusion\n        seq_encoder = self._seq_encoder.seq_encoder\n\n        base_embeddings = trx_encoder(x)\n        context_embeddings = seq_encoder(swin_fusion(base_embeddings))\n        \n        me = []\n        for l in self._linears:\n            me.append(l(context_embeddings.payload))\n        mapped_ctx_embeddings = PaddedBatch(torch.stack(me, dim=3), context_embeddings.seq_lens)\n\n        return (base_embeddings, context_embeddings, mapped_ctx_embeddings), y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:08:08.302141Z","iopub.execute_input":"2025-05-09T15:08:08.302445Z","iopub.status.idle":"2025-05-09T15:08:08.310752Z","shell.execute_reply.started":"2025-05-09T15:08:08.302420Z","shell.execute_reply":"2025-05-09T15:08:08.309909Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"# import gc\n\n# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:49:16.781175Z","iopub.execute_input":"2025-05-04T21:49:16.781492Z","iopub.status.idle":"2025-05-04T21:49:17.635937Z","shell.execute_reply.started":"2025-05-04T21:49:16.781471Z","shell.execute_reply":"2025-05-04T21:49:17.635193Z"}},"outputs":[],"execution_count":496},{"cell_type":"code","source":"seed_everything(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:28:16.730838Z","iopub.execute_input":"2025-05-09T16:28:16.731173Z","iopub.status.idle":"2025-05-09T16:28:16.736008Z","shell.execute_reply.started":"2025-05-09T16:28:16.731145Z","shell.execute_reply":"2025-05-09T16:28:16.735282Z"}},"outputs":[],"execution_count":164},{"cell_type":"markdown","source":"**DataLoaders:**","metadata":{}},{"cell_type":"code","source":"data = PtlsDataModule(\n    train_data=CpcDataset(\n        MemoryMapDataset(data=data_train),\n        min_len=83,             \n        max_len=107\n    ),\n    train_num_workers=4,\n    train_batch_size=128,\n    valid_data=CpcDataset(\n        MemoryMapDataset(data=data_test),\n        min_len=83,\n        max_len=107\n    ),\n    valid_num_workers=4,\n    valid_batch_size=128\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:28:18.570636Z","iopub.execute_input":"2025-05-09T16:28:18.570938Z","iopub.status.idle":"2025-05-09T16:28:18.575894Z","shell.execute_reply.started":"2025-05-09T16:28:18.570914Z","shell.execute_reply":"2025-05-09T16:28:18.574942Z"}},"outputs":[],"execution_count":165},{"cell_type":"markdown","source":"**Модель:**","metadata":{}},{"cell_type":"code","source":"N_EPOCHS = 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:28:20.650039Z","iopub.execute_input":"2025-05-09T16:28:20.650393Z","iopub.status.idle":"2025-05-09T16:28:20.654149Z","shell.execute_reply.started":"2025-05-09T16:28:20.650366Z","shell.execute_reply":"2025-05-09T16:28:20.653178Z"}},"outputs":[],"execution_count":166},{"cell_type":"code","source":"trx_encoder_params = dict(\n    embeddings={\n        \"MCC\": {\"in\": 342, \"out\": 32}, # 8 / 32\n        \"channel_type\": {\"in\": 7, \"out\": 32},\n        \"currency\": {\"in\": 60, \"out\": 32},\n        \"trx_category\": {\"in\": 11, \"out\": 32}            \n    },\n    numeric_values={\"amount\": \"log\"},\n    embeddings_noise=0.003,\n    linear_projection_size=192, # 192\n    k=31,\n    time_col=\"event_time\",\n    agg_samples=3, # 3, 5, 7, 9\n    use_window_attention=False\n)\n\n#trx_encoder = TrxEncoderT2V(**trx_encoder_params)\ntrx_encoder = ConvAggregator(**trx_encoder_params)\n\n# seq_encoder = SWIN_SeqEncoder(\n#     trx_encoder=trx_encoder,\n#     depths=[2, 2, 6, 2],\n#     num_heads=4,\n#     start_window_size=4,\n#     window_size_mult=2,\n#     drop=0.1,\n#     attn_drop=0.1,\n#     drop_path=0.1,\n#     decoder=True,\n#     start_end_fusion=False,\n#     is_reduce_sequence=False\n# )\n\nseq_encoder = SWIN_RNN_SeqEncoder(\n    trx_encoder=trx_encoder,\n    swin_depths=[2, 2, 6, 2],\n    swin_num_heads=[2, 4, 8, 16],\n    swin_start_window_size=2, # 2, 4\n    swin_window_size_mult=2,\n    swin_drop=0.1,\n    swin_attn_drop=0.1,\n    swin_drop_path=0.1,\n    swin_decoder=True,\n    swin_start_end_fusion=False,\n    hidden_size=512,\n    type=\"gru\"\n)\n\n\ncpc = CpcModule(\n    seq_encoder=seq_encoder,\n    n_forward_steps=6,\n    n_negatives=40,\n    shift='add', # 'none' / 'add'\n    optimizer_partial=partial(torch.optim.Adam, lr=3e-4), # Adam, 5e-5\n    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=5, gamma=0.5) # step_size=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:29:01.671967Z","iopub.execute_input":"2025-05-09T16:29:01.672333Z","iopub.status.idle":"2025-05-09T16:29:01.745366Z","shell.execute_reply.started":"2025-05-09T16:29:01.672306Z","shell.execute_reply":"2025-05-09T16:29:01.744715Z"}},"outputs":[],"execution_count":167},{"cell_type":"code","source":"# print(sum(p.numel() for p in cpc.parameters() if p.requires_grad))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **Params in standard CPC: 1720514**\n\n- **Params in CPC with SWIN_RNN SeqEnc: 7060306** => 4x time increase in params","metadata":{}},{"cell_type":"markdown","source":"**Обучение:**","metadata":{}},{"cell_type":"code","source":"logger = CometLogger(project_name=\"evs-ssl-rb\", experiment_name=\"CPC_modeling_SWIN_agg (w/ conv_agg, 3trx)\")\n\ntrainer = pl.Trainer(\n    logger=logger,\n    max_epochs=N_EPOCHS,\n    accelerator=\"gpu\",\n    devices=1,\n    enable_progress_bar=True,\n    gradient_clip_val=1.,\n    gradient_clip_algorithm=\"norm\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:29:07.734763Z","iopub.execute_input":"2025-05-09T16:29:07.735064Z","iopub.status.idle":"2025-05-09T16:29:07.779989Z","shell.execute_reply.started":"2025-05-09T16:29:07.735040Z","shell.execute_reply":"2025-05-09T16:29:07.779392Z"}},"outputs":[],"execution_count":168},{"cell_type":"code","source":"trainer.fit(cpc, data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:29:09.129065Z","iopub.execute_input":"2025-05-09T16:29:09.129378Z","iopub.status.idle":"2025-05-09T16:32:45.502076Z","shell.execute_reply.started":"2025-05-09T16:29:09.129354Z","shell.execute_reply":"2025-05-09T16:32:45.501344Z"}},"outputs":[{"name":"stderr","text":"\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/askoro/evs-ssl-rb/db4fb9b7eadf44ea83b29d58faeacbfc\n\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/kaggle/working' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:310: PossibleUserWarning:\n\nThe number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e12b22dc50324fd2b9ce846bde000b0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : CPC_modeling_SWIN_agg (w/ conv_agg, 3trx)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/askoro/evs-ssl-rb/db4fb9b7eadf44ea83b29d58faeacbfc\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [86]               : (1.686346411705017, 3.955394744873047)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seq_len [14]            : (60.2421875, 71.78125)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid/cpc_accuracy [20] : (0.13515004515647888, 0.5865319967269897)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : CPC_modeling_SWIN_agg (w/ conv_agg, 3trx)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_batch_size   : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_drop_last    : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_num_workers  : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_batch_size  : 128\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_drop_last   : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_num_workers : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid_batch_size  : 128\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid_drop_last   : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid_num_workers : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph         : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n","output_type":"stream"}],"execution_count":169},{"cell_type":"code","source":"trainer.logged_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:33:46.337072Z","iopub.execute_input":"2025-05-09T16:33:46.337467Z","iopub.status.idle":"2025-05-09T16:33:46.346095Z","shell.execute_reply.started":"2025-05-09T16:33:46.337439Z","shell.execute_reply":"2025-05-09T16:33:46.345198Z"}},"outputs":[{"execution_count":170,"output_type":"execute_result","data":{"text/plain":"{'loss': tensor(1.9482),\n 'seq_len': tensor(60.9500),\n 'valid/cpc_accuracy': tensor(0.5776)}"},"metadata":{}}],"execution_count":170},{"cell_type":"code","source":"torch.save(seq_encoder.state_dict(), \"cpc_enc_baseline_rosbank.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T01:55:55.069008Z","iopub.execute_input":"2025-02-26T01:55:55.069371Z","iopub.status.idle":"2025-02-26T01:55:55.082795Z","shell.execute_reply.started":"2025-02-26T01:55:55.069345Z","shell.execute_reply":"2025-02-26T01:55:55.081884Z"}},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":"**Измерим качество на тесте (catboost поверх эмбеддингов):**","metadata":{}},{"cell_type":"code","source":"# !wget \"https://drive.google.com/uc?export=download&id=11j6QgNsdOSTK-GRaAJLKObDW7ehS_aqK\" -O \"cpc_enc_baseline_higher_trx_dim.pt\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder = cpc.seq_encoder\n\n# state_dict = torch.load(\"./cpc_enc_baseline_higher_trx_dim.pt\")\n# encoder.load_state_dict(state_dict)\n\ndevice = \"cuda:0\"\n\nencoder.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:33:49.131534Z","iopub.execute_input":"2025-05-09T16:33:49.131849Z","iopub.status.idle":"2025-05-09T16:33:49.157599Z","shell.execute_reply.started":"2025-05-09T16:33:49.131827Z","shell.execute_reply":"2025-05-09T16:33:49.156901Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":171,"output_type":"execute_result","data":{"text/plain":"SWIN_RNN_SeqEncoder(\n  (trx_encoder): ConvAggregator(\n    (embeddings): ModuleDict(\n      (MCC): NoisyEmbedding(\n        342, 32, padding_idx=0\n        (dropout): Dropout(p=0, inplace=False)\n      )\n      (channel_type): NoisyEmbedding(\n        7, 32, padding_idx=0\n        (dropout): Dropout(p=0, inplace=False)\n      )\n      (currency): NoisyEmbedding(\n        60, 32, padding_idx=0\n        (dropout): Dropout(p=0, inplace=False)\n      )\n      (trx_category): NoisyEmbedding(\n        11, 32, padding_idx=0\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (custom_embeddings): ModuleDict(\n      (amount): LogScaler()\n    )\n    (time2vec_days): Time2Vec()\n    (linear_projection_head): Linear(in_features=161, out_features=192, bias=True)\n    (conv): Conv1d(192, 192, kernel_size=(3,), stride=(1,), padding=(2,), bias=False)\n  )\n  (seq_encoder): RnnEncoder(\n    (rnn): GRU(192, 512, batch_first=True)\n    (reducer): LastStepEncoder()\n  )\n  (swin_fusion): SwinTransformerBackbone(\n    (backbone): ModuleList(\n      (0): SwinTransformerLayer(\n        dim=192, depth=2, num_heads=2, window_size=2\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=192, num_heads=2, window_size=2, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=192, window_size=2, num_heads=2\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=192, num_heads=2, window_size=2, shift_size=1, mlp_ratio=4.0\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=192, window_size=2, num_heads=2\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (1): SwinTransformerLayer(\n        dim=192, depth=2, num_heads=4, window_size=4\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=192, num_heads=4, window_size=4, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=192, window_size=4, num_heads=4\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=192, num_heads=4, window_size=4, shift_size=2, mlp_ratio=4.0\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=192, window_size=4, num_heads=4\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (2): SwinTransformerLayer(\n        dim=192, depth=6, num_heads=8, window_size=8\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=192, num_heads=8, window_size=8, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=192, window_size=8, num_heads=8\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=192, num_heads=8, window_size=8, shift_size=4, mlp_ratio=4.0\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=192, window_size=8, num_heads=8\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): SwinTransformerBlock(\n            dim=192, num_heads=8, window_size=8, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=192, window_size=8, num_heads=8\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): SwinTransformerBlock(\n            dim=192, num_heads=8, window_size=8, shift_size=4, mlp_ratio=4.0\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=192, window_size=8, num_heads=8\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): SwinTransformerBlock(\n            dim=192, num_heads=8, window_size=8, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=192, window_size=8, num_heads=8\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): SwinTransformerBlock(\n            dim=192, num_heads=8, window_size=8, shift_size=4, mlp_ratio=4.0\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=192, window_size=8, num_heads=8\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (3): SwinTransformerLayer(\n        dim=192, depth=2, num_heads=16, window_size=16\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=192, num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=192, window_size=16, num_heads=16\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=192, num_heads=16, window_size=16, shift_size=8, mlp_ratio=4.0\n            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=192, window_size=16, num_heads=16\n              (qkv): Linear(in_features=192, out_features=576, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=192, out_features=192, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=192, out_features=768, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=768, out_features=192, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":171},{"cell_type":"code","source":"encoder.seq_encoder.is_reduce_sequence = True\nchange_to_enc(encoder.swin_fusion)\n\n\n\n# change_to_enc(encoder.seq_encoder.swin_fusion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:33:55.551769Z","iopub.execute_input":"2025-05-09T16:33:55.552077Z","iopub.status.idle":"2025-05-09T16:33:55.556073Z","shell.execute_reply.started":"2025-05-09T16:33:55.552054Z","shell.execute_reply":"2025-05-09T16:33:55.555139Z"}},"outputs":[],"execution_count":172},{"cell_type":"code","source":"# change_to_dec(encoder.swin_fusion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T19:06:49.334607Z","iopub.execute_input":"2025-05-08T19:06:49.334874Z","iopub.status.idle":"2025-05-08T19:06:49.338641Z","shell.execute_reply.started":"2025-05-08T19:06:49.334854Z","shell.execute_reply":"2025-05-08T19:06:49.337780Z"}},"outputs":[],"execution_count":143},{"cell_type":"code","source":"from tqdm import tqdm\n\nseed_everything(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:33:59.689624Z","iopub.execute_input":"2025-05-09T16:33:59.689914Z","iopub.status.idle":"2025-05-09T16:33:59.695125Z","shell.execute_reply.started":"2025-05-09T16:33:59.689892Z","shell.execute_reply":"2025-05-09T16:33:59.694297Z"}},"outputs":[],"execution_count":173},{"cell_type":"code","source":"import gc\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:34:01.478882Z","iopub.execute_input":"2025-05-09T16:34:01.479292Z","iopub.status.idle":"2025-05-09T16:34:02.025760Z","shell.execute_reply.started":"2025-05-09T16:34:01.479258Z","shell.execute_reply":"2025-05-09T16:34:02.024940Z"}},"outputs":[],"execution_count":174},{"cell_type":"code","source":"train_loader = inference_data_loader(data_train, num_workers=0, batch_size=32)\nencoder.eval()\ntrain_embeds = None\n\nfor i, batch in tqdm(enumerate(train_loader)):\n    train_embeds_batch = encoder(batch.to(device))\n    if i == 0:\n        train_embeds = train_embeds_batch.detach().cpu().numpy()\n    else:\n        train_embeds = np.concatenate([train_embeds, train_embeds_batch.detach().cpu().numpy()], axis=0)\n    \ntrain_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:34:04.694226Z","iopub.execute_input":"2025-05-09T16:34:04.694518Z","iopub.status.idle":"2025-05-09T16:34:12.333631Z","shell.execute_reply.started":"2025-05-09T16:34:04.694496Z","shell.execute_reply":"2025-05-09T16:34:12.332926Z"}},"outputs":[{"name":"stderr","text":"141it [00:07, 18.49it/s]\n","output_type":"stream"},{"execution_count":175,"output_type":"execute_result","data":{"text/plain":"array([[ 0.4763479 ,  0.12343363, -0.2737443 , ..., -0.4416202 ,\n         0.59304714,  0.15195346],\n       [-0.04151388,  0.6004812 , -0.40921816, ..., -0.3299001 ,\n         0.9126798 ,  0.4803991 ],\n       [ 0.24940437,  0.5722427 , -0.61975867, ..., -0.10690073,\n         0.8532574 ,  0.39472497],\n       ...,\n       [ 0.5277181 ,  0.7133706 , -0.69615996, ..., -0.5489958 ,\n         0.3946198 , -0.8759226 ],\n       [-0.33658493,  0.8712623 , -0.08205242, ..., -0.23512381,\n         0.9274619 , -0.706148  ],\n       [ 0.962061  ,  0.54293334, -0.8105549 , ..., -0.5115771 ,\n         0.9882774 , -0.5916329 ]], dtype=float32)"},"metadata":{}}],"execution_count":175},{"cell_type":"code","source":"test_loader = inference_data_loader(data_test, num_workers=0, batch_size=32)\nencoder.eval()\ntest_embeds = None\n\nfor i, batch in tqdm(enumerate(test_loader)):\n    test_embeds_batch = encoder(batch.to(device))\n    if i == 0:\n        test_embeds = test_embeds_batch.detach().cpu().numpy()\n    else:\n        test_embeds = np.concatenate([test_embeds, test_embeds_batch.detach().cpu().numpy()], axis=0)\n    \ntest_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:34:22.014514Z","iopub.execute_input":"2025-05-09T16:34:22.014819Z","iopub.status.idle":"2025-05-09T16:34:22.955872Z","shell.execute_reply.started":"2025-05-09T16:34:22.014795Z","shell.execute_reply":"2025-05-09T16:34:22.955197Z"}},"outputs":[{"name":"stderr","text":"16it [00:00, 17.20it/s]\n","output_type":"stream"},{"execution_count":176,"output_type":"execute_result","data":{"text/plain":"array([[-0.3532161 ,  0.44846803, -0.56362194, ..., -0.20272706,\n         0.82116485,  0.56089455],\n       [ 0.37977022,  0.35007468, -0.12814702, ...,  0.08900587,\n         0.43118227,  0.18706475],\n       [-0.04472409,  0.74503595, -0.05210087, ..., -0.28382245,\n         0.946484  ,  0.543487  ],\n       ...,\n       [ 0.34813413,  0.72305614, -0.42042637, ..., -0.36838108,\n         0.9422317 , -0.8351585 ],\n       [ 0.22155719,  0.68438727, -0.5866197 , ..., -0.616808  ,\n         0.05628479, -0.7080625 ],\n       [-0.6057585 ,  0.90392375,  0.05865079, ..., -0.43313178,\n         0.933504  , -0.82974684]], dtype=float32)"},"metadata":{}}],"execution_count":176},{"cell_type":"code","source":"clf = CatBoostClassifier(loss_function='MultiClass', task_type=\"GPU\", devices='0', random_state=42)\n\nclf.fit(train_embeds, target_train, plot_file=\"catboost_log.html\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:34:26.335572Z","iopub.execute_input":"2025-05-09T16:34:26.335861Z","iopub.status.idle":"2025-05-09T16:34:32.795074Z","shell.execute_reply.started":"2025-05-09T16:34:26.335838Z","shell.execute_reply":"2025-05-09T16:34:32.794216Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 6121.125 Total: 16269.25\n","output_type":"stream"},{"name":"stdout","text":"Learning rate set to 0.088214\n0:\tlearn: 0.6660331\ttotal: 11.2ms\tremaining: 11.2s\n1:\tlearn: 0.6423336\ttotal: 17.9ms\tremaining: 8.91s\n2:\tlearn: 0.6228551\ttotal: 24.4ms\tremaining: 8.1s\n3:\tlearn: 0.6049975\ttotal: 31ms\tremaining: 7.71s\n4:\tlearn: 0.5897903\ttotal: 37.5ms\tremaining: 7.47s\n5:\tlearn: 0.5760603\ttotal: 43.5ms\tremaining: 7.2s\n6:\tlearn: 0.5633280\ttotal: 49.5ms\tremaining: 7.02s\n7:\tlearn: 0.5520736\ttotal: 55.5ms\tremaining: 6.88s\n8:\tlearn: 0.5420958\ttotal: 61.4ms\tremaining: 6.76s\n9:\tlearn: 0.5325355\ttotal: 67.5ms\tremaining: 6.68s\n10:\tlearn: 0.5252810\ttotal: 73.6ms\tremaining: 6.62s\n11:\tlearn: 0.5187271\ttotal: 79.7ms\tremaining: 6.56s\n12:\tlearn: 0.5118655\ttotal: 85.7ms\tremaining: 6.51s\n13:\tlearn: 0.5056080\ttotal: 91.8ms\tremaining: 6.46s\n14:\tlearn: 0.4996784\ttotal: 97.8ms\tremaining: 6.42s\n15:\tlearn: 0.4941885\ttotal: 104ms\tremaining: 6.38s\n16:\tlearn: 0.4892911\ttotal: 110ms\tremaining: 6.35s\n17:\tlearn: 0.4856222\ttotal: 116ms\tremaining: 6.31s\n18:\tlearn: 0.4816863\ttotal: 122ms\tremaining: 6.28s\n19:\tlearn: 0.4778848\ttotal: 127ms\tremaining: 6.25s\n20:\tlearn: 0.4740489\ttotal: 133ms\tremaining: 6.21s\n21:\tlearn: 0.4709303\ttotal: 139ms\tremaining: 6.19s\n22:\tlearn: 0.4680999\ttotal: 145ms\tremaining: 6.16s\n23:\tlearn: 0.4656348\ttotal: 151ms\tremaining: 6.14s\n24:\tlearn: 0.4632983\ttotal: 157ms\tremaining: 6.11s\n25:\tlearn: 0.4611283\ttotal: 163ms\tremaining: 6.1s\n26:\tlearn: 0.4589258\ttotal: 169ms\tremaining: 6.1s\n27:\tlearn: 0.4566490\ttotal: 176ms\tremaining: 6.1s\n28:\tlearn: 0.4545390\ttotal: 182ms\tremaining: 6.09s\n29:\tlearn: 0.4524510\ttotal: 188ms\tremaining: 6.09s\n30:\tlearn: 0.4503083\ttotal: 195ms\tremaining: 6.09s\n31:\tlearn: 0.4486586\ttotal: 201ms\tremaining: 6.08s\n32:\tlearn: 0.4468134\ttotal: 208ms\tremaining: 6.08s\n33:\tlearn: 0.4448933\ttotal: 214ms\tremaining: 6.08s\n34:\tlearn: 0.4432499\ttotal: 220ms\tremaining: 6.07s\n35:\tlearn: 0.4417419\ttotal: 226ms\tremaining: 6.07s\n36:\tlearn: 0.4397518\ttotal: 233ms\tremaining: 6.06s\n37:\tlearn: 0.4380324\ttotal: 239ms\tremaining: 6.06s\n38:\tlearn: 0.4359514\ttotal: 246ms\tremaining: 6.05s\n39:\tlearn: 0.4342749\ttotal: 252ms\tremaining: 6.05s\n40:\tlearn: 0.4328937\ttotal: 258ms\tremaining: 6.04s\n41:\tlearn: 0.4316036\ttotal: 264ms\tremaining: 6.03s\n42:\tlearn: 0.4303071\ttotal: 271ms\tremaining: 6.03s\n43:\tlearn: 0.4289886\ttotal: 277ms\tremaining: 6.02s\n44:\tlearn: 0.4276726\ttotal: 283ms\tremaining: 6.01s\n45:\tlearn: 0.4266735\ttotal: 290ms\tremaining: 6.01s\n46:\tlearn: 0.4254578\ttotal: 296ms\tremaining: 6s\n47:\tlearn: 0.4241400\ttotal: 302ms\tremaining: 6s\n48:\tlearn: 0.4231749\ttotal: 309ms\tremaining: 5.99s\n49:\tlearn: 0.4220614\ttotal: 315ms\tremaining: 5.98s\n50:\tlearn: 0.4207280\ttotal: 321ms\tremaining: 5.97s\n51:\tlearn: 0.4193716\ttotal: 327ms\tremaining: 5.97s\n52:\tlearn: 0.4176733\ttotal: 334ms\tremaining: 5.96s\n53:\tlearn: 0.4163722\ttotal: 340ms\tremaining: 5.96s\n54:\tlearn: 0.4153045\ttotal: 346ms\tremaining: 5.95s\n55:\tlearn: 0.4143576\ttotal: 352ms\tremaining: 5.94s\n56:\tlearn: 0.4130552\ttotal: 359ms\tremaining: 5.93s\n57:\tlearn: 0.4118499\ttotal: 365ms\tremaining: 5.93s\n58:\tlearn: 0.4107929\ttotal: 372ms\tremaining: 5.92s\n59:\tlearn: 0.4093640\ttotal: 378ms\tremaining: 5.92s\n60:\tlearn: 0.4082918\ttotal: 384ms\tremaining: 5.91s\n61:\tlearn: 0.4075731\ttotal: 390ms\tremaining: 5.9s\n62:\tlearn: 0.4061325\ttotal: 396ms\tremaining: 5.89s\n63:\tlearn: 0.4053207\ttotal: 402ms\tremaining: 5.88s\n64:\tlearn: 0.4043263\ttotal: 409ms\tremaining: 5.88s\n65:\tlearn: 0.4032248\ttotal: 415ms\tremaining: 5.87s\n66:\tlearn: 0.4022723\ttotal: 421ms\tremaining: 5.86s\n67:\tlearn: 0.4014544\ttotal: 427ms\tremaining: 5.86s\n68:\tlearn: 0.4005547\ttotal: 433ms\tremaining: 5.85s\n69:\tlearn: 0.3997515\ttotal: 439ms\tremaining: 5.84s\n70:\tlearn: 0.3990509\ttotal: 445ms\tremaining: 5.83s\n71:\tlearn: 0.3978831\ttotal: 452ms\tremaining: 5.82s\n72:\tlearn: 0.3967685\ttotal: 458ms\tremaining: 5.81s\n73:\tlearn: 0.3957341\ttotal: 464ms\tremaining: 5.81s\n74:\tlearn: 0.3948279\ttotal: 470ms\tremaining: 5.8s\n75:\tlearn: 0.3940562\ttotal: 476ms\tremaining: 5.79s\n76:\tlearn: 0.3929671\ttotal: 483ms\tremaining: 5.78s\n77:\tlearn: 0.3919199\ttotal: 489ms\tremaining: 5.78s\n78:\tlearn: 0.3910792\ttotal: 495ms\tremaining: 5.77s\n79:\tlearn: 0.3901509\ttotal: 501ms\tremaining: 5.76s\n80:\tlearn: 0.3891474\ttotal: 508ms\tremaining: 5.76s\n81:\tlearn: 0.3879161\ttotal: 514ms\tremaining: 5.75s\n82:\tlearn: 0.3869291\ttotal: 520ms\tremaining: 5.75s\n83:\tlearn: 0.3861208\ttotal: 526ms\tremaining: 5.74s\n84:\tlearn: 0.3853165\ttotal: 532ms\tremaining: 5.73s\n85:\tlearn: 0.3842754\ttotal: 539ms\tremaining: 5.72s\n86:\tlearn: 0.3836186\ttotal: 545ms\tremaining: 5.72s\n87:\tlearn: 0.3828204\ttotal: 551ms\tremaining: 5.71s\n88:\tlearn: 0.3821874\ttotal: 557ms\tremaining: 5.7s\n89:\tlearn: 0.3809640\ttotal: 563ms\tremaining: 5.7s\n90:\tlearn: 0.3794704\ttotal: 570ms\tremaining: 5.69s\n91:\tlearn: 0.3784026\ttotal: 576ms\tremaining: 5.69s\n92:\tlearn: 0.3775457\ttotal: 582ms\tremaining: 5.68s\n93:\tlearn: 0.3768766\ttotal: 588ms\tremaining: 5.67s\n94:\tlearn: 0.3760811\ttotal: 595ms\tremaining: 5.67s\n95:\tlearn: 0.3751117\ttotal: 601ms\tremaining: 5.66s\n96:\tlearn: 0.3742097\ttotal: 607ms\tremaining: 5.65s\n97:\tlearn: 0.3731061\ttotal: 614ms\tremaining: 5.65s\n98:\tlearn: 0.3721821\ttotal: 620ms\tremaining: 5.64s\n99:\tlearn: 0.3713041\ttotal: 626ms\tremaining: 5.63s\n100:\tlearn: 0.3703518\ttotal: 632ms\tremaining: 5.63s\n101:\tlearn: 0.3697065\ttotal: 639ms\tremaining: 5.62s\n102:\tlearn: 0.3686625\ttotal: 645ms\tremaining: 5.61s\n103:\tlearn: 0.3678517\ttotal: 651ms\tremaining: 5.61s\n104:\tlearn: 0.3668814\ttotal: 657ms\tremaining: 5.6s\n105:\tlearn: 0.3659539\ttotal: 663ms\tremaining: 5.59s\n106:\tlearn: 0.3653364\ttotal: 668ms\tremaining: 5.58s\n107:\tlearn: 0.3645595\ttotal: 674ms\tremaining: 5.57s\n108:\tlearn: 0.3640039\ttotal: 680ms\tremaining: 5.56s\n109:\tlearn: 0.3630343\ttotal: 686ms\tremaining: 5.55s\n110:\tlearn: 0.3623206\ttotal: 692ms\tremaining: 5.55s\n111:\tlearn: 0.3613874\ttotal: 698ms\tremaining: 5.54s\n112:\tlearn: 0.3606262\ttotal: 704ms\tremaining: 5.53s\n113:\tlearn: 0.3599862\ttotal: 710ms\tremaining: 5.52s\n114:\tlearn: 0.3590270\ttotal: 716ms\tremaining: 5.51s\n115:\tlearn: 0.3584517\ttotal: 722ms\tremaining: 5.5s\n116:\tlearn: 0.3575720\ttotal: 728ms\tremaining: 5.49s\n117:\tlearn: 0.3568347\ttotal: 734ms\tremaining: 5.49s\n118:\tlearn: 0.3560580\ttotal: 740ms\tremaining: 5.48s\n119:\tlearn: 0.3550297\ttotal: 746ms\tremaining: 5.47s\n120:\tlearn: 0.3541638\ttotal: 752ms\tremaining: 5.46s\n121:\tlearn: 0.3532766\ttotal: 758ms\tremaining: 5.45s\n122:\tlearn: 0.3525119\ttotal: 764ms\tremaining: 5.44s\n123:\tlearn: 0.3518944\ttotal: 770ms\tremaining: 5.44s\n124:\tlearn: 0.3506478\ttotal: 776ms\tremaining: 5.43s\n125:\tlearn: 0.3502469\ttotal: 782ms\tremaining: 5.42s\n126:\tlearn: 0.3498654\ttotal: 788ms\tremaining: 5.42s\n127:\tlearn: 0.3492625\ttotal: 794ms\tremaining: 5.41s\n128:\tlearn: 0.3487121\ttotal: 801ms\tremaining: 5.41s\n129:\tlearn: 0.3481663\ttotal: 806ms\tremaining: 5.39s\n130:\tlearn: 0.3474340\ttotal: 812ms\tremaining: 5.39s\n131:\tlearn: 0.3467006\ttotal: 819ms\tremaining: 5.38s\n132:\tlearn: 0.3458465\ttotal: 825ms\tremaining: 5.38s\n133:\tlearn: 0.3454153\ttotal: 830ms\tremaining: 5.37s\n134:\tlearn: 0.3443397\ttotal: 836ms\tremaining: 5.36s\n135:\tlearn: 0.3436407\ttotal: 842ms\tremaining: 5.35s\n136:\tlearn: 0.3429957\ttotal: 848ms\tremaining: 5.34s\n137:\tlearn: 0.3423402\ttotal: 853ms\tremaining: 5.33s\n138:\tlearn: 0.3416735\ttotal: 859ms\tremaining: 5.32s\n139:\tlearn: 0.3411668\ttotal: 865ms\tremaining: 5.31s\n140:\tlearn: 0.3405494\ttotal: 870ms\tremaining: 5.3s\n141:\tlearn: 0.3398773\ttotal: 876ms\tremaining: 5.29s\n142:\tlearn: 0.3392022\ttotal: 882ms\tremaining: 5.28s\n143:\tlearn: 0.3385165\ttotal: 887ms\tremaining: 5.27s\n144:\tlearn: 0.3379562\ttotal: 893ms\tremaining: 5.26s\n145:\tlearn: 0.3374038\ttotal: 898ms\tremaining: 5.25s\n146:\tlearn: 0.3362825\ttotal: 904ms\tremaining: 5.25s\n147:\tlearn: 0.3355646\ttotal: 910ms\tremaining: 5.24s\n148:\tlearn: 0.3348729\ttotal: 916ms\tremaining: 5.23s\n149:\tlearn: 0.3340759\ttotal: 922ms\tremaining: 5.22s\n150:\tlearn: 0.3336466\ttotal: 927ms\tremaining: 5.21s\n151:\tlearn: 0.3331277\ttotal: 933ms\tremaining: 5.2s\n152:\tlearn: 0.3322955\ttotal: 939ms\tremaining: 5.2s\n153:\tlearn: 0.3316527\ttotal: 944ms\tremaining: 5.19s\n154:\tlearn: 0.3310076\ttotal: 950ms\tremaining: 5.18s\n155:\tlearn: 0.3303384\ttotal: 956ms\tremaining: 5.17s\n156:\tlearn: 0.3297127\ttotal: 961ms\tremaining: 5.16s\n157:\tlearn: 0.3289702\ttotal: 968ms\tremaining: 5.16s\n158:\tlearn: 0.3283709\ttotal: 973ms\tremaining: 5.15s\n159:\tlearn: 0.3277344\ttotal: 979ms\tremaining: 5.14s\n160:\tlearn: 0.3270755\ttotal: 985ms\tremaining: 5.13s\n161:\tlearn: 0.3265301\ttotal: 991ms\tremaining: 5.13s\n162:\tlearn: 0.3257274\ttotal: 997ms\tremaining: 5.12s\n163:\tlearn: 0.3250454\ttotal: 1s\tremaining: 5.12s\n164:\tlearn: 0.3243777\ttotal: 1.01s\tremaining: 5.11s\n165:\tlearn: 0.3237772\ttotal: 1.01s\tremaining: 5.1s\n166:\tlearn: 0.3232714\ttotal: 1.02s\tremaining: 5.1s\n167:\tlearn: 0.3228907\ttotal: 1.03s\tremaining: 5.09s\n168:\tlearn: 0.3222701\ttotal: 1.03s\tremaining: 5.09s\n169:\tlearn: 0.3216759\ttotal: 1.04s\tremaining: 5.08s\n170:\tlearn: 0.3212396\ttotal: 1.05s\tremaining: 5.08s\n171:\tlearn: 0.3207684\ttotal: 1.05s\tremaining: 5.07s\n172:\tlearn: 0.3200651\ttotal: 1.06s\tremaining: 5.07s\n173:\tlearn: 0.3196348\ttotal: 1.07s\tremaining: 5.06s\n174:\tlearn: 0.3190046\ttotal: 1.07s\tremaining: 5.06s\n175:\tlearn: 0.3184057\ttotal: 1.08s\tremaining: 5.05s\n176:\tlearn: 0.3179328\ttotal: 1.08s\tremaining: 5.05s\n177:\tlearn: 0.3172592\ttotal: 1.09s\tremaining: 5.04s\n178:\tlearn: 0.3165755\ttotal: 1.1s\tremaining: 5.04s\n179:\tlearn: 0.3160100\ttotal: 1.1s\tremaining: 5.03s\n180:\tlearn: 0.3153542\ttotal: 1.11s\tremaining: 5.03s\n181:\tlearn: 0.3147111\ttotal: 1.12s\tremaining: 5.02s\n182:\tlearn: 0.3142138\ttotal: 1.12s\tremaining: 5.02s\n183:\tlearn: 0.3137435\ttotal: 1.13s\tremaining: 5.01s\n184:\tlearn: 0.3132429\ttotal: 1.14s\tremaining: 5.01s\n185:\tlearn: 0.3128194\ttotal: 1.14s\tremaining: 5s\n186:\tlearn: 0.3122730\ttotal: 1.15s\tremaining: 5s\n187:\tlearn: 0.3116938\ttotal: 1.16s\tremaining: 4.99s\n188:\tlearn: 0.3109048\ttotal: 1.16s\tremaining: 4.99s\n189:\tlearn: 0.3103434\ttotal: 1.17s\tremaining: 4.98s\n190:\tlearn: 0.3098084\ttotal: 1.17s\tremaining: 4.98s\n191:\tlearn: 0.3092686\ttotal: 1.18s\tremaining: 4.97s\n192:\tlearn: 0.3086915\ttotal: 1.19s\tremaining: 4.97s\n193:\tlearn: 0.3079702\ttotal: 1.19s\tremaining: 4.96s\n194:\tlearn: 0.3074956\ttotal: 1.2s\tremaining: 4.95s\n195:\tlearn: 0.3064564\ttotal: 1.21s\tremaining: 4.95s\n196:\tlearn: 0.3056824\ttotal: 1.21s\tremaining: 4.94s\n197:\tlearn: 0.3052692\ttotal: 1.22s\tremaining: 4.93s\n198:\tlearn: 0.3043973\ttotal: 1.22s\tremaining: 4.93s\n199:\tlearn: 0.3034347\ttotal: 1.23s\tremaining: 4.92s\n200:\tlearn: 0.3028512\ttotal: 1.24s\tremaining: 4.92s\n201:\tlearn: 0.3023243\ttotal: 1.24s\tremaining: 4.91s\n202:\tlearn: 0.3018261\ttotal: 1.25s\tremaining: 4.91s\n203:\tlearn: 0.3014701\ttotal: 1.25s\tremaining: 4.9s\n204:\tlearn: 0.3010256\ttotal: 1.26s\tremaining: 4.89s\n205:\tlearn: 0.3003205\ttotal: 1.27s\tremaining: 4.88s\n206:\tlearn: 0.2996021\ttotal: 1.27s\tremaining: 4.88s\n207:\tlearn: 0.2988517\ttotal: 1.28s\tremaining: 4.87s\n208:\tlearn: 0.2983563\ttotal: 1.29s\tremaining: 4.87s\n209:\tlearn: 0.2978917\ttotal: 1.29s\tremaining: 4.86s\n210:\tlearn: 0.2971863\ttotal: 1.3s\tremaining: 4.86s\n211:\tlearn: 0.2967227\ttotal: 1.3s\tremaining: 4.85s\n212:\tlearn: 0.2960517\ttotal: 1.31s\tremaining: 4.84s\n213:\tlearn: 0.2955913\ttotal: 1.32s\tremaining: 4.84s\n214:\tlearn: 0.2951902\ttotal: 1.32s\tremaining: 4.83s\n215:\tlearn: 0.2945892\ttotal: 1.33s\tremaining: 4.83s\n216:\tlearn: 0.2939147\ttotal: 1.33s\tremaining: 4.82s\n217:\tlearn: 0.2933510\ttotal: 1.34s\tremaining: 4.81s\n218:\tlearn: 0.2925947\ttotal: 1.35s\tremaining: 4.81s\n219:\tlearn: 0.2920938\ttotal: 1.35s\tremaining: 4.8s\n220:\tlearn: 0.2916292\ttotal: 1.36s\tremaining: 4.79s\n221:\tlearn: 0.2910856\ttotal: 1.37s\tremaining: 4.79s\n222:\tlearn: 0.2907056\ttotal: 1.37s\tremaining: 4.78s\n223:\tlearn: 0.2901013\ttotal: 1.38s\tremaining: 4.77s\n224:\tlearn: 0.2894215\ttotal: 1.38s\tremaining: 4.76s\n225:\tlearn: 0.2887394\ttotal: 1.39s\tremaining: 4.76s\n226:\tlearn: 0.2883539\ttotal: 1.4s\tremaining: 4.75s\n227:\tlearn: 0.2879423\ttotal: 1.4s\tremaining: 4.74s\n228:\tlearn: 0.2870186\ttotal: 1.41s\tremaining: 4.74s\n229:\tlearn: 0.2865210\ttotal: 1.41s\tremaining: 4.73s\n230:\tlearn: 0.2860724\ttotal: 1.42s\tremaining: 4.72s\n231:\tlearn: 0.2855297\ttotal: 1.42s\tremaining: 4.71s\n232:\tlearn: 0.2849430\ttotal: 1.43s\tremaining: 4.7s\n233:\tlearn: 0.2842864\ttotal: 1.43s\tremaining: 4.7s\n234:\tlearn: 0.2837728\ttotal: 1.44s\tremaining: 4.69s\n235:\tlearn: 0.2833359\ttotal: 1.45s\tremaining: 4.68s\n236:\tlearn: 0.2828241\ttotal: 1.45s\tremaining: 4.67s\n237:\tlearn: 0.2822596\ttotal: 1.46s\tremaining: 4.67s\n238:\tlearn: 0.2816044\ttotal: 1.46s\tremaining: 4.66s\n239:\tlearn: 0.2812894\ttotal: 1.47s\tremaining: 4.65s\n240:\tlearn: 0.2809426\ttotal: 1.48s\tremaining: 4.64s\n241:\tlearn: 0.2803846\ttotal: 1.48s\tremaining: 4.64s\n242:\tlearn: 0.2800206\ttotal: 1.49s\tremaining: 4.63s\n243:\tlearn: 0.2795992\ttotal: 1.49s\tremaining: 4.62s\n244:\tlearn: 0.2790111\ttotal: 1.5s\tremaining: 4.62s\n245:\tlearn: 0.2786704\ttotal: 1.5s\tremaining: 4.61s\n246:\tlearn: 0.2782132\ttotal: 1.51s\tremaining: 4.6s\n247:\tlearn: 0.2778520\ttotal: 1.51s\tremaining: 4.59s\n248:\tlearn: 0.2774521\ttotal: 1.52s\tremaining: 4.59s\n249:\tlearn: 0.2769162\ttotal: 1.53s\tremaining: 4.58s\n250:\tlearn: 0.2764787\ttotal: 1.53s\tremaining: 4.57s\n251:\tlearn: 0.2760644\ttotal: 1.54s\tremaining: 4.57s\n252:\tlearn: 0.2755329\ttotal: 1.54s\tremaining: 4.56s\n253:\tlearn: 0.2750612\ttotal: 1.55s\tremaining: 4.55s\n254:\tlearn: 0.2745595\ttotal: 1.56s\tremaining: 4.55s\n255:\tlearn: 0.2739575\ttotal: 1.56s\tremaining: 4.54s\n256:\tlearn: 0.2733032\ttotal: 1.57s\tremaining: 4.54s\n257:\tlearn: 0.2728725\ttotal: 1.57s\tremaining: 4.53s\n258:\tlearn: 0.2723909\ttotal: 1.58s\tremaining: 4.52s\n259:\tlearn: 0.2721465\ttotal: 1.59s\tremaining: 4.51s\n260:\tlearn: 0.2715187\ttotal: 1.59s\tremaining: 4.51s\n261:\tlearn: 0.2710756\ttotal: 1.6s\tremaining: 4.5s\n262:\tlearn: 0.2706258\ttotal: 1.6s\tremaining: 4.49s\n263:\tlearn: 0.2702052\ttotal: 1.61s\tremaining: 4.49s\n264:\tlearn: 0.2697668\ttotal: 1.61s\tremaining: 4.48s\n265:\tlearn: 0.2693012\ttotal: 1.62s\tremaining: 4.47s\n266:\tlearn: 0.2686473\ttotal: 1.63s\tremaining: 4.47s\n267:\tlearn: 0.2681478\ttotal: 1.63s\tremaining: 4.46s\n268:\tlearn: 0.2676262\ttotal: 1.64s\tremaining: 4.45s\n269:\tlearn: 0.2669699\ttotal: 1.64s\tremaining: 4.45s\n270:\tlearn: 0.2661852\ttotal: 1.65s\tremaining: 4.44s\n271:\tlearn: 0.2657697\ttotal: 1.66s\tremaining: 4.43s\n272:\tlearn: 0.2654924\ttotal: 1.66s\tremaining: 4.43s\n273:\tlearn: 0.2647393\ttotal: 1.67s\tremaining: 4.42s\n274:\tlearn: 0.2642508\ttotal: 1.67s\tremaining: 4.41s\n275:\tlearn: 0.2635603\ttotal: 1.68s\tremaining: 4.41s\n276:\tlearn: 0.2631045\ttotal: 1.69s\tremaining: 4.4s\n277:\tlearn: 0.2623806\ttotal: 1.69s\tremaining: 4.39s\n278:\tlearn: 0.2618920\ttotal: 1.7s\tremaining: 4.39s\n279:\tlearn: 0.2612325\ttotal: 1.7s\tremaining: 4.38s\n280:\tlearn: 0.2606954\ttotal: 1.71s\tremaining: 4.37s\n281:\tlearn: 0.2603136\ttotal: 1.72s\tremaining: 4.37s\n282:\tlearn: 0.2599209\ttotal: 1.72s\tremaining: 4.36s\n283:\tlearn: 0.2594679\ttotal: 1.73s\tremaining: 4.35s\n284:\tlearn: 0.2590117\ttotal: 1.73s\tremaining: 4.35s\n285:\tlearn: 0.2582629\ttotal: 1.74s\tremaining: 4.34s\n286:\tlearn: 0.2576387\ttotal: 1.74s\tremaining: 4.33s\n287:\tlearn: 0.2573902\ttotal: 1.75s\tremaining: 4.33s\n288:\tlearn: 0.2569188\ttotal: 1.76s\tremaining: 4.32s\n289:\tlearn: 0.2564717\ttotal: 1.76s\tremaining: 4.31s\n290:\tlearn: 0.2560016\ttotal: 1.77s\tremaining: 4.31s\n291:\tlearn: 0.2556180\ttotal: 1.77s\tremaining: 4.3s\n292:\tlearn: 0.2552618\ttotal: 1.78s\tremaining: 4.29s\n293:\tlearn: 0.2547935\ttotal: 1.78s\tremaining: 4.29s\n294:\tlearn: 0.2544895\ttotal: 1.79s\tremaining: 4.28s\n295:\tlearn: 0.2539338\ttotal: 1.8s\tremaining: 4.27s\n296:\tlearn: 0.2534106\ttotal: 1.8s\tremaining: 4.27s\n297:\tlearn: 0.2529324\ttotal: 1.81s\tremaining: 4.26s\n298:\tlearn: 0.2525821\ttotal: 1.81s\tremaining: 4.25s\n299:\tlearn: 0.2520991\ttotal: 1.82s\tremaining: 4.25s\n300:\tlearn: 0.2516673\ttotal: 1.82s\tremaining: 4.24s\n301:\tlearn: 0.2513278\ttotal: 1.83s\tremaining: 4.23s\n302:\tlearn: 0.2510186\ttotal: 1.84s\tremaining: 4.22s\n303:\tlearn: 0.2504332\ttotal: 1.84s\tremaining: 4.22s\n304:\tlearn: 0.2499075\ttotal: 1.85s\tremaining: 4.21s\n305:\tlearn: 0.2494717\ttotal: 1.85s\tremaining: 4.2s\n306:\tlearn: 0.2489722\ttotal: 1.86s\tremaining: 4.2s\n307:\tlearn: 0.2484039\ttotal: 1.86s\tremaining: 4.19s\n308:\tlearn: 0.2480511\ttotal: 1.87s\tremaining: 4.18s\n309:\tlearn: 0.2475212\ttotal: 1.88s\tremaining: 4.18s\n310:\tlearn: 0.2471344\ttotal: 1.88s\tremaining: 4.17s\n311:\tlearn: 0.2468318\ttotal: 1.89s\tremaining: 4.16s\n312:\tlearn: 0.2464691\ttotal: 1.89s\tremaining: 4.16s\n313:\tlearn: 0.2459174\ttotal: 1.9s\tremaining: 4.15s\n314:\tlearn: 0.2453764\ttotal: 1.9s\tremaining: 4.14s\n315:\tlearn: 0.2448423\ttotal: 1.91s\tremaining: 4.13s\n316:\tlearn: 0.2445433\ttotal: 1.92s\tremaining: 4.13s\n317:\tlearn: 0.2441141\ttotal: 1.92s\tremaining: 4.12s\n318:\tlearn: 0.2437756\ttotal: 1.93s\tremaining: 4.11s\n319:\tlearn: 0.2434857\ttotal: 1.93s\tremaining: 4.11s\n320:\tlearn: 0.2431460\ttotal: 1.94s\tremaining: 4.1s\n321:\tlearn: 0.2426794\ttotal: 1.94s\tremaining: 4.09s\n322:\tlearn: 0.2420446\ttotal: 1.95s\tremaining: 4.08s\n323:\tlearn: 0.2416459\ttotal: 1.96s\tremaining: 4.08s\n324:\tlearn: 0.2414189\ttotal: 1.96s\tremaining: 4.07s\n325:\tlearn: 0.2410670\ttotal: 1.97s\tremaining: 4.07s\n326:\tlearn: 0.2404432\ttotal: 1.97s\tremaining: 4.06s\n327:\tlearn: 0.2400974\ttotal: 1.98s\tremaining: 4.05s\n328:\tlearn: 0.2395538\ttotal: 1.99s\tremaining: 4.05s\n329:\tlearn: 0.2391507\ttotal: 1.99s\tremaining: 4.04s\n330:\tlearn: 0.2387472\ttotal: 2s\tremaining: 4.04s\n331:\tlearn: 0.2382822\ttotal: 2s\tremaining: 4.03s\n332:\tlearn: 0.2379912\ttotal: 2.01s\tremaining: 4.03s\n333:\tlearn: 0.2376433\ttotal: 2.02s\tremaining: 4.02s\n334:\tlearn: 0.2371420\ttotal: 2.02s\tremaining: 4.02s\n335:\tlearn: 0.2367324\ttotal: 2.03s\tremaining: 4.01s\n336:\tlearn: 0.2364466\ttotal: 2.04s\tremaining: 4s\n337:\tlearn: 0.2360782\ttotal: 2.04s\tremaining: 4s\n338:\tlearn: 0.2355125\ttotal: 2.05s\tremaining: 3.99s\n339:\tlearn: 0.2351455\ttotal: 2.05s\tremaining: 3.98s\n340:\tlearn: 0.2349436\ttotal: 2.06s\tremaining: 3.98s\n341:\tlearn: 0.2346869\ttotal: 2.06s\tremaining: 3.97s\n342:\tlearn: 0.2344796\ttotal: 2.07s\tremaining: 3.96s\n343:\tlearn: 0.2339947\ttotal: 2.08s\tremaining: 3.96s\n344:\tlearn: 0.2337329\ttotal: 2.08s\tremaining: 3.95s\n345:\tlearn: 0.2335115\ttotal: 2.09s\tremaining: 3.95s\n346:\tlearn: 0.2330251\ttotal: 2.09s\tremaining: 3.94s\n347:\tlearn: 0.2326001\ttotal: 2.1s\tremaining: 3.93s\n348:\tlearn: 0.2320667\ttotal: 2.11s\tremaining: 3.93s\n349:\tlearn: 0.2318800\ttotal: 2.11s\tremaining: 3.92s\n350:\tlearn: 0.2315957\ttotal: 2.12s\tremaining: 3.92s\n351:\tlearn: 0.2311031\ttotal: 2.12s\tremaining: 3.91s\n352:\tlearn: 0.2306258\ttotal: 2.13s\tremaining: 3.9s\n353:\tlearn: 0.2302897\ttotal: 2.14s\tremaining: 3.9s\n354:\tlearn: 0.2299797\ttotal: 2.14s\tremaining: 3.89s\n355:\tlearn: 0.2295251\ttotal: 2.15s\tremaining: 3.88s\n356:\tlearn: 0.2290868\ttotal: 2.15s\tremaining: 3.88s\n357:\tlearn: 0.2286478\ttotal: 2.16s\tremaining: 3.87s\n358:\tlearn: 0.2282032\ttotal: 2.17s\tremaining: 3.87s\n359:\tlearn: 0.2278836\ttotal: 2.17s\tremaining: 3.86s\n360:\tlearn: 0.2276570\ttotal: 2.18s\tremaining: 3.85s\n361:\tlearn: 0.2272640\ttotal: 2.18s\tremaining: 3.85s\n362:\tlearn: 0.2269020\ttotal: 2.19s\tremaining: 3.84s\n363:\tlearn: 0.2266523\ttotal: 2.19s\tremaining: 3.83s\n364:\tlearn: 0.2262582\ttotal: 2.2s\tremaining: 3.83s\n365:\tlearn: 0.2256659\ttotal: 2.21s\tremaining: 3.82s\n366:\tlearn: 0.2253043\ttotal: 2.21s\tremaining: 3.81s\n367:\tlearn: 0.2250281\ttotal: 2.22s\tremaining: 3.81s\n368:\tlearn: 0.2245415\ttotal: 2.22s\tremaining: 3.8s\n369:\tlearn: 0.2241532\ttotal: 2.23s\tremaining: 3.8s\n370:\tlearn: 0.2239274\ttotal: 2.24s\tremaining: 3.79s\n371:\tlearn: 0.2236829\ttotal: 2.24s\tremaining: 3.79s\n372:\tlearn: 0.2232738\ttotal: 2.25s\tremaining: 3.78s\n373:\tlearn: 0.2230082\ttotal: 2.25s\tremaining: 3.77s\n374:\tlearn: 0.2225063\ttotal: 2.26s\tremaining: 3.77s\n375:\tlearn: 0.2220300\ttotal: 2.27s\tremaining: 3.76s\n376:\tlearn: 0.2215569\ttotal: 2.27s\tremaining: 3.76s\n377:\tlearn: 0.2212356\ttotal: 2.28s\tremaining: 3.75s\n378:\tlearn: 0.2209819\ttotal: 2.29s\tremaining: 3.75s\n379:\tlearn: 0.2205702\ttotal: 2.29s\tremaining: 3.74s\n380:\tlearn: 0.2202002\ttotal: 2.3s\tremaining: 3.73s\n381:\tlearn: 0.2198605\ttotal: 2.3s\tremaining: 3.73s\n382:\tlearn: 0.2192885\ttotal: 2.31s\tremaining: 3.72s\n383:\tlearn: 0.2188969\ttotal: 2.31s\tremaining: 3.71s\n384:\tlearn: 0.2185155\ttotal: 2.32s\tremaining: 3.71s\n385:\tlearn: 0.2182679\ttotal: 2.33s\tremaining: 3.7s\n386:\tlearn: 0.2180122\ttotal: 2.33s\tremaining: 3.69s\n387:\tlearn: 0.2175752\ttotal: 2.34s\tremaining: 3.69s\n388:\tlearn: 0.2171524\ttotal: 2.34s\tremaining: 3.68s\n389:\tlearn: 0.2166016\ttotal: 2.35s\tremaining: 3.67s\n390:\tlearn: 0.2161433\ttotal: 2.35s\tremaining: 3.67s\n391:\tlearn: 0.2157800\ttotal: 2.36s\tremaining: 3.66s\n392:\tlearn: 0.2154523\ttotal: 2.37s\tremaining: 3.66s\n393:\tlearn: 0.2149574\ttotal: 2.37s\tremaining: 3.65s\n394:\tlearn: 0.2146860\ttotal: 2.38s\tremaining: 3.65s\n395:\tlearn: 0.2142576\ttotal: 2.39s\tremaining: 3.64s\n396:\tlearn: 0.2138092\ttotal: 2.39s\tremaining: 3.63s\n397:\tlearn: 0.2135758\ttotal: 2.4s\tremaining: 3.63s\n398:\tlearn: 0.2130927\ttotal: 2.4s\tremaining: 3.62s\n399:\tlearn: 0.2127709\ttotal: 2.41s\tremaining: 3.61s\n400:\tlearn: 0.2125397\ttotal: 2.42s\tremaining: 3.61s\n401:\tlearn: 0.2121056\ttotal: 2.42s\tremaining: 3.6s\n402:\tlearn: 0.2117983\ttotal: 2.43s\tremaining: 3.6s\n403:\tlearn: 0.2112823\ttotal: 2.43s\tremaining: 3.59s\n404:\tlearn: 0.2109883\ttotal: 2.44s\tremaining: 3.58s\n405:\tlearn: 0.2107642\ttotal: 2.44s\tremaining: 3.58s\n406:\tlearn: 0.2105001\ttotal: 2.45s\tremaining: 3.57s\n407:\tlearn: 0.2101158\ttotal: 2.46s\tremaining: 3.56s\n408:\tlearn: 0.2095894\ttotal: 2.46s\tremaining: 3.56s\n409:\tlearn: 0.2093126\ttotal: 2.47s\tremaining: 3.55s\n410:\tlearn: 0.2090210\ttotal: 2.47s\tremaining: 3.54s\n411:\tlearn: 0.2088209\ttotal: 2.48s\tremaining: 3.54s\n412:\tlearn: 0.2084180\ttotal: 2.48s\tremaining: 3.53s\n413:\tlearn: 0.2081778\ttotal: 2.49s\tremaining: 3.52s\n414:\tlearn: 0.2079909\ttotal: 2.5s\tremaining: 3.52s\n415:\tlearn: 0.2074878\ttotal: 2.5s\tremaining: 3.51s\n416:\tlearn: 0.2071590\ttotal: 2.51s\tremaining: 3.5s\n417:\tlearn: 0.2067028\ttotal: 2.51s\tremaining: 3.5s\n418:\tlearn: 0.2064127\ttotal: 2.52s\tremaining: 3.49s\n419:\tlearn: 0.2059878\ttotal: 2.52s\tremaining: 3.49s\n420:\tlearn: 0.2056380\ttotal: 2.53s\tremaining: 3.48s\n421:\tlearn: 0.2052628\ttotal: 2.54s\tremaining: 3.47s\n422:\tlearn: 0.2049078\ttotal: 2.54s\tremaining: 3.47s\n423:\tlearn: 0.2044920\ttotal: 2.55s\tremaining: 3.46s\n424:\tlearn: 0.2040864\ttotal: 2.56s\tremaining: 3.46s\n425:\tlearn: 0.2037905\ttotal: 2.56s\tremaining: 3.45s\n426:\tlearn: 0.2035575\ttotal: 2.57s\tremaining: 3.44s\n427:\tlearn: 0.2031331\ttotal: 2.57s\tremaining: 3.44s\n428:\tlearn: 0.2027825\ttotal: 2.58s\tremaining: 3.43s\n429:\tlearn: 0.2025383\ttotal: 2.58s\tremaining: 3.42s\n430:\tlearn: 0.2022660\ttotal: 2.59s\tremaining: 3.42s\n431:\tlearn: 0.2018322\ttotal: 2.6s\tremaining: 3.41s\n432:\tlearn: 0.2015521\ttotal: 2.6s\tremaining: 3.4s\n433:\tlearn: 0.2012731\ttotal: 2.61s\tremaining: 3.4s\n434:\tlearn: 0.2010291\ttotal: 2.61s\tremaining: 3.39s\n435:\tlearn: 0.2006432\ttotal: 2.62s\tremaining: 3.39s\n436:\tlearn: 0.2004486\ttotal: 2.62s\tremaining: 3.38s\n437:\tlearn: 0.2002456\ttotal: 2.63s\tremaining: 3.37s\n438:\tlearn: 0.1998982\ttotal: 2.63s\tremaining: 3.37s\n439:\tlearn: 0.1997024\ttotal: 2.64s\tremaining: 3.36s\n440:\tlearn: 0.1995564\ttotal: 2.65s\tremaining: 3.35s\n441:\tlearn: 0.1992955\ttotal: 2.65s\tremaining: 3.35s\n442:\tlearn: 0.1989613\ttotal: 2.66s\tremaining: 3.34s\n443:\tlearn: 0.1986905\ttotal: 2.66s\tremaining: 3.34s\n444:\tlearn: 0.1983661\ttotal: 2.67s\tremaining: 3.33s\n445:\tlearn: 0.1979012\ttotal: 2.68s\tremaining: 3.33s\n446:\tlearn: 0.1977081\ttotal: 2.68s\tremaining: 3.32s\n447:\tlearn: 0.1973622\ttotal: 2.69s\tremaining: 3.31s\n448:\tlearn: 0.1970941\ttotal: 2.69s\tremaining: 3.31s\n449:\tlearn: 0.1967162\ttotal: 2.7s\tremaining: 3.3s\n450:\tlearn: 0.1964015\ttotal: 2.71s\tremaining: 3.29s\n451:\tlearn: 0.1959808\ttotal: 2.71s\tremaining: 3.29s\n452:\tlearn: 0.1955760\ttotal: 2.72s\tremaining: 3.28s\n453:\tlearn: 0.1950408\ttotal: 2.73s\tremaining: 3.28s\n454:\tlearn: 0.1948517\ttotal: 2.73s\tremaining: 3.27s\n455:\tlearn: 0.1944731\ttotal: 2.74s\tremaining: 3.27s\n456:\tlearn: 0.1943016\ttotal: 2.74s\tremaining: 3.26s\n457:\tlearn: 0.1940856\ttotal: 2.75s\tremaining: 3.25s\n458:\tlearn: 0.1938859\ttotal: 2.75s\tremaining: 3.25s\n459:\tlearn: 0.1936225\ttotal: 2.76s\tremaining: 3.24s\n460:\tlearn: 0.1934609\ttotal: 2.77s\tremaining: 3.23s\n461:\tlearn: 0.1931503\ttotal: 2.77s\tremaining: 3.23s\n462:\tlearn: 0.1928694\ttotal: 2.78s\tremaining: 3.22s\n463:\tlearn: 0.1924681\ttotal: 2.78s\tremaining: 3.22s\n464:\tlearn: 0.1921311\ttotal: 2.79s\tremaining: 3.21s\n465:\tlearn: 0.1918354\ttotal: 2.8s\tremaining: 3.21s\n466:\tlearn: 0.1915493\ttotal: 2.8s\tremaining: 3.2s\n467:\tlearn: 0.1913229\ttotal: 2.81s\tremaining: 3.19s\n468:\tlearn: 0.1909963\ttotal: 2.81s\tremaining: 3.19s\n469:\tlearn: 0.1907257\ttotal: 2.82s\tremaining: 3.18s\n470:\tlearn: 0.1901922\ttotal: 2.83s\tremaining: 3.17s\n471:\tlearn: 0.1899541\ttotal: 2.83s\tremaining: 3.17s\n472:\tlearn: 0.1896407\ttotal: 2.84s\tremaining: 3.16s\n473:\tlearn: 0.1894738\ttotal: 2.84s\tremaining: 3.15s\n474:\tlearn: 0.1891782\ttotal: 2.85s\tremaining: 3.15s\n475:\tlearn: 0.1887973\ttotal: 2.85s\tremaining: 3.14s\n476:\tlearn: 0.1885093\ttotal: 2.86s\tremaining: 3.14s\n477:\tlearn: 0.1882121\ttotal: 2.87s\tremaining: 3.13s\n478:\tlearn: 0.1878499\ttotal: 2.87s\tremaining: 3.12s\n479:\tlearn: 0.1875680\ttotal: 2.88s\tremaining: 3.12s\n480:\tlearn: 0.1872551\ttotal: 2.88s\tremaining: 3.11s\n481:\tlearn: 0.1869448\ttotal: 2.89s\tremaining: 3.1s\n482:\tlearn: 0.1866599\ttotal: 2.89s\tremaining: 3.1s\n483:\tlearn: 0.1861061\ttotal: 2.9s\tremaining: 3.09s\n484:\tlearn: 0.1858130\ttotal: 2.9s\tremaining: 3.08s\n485:\tlearn: 0.1855159\ttotal: 2.91s\tremaining: 3.08s\n486:\tlearn: 0.1852014\ttotal: 2.92s\tremaining: 3.07s\n487:\tlearn: 0.1849825\ttotal: 2.92s\tremaining: 3.07s\n488:\tlearn: 0.1847242\ttotal: 2.93s\tremaining: 3.06s\n489:\tlearn: 0.1844456\ttotal: 2.93s\tremaining: 3.05s\n490:\tlearn: 0.1841139\ttotal: 2.94s\tremaining: 3.05s\n491:\tlearn: 0.1838125\ttotal: 2.95s\tremaining: 3.04s\n492:\tlearn: 0.1836069\ttotal: 2.95s\tremaining: 3.04s\n493:\tlearn: 0.1833336\ttotal: 2.96s\tremaining: 3.03s\n494:\tlearn: 0.1831564\ttotal: 2.96s\tremaining: 3.02s\n495:\tlearn: 0.1829267\ttotal: 2.97s\tremaining: 3.02s\n496:\tlearn: 0.1826140\ttotal: 2.97s\tremaining: 3.01s\n497:\tlearn: 0.1823535\ttotal: 2.98s\tremaining: 3s\n498:\tlearn: 0.1819481\ttotal: 2.99s\tremaining: 3s\n499:\tlearn: 0.1815749\ttotal: 2.99s\tremaining: 2.99s\n500:\tlearn: 0.1813478\ttotal: 3s\tremaining: 2.99s\n501:\tlearn: 0.1811578\ttotal: 3s\tremaining: 2.98s\n502:\tlearn: 0.1807954\ttotal: 3.01s\tremaining: 2.97s\n503:\tlearn: 0.1805794\ttotal: 3.02s\tremaining: 2.97s\n504:\tlearn: 0.1802723\ttotal: 3.02s\tremaining: 2.96s\n505:\tlearn: 0.1799753\ttotal: 3.03s\tremaining: 2.96s\n506:\tlearn: 0.1796177\ttotal: 3.03s\tremaining: 2.95s\n507:\tlearn: 0.1793204\ttotal: 3.04s\tremaining: 2.94s\n508:\tlearn: 0.1791306\ttotal: 3.05s\tremaining: 2.94s\n509:\tlearn: 0.1788773\ttotal: 3.05s\tremaining: 2.93s\n510:\tlearn: 0.1787810\ttotal: 3.06s\tremaining: 2.93s\n511:\tlearn: 0.1785501\ttotal: 3.06s\tremaining: 2.92s\n512:\tlearn: 0.1780511\ttotal: 3.07s\tremaining: 2.92s\n513:\tlearn: 0.1778000\ttotal: 3.08s\tremaining: 2.91s\n514:\tlearn: 0.1774578\ttotal: 3.08s\tremaining: 2.9s\n515:\tlearn: 0.1771924\ttotal: 3.09s\tremaining: 2.9s\n516:\tlearn: 0.1769932\ttotal: 3.1s\tremaining: 2.89s\n517:\tlearn: 0.1766794\ttotal: 3.1s\tremaining: 2.89s\n518:\tlearn: 0.1763754\ttotal: 3.11s\tremaining: 2.88s\n519:\tlearn: 0.1759450\ttotal: 3.12s\tremaining: 2.88s\n520:\tlearn: 0.1756405\ttotal: 3.12s\tremaining: 2.87s\n521:\tlearn: 0.1753649\ttotal: 3.13s\tremaining: 2.86s\n522:\tlearn: 0.1751128\ttotal: 3.13s\tremaining: 2.86s\n523:\tlearn: 0.1749242\ttotal: 3.14s\tremaining: 2.85s\n524:\tlearn: 0.1746064\ttotal: 3.15s\tremaining: 2.85s\n525:\tlearn: 0.1744073\ttotal: 3.15s\tremaining: 2.84s\n526:\tlearn: 0.1741340\ttotal: 3.16s\tremaining: 2.83s\n527:\tlearn: 0.1737691\ttotal: 3.16s\tremaining: 2.83s\n528:\tlearn: 0.1736150\ttotal: 3.17s\tremaining: 2.82s\n529:\tlearn: 0.1731941\ttotal: 3.18s\tremaining: 2.82s\n530:\tlearn: 0.1728630\ttotal: 3.18s\tremaining: 2.81s\n531:\tlearn: 0.1725562\ttotal: 3.19s\tremaining: 2.81s\n532:\tlearn: 0.1722193\ttotal: 3.19s\tremaining: 2.8s\n533:\tlearn: 0.1719431\ttotal: 3.2s\tremaining: 2.79s\n534:\tlearn: 0.1717699\ttotal: 3.21s\tremaining: 2.79s\n535:\tlearn: 0.1715351\ttotal: 3.21s\tremaining: 2.78s\n536:\tlearn: 0.1713857\ttotal: 3.22s\tremaining: 2.77s\n537:\tlearn: 0.1711436\ttotal: 3.22s\tremaining: 2.77s\n538:\tlearn: 0.1708548\ttotal: 3.23s\tremaining: 2.76s\n539:\tlearn: 0.1705667\ttotal: 3.23s\tremaining: 2.76s\n540:\tlearn: 0.1702589\ttotal: 3.24s\tremaining: 2.75s\n541:\tlearn: 0.1700927\ttotal: 3.25s\tremaining: 2.74s\n542:\tlearn: 0.1699101\ttotal: 3.25s\tremaining: 2.74s\n543:\tlearn: 0.1698169\ttotal: 3.26s\tremaining: 2.73s\n544:\tlearn: 0.1696421\ttotal: 3.26s\tremaining: 2.73s\n545:\tlearn: 0.1694964\ttotal: 3.27s\tremaining: 2.72s\n546:\tlearn: 0.1690678\ttotal: 3.28s\tremaining: 2.71s\n547:\tlearn: 0.1689679\ttotal: 3.28s\tremaining: 2.71s\n548:\tlearn: 0.1687518\ttotal: 3.29s\tremaining: 2.7s\n549:\tlearn: 0.1684735\ttotal: 3.3s\tremaining: 2.7s\n550:\tlearn: 0.1682755\ttotal: 3.3s\tremaining: 2.69s\n551:\tlearn: 0.1679277\ttotal: 3.31s\tremaining: 2.69s\n552:\tlearn: 0.1675758\ttotal: 3.32s\tremaining: 2.68s\n553:\tlearn: 0.1672825\ttotal: 3.32s\tremaining: 2.68s\n554:\tlearn: 0.1670264\ttotal: 3.33s\tremaining: 2.67s\n555:\tlearn: 0.1667561\ttotal: 3.34s\tremaining: 2.67s\n556:\tlearn: 0.1665582\ttotal: 3.35s\tremaining: 2.66s\n557:\tlearn: 0.1663101\ttotal: 3.35s\tremaining: 2.65s\n558:\tlearn: 0.1659935\ttotal: 3.36s\tremaining: 2.65s\n559:\tlearn: 0.1657285\ttotal: 3.36s\tremaining: 2.64s\n560:\tlearn: 0.1653730\ttotal: 3.37s\tremaining: 2.64s\n561:\tlearn: 0.1649678\ttotal: 3.38s\tremaining: 2.63s\n562:\tlearn: 0.1647043\ttotal: 3.38s\tremaining: 2.62s\n563:\tlearn: 0.1644293\ttotal: 3.39s\tremaining: 2.62s\n564:\tlearn: 0.1642658\ttotal: 3.39s\tremaining: 2.61s\n565:\tlearn: 0.1640688\ttotal: 3.4s\tremaining: 2.6s\n566:\tlearn: 0.1638297\ttotal: 3.4s\tremaining: 2.6s\n567:\tlearn: 0.1635541\ttotal: 3.41s\tremaining: 2.59s\n568:\tlearn: 0.1633344\ttotal: 3.42s\tremaining: 2.59s\n569:\tlearn: 0.1631288\ttotal: 3.42s\tremaining: 2.58s\n570:\tlearn: 0.1629787\ttotal: 3.43s\tremaining: 2.57s\n571:\tlearn: 0.1627016\ttotal: 3.43s\tremaining: 2.57s\n572:\tlearn: 0.1624157\ttotal: 3.44s\tremaining: 2.56s\n573:\tlearn: 0.1620537\ttotal: 3.44s\tremaining: 2.56s\n574:\tlearn: 0.1615794\ttotal: 3.45s\tremaining: 2.55s\n575:\tlearn: 0.1614300\ttotal: 3.46s\tremaining: 2.54s\n576:\tlearn: 0.1612185\ttotal: 3.46s\tremaining: 2.54s\n577:\tlearn: 0.1610302\ttotal: 3.47s\tremaining: 2.53s\n578:\tlearn: 0.1608367\ttotal: 3.47s\tremaining: 2.52s\n579:\tlearn: 0.1606479\ttotal: 3.48s\tremaining: 2.52s\n580:\tlearn: 0.1604926\ttotal: 3.48s\tremaining: 2.51s\n581:\tlearn: 0.1602037\ttotal: 3.49s\tremaining: 2.5s\n582:\tlearn: 0.1598771\ttotal: 3.49s\tremaining: 2.5s\n583:\tlearn: 0.1596682\ttotal: 3.5s\tremaining: 2.49s\n584:\tlearn: 0.1593794\ttotal: 3.51s\tremaining: 2.49s\n585:\tlearn: 0.1591803\ttotal: 3.51s\tremaining: 2.48s\n586:\tlearn: 0.1590616\ttotal: 3.52s\tremaining: 2.48s\n587:\tlearn: 0.1588519\ttotal: 3.52s\tremaining: 2.47s\n588:\tlearn: 0.1585845\ttotal: 3.53s\tremaining: 2.46s\n589:\tlearn: 0.1584292\ttotal: 3.54s\tremaining: 2.46s\n590:\tlearn: 0.1582295\ttotal: 3.54s\tremaining: 2.45s\n591:\tlearn: 0.1580913\ttotal: 3.55s\tremaining: 2.44s\n592:\tlearn: 0.1579460\ttotal: 3.55s\tremaining: 2.44s\n593:\tlearn: 0.1576221\ttotal: 3.56s\tremaining: 2.43s\n594:\tlearn: 0.1574660\ttotal: 3.56s\tremaining: 2.42s\n595:\tlearn: 0.1572355\ttotal: 3.57s\tremaining: 2.42s\n596:\tlearn: 0.1569963\ttotal: 3.58s\tremaining: 2.41s\n597:\tlearn: 0.1567256\ttotal: 3.58s\tremaining: 2.41s\n598:\tlearn: 0.1564123\ttotal: 3.59s\tremaining: 2.4s\n599:\tlearn: 0.1562041\ttotal: 3.59s\tremaining: 2.4s\n600:\tlearn: 0.1560373\ttotal: 3.6s\tremaining: 2.39s\n601:\tlearn: 0.1557962\ttotal: 3.6s\tremaining: 2.38s\n602:\tlearn: 0.1555225\ttotal: 3.61s\tremaining: 2.38s\n603:\tlearn: 0.1553035\ttotal: 3.62s\tremaining: 2.37s\n604:\tlearn: 0.1551199\ttotal: 3.62s\tremaining: 2.37s\n605:\tlearn: 0.1548794\ttotal: 3.63s\tremaining: 2.36s\n606:\tlearn: 0.1546257\ttotal: 3.63s\tremaining: 2.35s\n607:\tlearn: 0.1543092\ttotal: 3.64s\tremaining: 2.35s\n608:\tlearn: 0.1540047\ttotal: 3.65s\tremaining: 2.34s\n609:\tlearn: 0.1538783\ttotal: 3.65s\tremaining: 2.33s\n610:\tlearn: 0.1536490\ttotal: 3.66s\tremaining: 2.33s\n611:\tlearn: 0.1534929\ttotal: 3.66s\tremaining: 2.32s\n612:\tlearn: 0.1533517\ttotal: 3.67s\tremaining: 2.32s\n613:\tlearn: 0.1530556\ttotal: 3.67s\tremaining: 2.31s\n614:\tlearn: 0.1529405\ttotal: 3.68s\tremaining: 2.3s\n615:\tlearn: 0.1527164\ttotal: 3.69s\tremaining: 2.3s\n616:\tlearn: 0.1525702\ttotal: 3.69s\tremaining: 2.29s\n617:\tlearn: 0.1524417\ttotal: 3.7s\tremaining: 2.29s\n618:\tlearn: 0.1522038\ttotal: 3.71s\tremaining: 2.28s\n619:\tlearn: 0.1518941\ttotal: 3.71s\tremaining: 2.27s\n620:\tlearn: 0.1516913\ttotal: 3.72s\tremaining: 2.27s\n621:\tlearn: 0.1512912\ttotal: 3.72s\tremaining: 2.26s\n622:\tlearn: 0.1510291\ttotal: 3.73s\tremaining: 2.26s\n623:\tlearn: 0.1508269\ttotal: 3.73s\tremaining: 2.25s\n624:\tlearn: 0.1506328\ttotal: 3.74s\tremaining: 2.24s\n625:\tlearn: 0.1504163\ttotal: 3.75s\tremaining: 2.24s\n626:\tlearn: 0.1501377\ttotal: 3.75s\tremaining: 2.23s\n627:\tlearn: 0.1499301\ttotal: 3.76s\tremaining: 2.23s\n628:\tlearn: 0.1496235\ttotal: 3.77s\tremaining: 2.22s\n629:\tlearn: 0.1493917\ttotal: 3.77s\tremaining: 2.21s\n630:\tlearn: 0.1490928\ttotal: 3.78s\tremaining: 2.21s\n631:\tlearn: 0.1488205\ttotal: 3.78s\tremaining: 2.2s\n632:\tlearn: 0.1487200\ttotal: 3.79s\tremaining: 2.2s\n633:\tlearn: 0.1485669\ttotal: 3.8s\tremaining: 2.19s\n634:\tlearn: 0.1483013\ttotal: 3.8s\tremaining: 2.19s\n635:\tlearn: 0.1481263\ttotal: 3.81s\tremaining: 2.18s\n636:\tlearn: 0.1479844\ttotal: 3.82s\tremaining: 2.17s\n637:\tlearn: 0.1477533\ttotal: 3.82s\tremaining: 2.17s\n638:\tlearn: 0.1476197\ttotal: 3.83s\tremaining: 2.16s\n639:\tlearn: 0.1474222\ttotal: 3.83s\tremaining: 2.16s\n640:\tlearn: 0.1472569\ttotal: 3.84s\tremaining: 2.15s\n641:\tlearn: 0.1469949\ttotal: 3.85s\tremaining: 2.15s\n642:\tlearn: 0.1468622\ttotal: 3.85s\tremaining: 2.14s\n643:\tlearn: 0.1467644\ttotal: 3.86s\tremaining: 2.13s\n644:\tlearn: 0.1464808\ttotal: 3.87s\tremaining: 2.13s\n645:\tlearn: 0.1462012\ttotal: 3.87s\tremaining: 2.12s\n646:\tlearn: 0.1459664\ttotal: 3.88s\tremaining: 2.12s\n647:\tlearn: 0.1458406\ttotal: 3.88s\tremaining: 2.11s\n648:\tlearn: 0.1455959\ttotal: 3.89s\tremaining: 2.1s\n649:\tlearn: 0.1453081\ttotal: 3.9s\tremaining: 2.1s\n650:\tlearn: 0.1450962\ttotal: 3.9s\tremaining: 2.09s\n651:\tlearn: 0.1449250\ttotal: 3.91s\tremaining: 2.09s\n652:\tlearn: 0.1447775\ttotal: 3.92s\tremaining: 2.08s\n653:\tlearn: 0.1445933\ttotal: 3.92s\tremaining: 2.07s\n654:\tlearn: 0.1443954\ttotal: 3.93s\tremaining: 2.07s\n655:\tlearn: 0.1441100\ttotal: 3.93s\tremaining: 2.06s\n656:\tlearn: 0.1439142\ttotal: 3.94s\tremaining: 2.06s\n657:\tlearn: 0.1437528\ttotal: 3.95s\tremaining: 2.05s\n658:\tlearn: 0.1435082\ttotal: 3.95s\tremaining: 2.04s\n659:\tlearn: 0.1433055\ttotal: 3.96s\tremaining: 2.04s\n660:\tlearn: 0.1430834\ttotal: 3.96s\tremaining: 2.03s\n661:\tlearn: 0.1428058\ttotal: 3.97s\tremaining: 2.03s\n662:\tlearn: 0.1426713\ttotal: 3.98s\tremaining: 2.02s\n663:\tlearn: 0.1425199\ttotal: 3.98s\tremaining: 2.01s\n664:\tlearn: 0.1423080\ttotal: 3.99s\tremaining: 2.01s\n665:\tlearn: 0.1421437\ttotal: 3.99s\tremaining: 2s\n666:\tlearn: 0.1419904\ttotal: 4s\tremaining: 2s\n667:\tlearn: 0.1418010\ttotal: 4s\tremaining: 1.99s\n668:\tlearn: 0.1415543\ttotal: 4.01s\tremaining: 1.98s\n669:\tlearn: 0.1413819\ttotal: 4.02s\tremaining: 1.98s\n670:\tlearn: 0.1412283\ttotal: 4.02s\tremaining: 1.97s\n671:\tlearn: 0.1409712\ttotal: 4.03s\tremaining: 1.97s\n672:\tlearn: 0.1408626\ttotal: 4.03s\tremaining: 1.96s\n673:\tlearn: 0.1406060\ttotal: 4.04s\tremaining: 1.95s\n674:\tlearn: 0.1404224\ttotal: 4.04s\tremaining: 1.95s\n675:\tlearn: 0.1402270\ttotal: 4.05s\tremaining: 1.94s\n676:\tlearn: 0.1401288\ttotal: 4.05s\tremaining: 1.94s\n677:\tlearn: 0.1398966\ttotal: 4.06s\tremaining: 1.93s\n678:\tlearn: 0.1395928\ttotal: 4.07s\tremaining: 1.92s\n679:\tlearn: 0.1394508\ttotal: 4.07s\tremaining: 1.92s\n680:\tlearn: 0.1391642\ttotal: 4.08s\tremaining: 1.91s\n681:\tlearn: 0.1390126\ttotal: 4.08s\tremaining: 1.9s\n682:\tlearn: 0.1388092\ttotal: 4.09s\tremaining: 1.9s\n683:\tlearn: 0.1385903\ttotal: 4.09s\tremaining: 1.89s\n684:\tlearn: 0.1384760\ttotal: 4.1s\tremaining: 1.89s\n685:\tlearn: 0.1383533\ttotal: 4.11s\tremaining: 1.88s\n686:\tlearn: 0.1381499\ttotal: 4.11s\tremaining: 1.87s\n687:\tlearn: 0.1379523\ttotal: 4.12s\tremaining: 1.87s\n688:\tlearn: 0.1377582\ttotal: 4.12s\tremaining: 1.86s\n689:\tlearn: 0.1376519\ttotal: 4.13s\tremaining: 1.85s\n690:\tlearn: 0.1374937\ttotal: 4.13s\tremaining: 1.85s\n691:\tlearn: 0.1373480\ttotal: 4.14s\tremaining: 1.84s\n692:\tlearn: 0.1371170\ttotal: 4.14s\tremaining: 1.84s\n693:\tlearn: 0.1370162\ttotal: 4.15s\tremaining: 1.83s\n694:\tlearn: 0.1367625\ttotal: 4.16s\tremaining: 1.82s\n695:\tlearn: 0.1366437\ttotal: 4.16s\tremaining: 1.82s\n696:\tlearn: 0.1365288\ttotal: 4.17s\tremaining: 1.81s\n697:\tlearn: 0.1362986\ttotal: 4.18s\tremaining: 1.81s\n698:\tlearn: 0.1360736\ttotal: 4.18s\tremaining: 1.8s\n699:\tlearn: 0.1357999\ttotal: 4.19s\tremaining: 1.79s\n700:\tlearn: 0.1356408\ttotal: 4.2s\tremaining: 1.79s\n701:\tlearn: 0.1354613\ttotal: 4.2s\tremaining: 1.78s\n702:\tlearn: 0.1352310\ttotal: 4.21s\tremaining: 1.78s\n703:\tlearn: 0.1350918\ttotal: 4.21s\tremaining: 1.77s\n704:\tlearn: 0.1349271\ttotal: 4.22s\tremaining: 1.76s\n705:\tlearn: 0.1347625\ttotal: 4.23s\tremaining: 1.76s\n706:\tlearn: 0.1345085\ttotal: 4.23s\tremaining: 1.75s\n707:\tlearn: 0.1342751\ttotal: 4.24s\tremaining: 1.75s\n708:\tlearn: 0.1340911\ttotal: 4.24s\tremaining: 1.74s\n709:\tlearn: 0.1338937\ttotal: 4.25s\tremaining: 1.74s\n710:\tlearn: 0.1337528\ttotal: 4.25s\tremaining: 1.73s\n711:\tlearn: 0.1336032\ttotal: 4.26s\tremaining: 1.72s\n712:\tlearn: 0.1333834\ttotal: 4.27s\tremaining: 1.72s\n713:\tlearn: 0.1331688\ttotal: 4.27s\tremaining: 1.71s\n714:\tlearn: 0.1330248\ttotal: 4.28s\tremaining: 1.71s\n715:\tlearn: 0.1328925\ttotal: 4.28s\tremaining: 1.7s\n716:\tlearn: 0.1326207\ttotal: 4.29s\tremaining: 1.69s\n717:\tlearn: 0.1324728\ttotal: 4.29s\tremaining: 1.69s\n718:\tlearn: 0.1322828\ttotal: 4.3s\tremaining: 1.68s\n719:\tlearn: 0.1320777\ttotal: 4.3s\tremaining: 1.67s\n720:\tlearn: 0.1318640\ttotal: 4.31s\tremaining: 1.67s\n721:\tlearn: 0.1316593\ttotal: 4.32s\tremaining: 1.66s\n722:\tlearn: 0.1314042\ttotal: 4.32s\tremaining: 1.66s\n723:\tlearn: 0.1312000\ttotal: 4.33s\tremaining: 1.65s\n724:\tlearn: 0.1309606\ttotal: 4.33s\tremaining: 1.64s\n725:\tlearn: 0.1308019\ttotal: 4.34s\tremaining: 1.64s\n726:\tlearn: 0.1306449\ttotal: 4.35s\tremaining: 1.63s\n727:\tlearn: 0.1305712\ttotal: 4.35s\tremaining: 1.63s\n728:\tlearn: 0.1304321\ttotal: 4.36s\tremaining: 1.62s\n729:\tlearn: 0.1302171\ttotal: 4.36s\tremaining: 1.61s\n730:\tlearn: 0.1301181\ttotal: 4.37s\tremaining: 1.61s\n731:\tlearn: 0.1298853\ttotal: 4.37s\tremaining: 1.6s\n732:\tlearn: 0.1297408\ttotal: 4.38s\tremaining: 1.59s\n733:\tlearn: 0.1296544\ttotal: 4.38s\tremaining: 1.59s\n734:\tlearn: 0.1294686\ttotal: 4.39s\tremaining: 1.58s\n735:\tlearn: 0.1293101\ttotal: 4.4s\tremaining: 1.58s\n736:\tlearn: 0.1290044\ttotal: 4.4s\tremaining: 1.57s\n737:\tlearn: 0.1287851\ttotal: 4.41s\tremaining: 1.56s\n738:\tlearn: 0.1286058\ttotal: 4.41s\tremaining: 1.56s\n739:\tlearn: 0.1283890\ttotal: 4.42s\tremaining: 1.55s\n740:\tlearn: 0.1281600\ttotal: 4.42s\tremaining: 1.55s\n741:\tlearn: 0.1278876\ttotal: 4.43s\tremaining: 1.54s\n742:\tlearn: 0.1277791\ttotal: 4.44s\tremaining: 1.53s\n743:\tlearn: 0.1276024\ttotal: 4.44s\tremaining: 1.53s\n744:\tlearn: 0.1273310\ttotal: 4.45s\tremaining: 1.52s\n745:\tlearn: 0.1270282\ttotal: 4.45s\tremaining: 1.52s\n746:\tlearn: 0.1268948\ttotal: 4.46s\tremaining: 1.51s\n747:\tlearn: 0.1267531\ttotal: 4.47s\tremaining: 1.5s\n748:\tlearn: 0.1266061\ttotal: 4.47s\tremaining: 1.5s\n749:\tlearn: 0.1264620\ttotal: 4.48s\tremaining: 1.49s\n750:\tlearn: 0.1263381\ttotal: 4.49s\tremaining: 1.49s\n751:\tlearn: 0.1262360\ttotal: 4.49s\tremaining: 1.48s\n752:\tlearn: 0.1261019\ttotal: 4.5s\tremaining: 1.48s\n753:\tlearn: 0.1260220\ttotal: 4.5s\tremaining: 1.47s\n754:\tlearn: 0.1258588\ttotal: 4.51s\tremaining: 1.46s\n755:\tlearn: 0.1256588\ttotal: 4.52s\tremaining: 1.46s\n756:\tlearn: 0.1255167\ttotal: 4.52s\tremaining: 1.45s\n757:\tlearn: 0.1253245\ttotal: 4.53s\tremaining: 1.45s\n758:\tlearn: 0.1251120\ttotal: 4.54s\tremaining: 1.44s\n759:\tlearn: 0.1249365\ttotal: 4.54s\tremaining: 1.43s\n760:\tlearn: 0.1248204\ttotal: 4.55s\tremaining: 1.43s\n761:\tlearn: 0.1246475\ttotal: 4.55s\tremaining: 1.42s\n762:\tlearn: 0.1244981\ttotal: 4.56s\tremaining: 1.42s\n763:\tlearn: 0.1244115\ttotal: 4.57s\tremaining: 1.41s\n764:\tlearn: 0.1241918\ttotal: 4.57s\tremaining: 1.4s\n765:\tlearn: 0.1240469\ttotal: 4.58s\tremaining: 1.4s\n766:\tlearn: 0.1238541\ttotal: 4.58s\tremaining: 1.39s\n767:\tlearn: 0.1237334\ttotal: 4.59s\tremaining: 1.39s\n768:\tlearn: 0.1235480\ttotal: 4.6s\tremaining: 1.38s\n769:\tlearn: 0.1233236\ttotal: 4.6s\tremaining: 1.37s\n770:\tlearn: 0.1232157\ttotal: 4.61s\tremaining: 1.37s\n771:\tlearn: 0.1229657\ttotal: 4.62s\tremaining: 1.36s\n772:\tlearn: 0.1227773\ttotal: 4.62s\tremaining: 1.36s\n773:\tlearn: 0.1226371\ttotal: 4.63s\tremaining: 1.35s\n774:\tlearn: 0.1225278\ttotal: 4.63s\tremaining: 1.34s\n775:\tlearn: 0.1223393\ttotal: 4.64s\tremaining: 1.34s\n776:\tlearn: 0.1221466\ttotal: 4.65s\tremaining: 1.33s\n777:\tlearn: 0.1219865\ttotal: 4.65s\tremaining: 1.33s\n778:\tlearn: 0.1218390\ttotal: 4.66s\tremaining: 1.32s\n779:\tlearn: 0.1215916\ttotal: 4.67s\tremaining: 1.31s\n780:\tlearn: 0.1213739\ttotal: 4.67s\tremaining: 1.31s\n781:\tlearn: 0.1212762\ttotal: 4.68s\tremaining: 1.3s\n782:\tlearn: 0.1211961\ttotal: 4.68s\tremaining: 1.3s\n783:\tlearn: 0.1210898\ttotal: 4.69s\tremaining: 1.29s\n784:\tlearn: 0.1209380\ttotal: 4.7s\tremaining: 1.29s\n785:\tlearn: 0.1208001\ttotal: 4.7s\tremaining: 1.28s\n786:\tlearn: 0.1205597\ttotal: 4.71s\tremaining: 1.27s\n787:\tlearn: 0.1203854\ttotal: 4.71s\tremaining: 1.27s\n788:\tlearn: 0.1202789\ttotal: 4.72s\tremaining: 1.26s\n789:\tlearn: 0.1201187\ttotal: 4.72s\tremaining: 1.26s\n790:\tlearn: 0.1199175\ttotal: 4.73s\tremaining: 1.25s\n791:\tlearn: 0.1198392\ttotal: 4.74s\tremaining: 1.24s\n792:\tlearn: 0.1197371\ttotal: 4.74s\tremaining: 1.24s\n793:\tlearn: 0.1196261\ttotal: 4.75s\tremaining: 1.23s\n794:\tlearn: 0.1194468\ttotal: 4.75s\tremaining: 1.23s\n795:\tlearn: 0.1193671\ttotal: 4.76s\tremaining: 1.22s\n796:\tlearn: 0.1192265\ttotal: 4.77s\tremaining: 1.21s\n797:\tlearn: 0.1190681\ttotal: 4.77s\tremaining: 1.21s\n798:\tlearn: 0.1188693\ttotal: 4.78s\tremaining: 1.2s\n799:\tlearn: 0.1187474\ttotal: 4.78s\tremaining: 1.2s\n800:\tlearn: 0.1185708\ttotal: 4.79s\tremaining: 1.19s\n801:\tlearn: 0.1185111\ttotal: 4.79s\tremaining: 1.18s\n802:\tlearn: 0.1183751\ttotal: 4.8s\tremaining: 1.18s\n803:\tlearn: 0.1182949\ttotal: 4.81s\tremaining: 1.17s\n804:\tlearn: 0.1180900\ttotal: 4.81s\tremaining: 1.17s\n805:\tlearn: 0.1180097\ttotal: 4.82s\tremaining: 1.16s\n806:\tlearn: 0.1179246\ttotal: 4.82s\tremaining: 1.15s\n807:\tlearn: 0.1176932\ttotal: 4.83s\tremaining: 1.15s\n808:\tlearn: 0.1174970\ttotal: 4.83s\tremaining: 1.14s\n809:\tlearn: 0.1174054\ttotal: 4.84s\tremaining: 1.14s\n810:\tlearn: 0.1172405\ttotal: 4.85s\tremaining: 1.13s\n811:\tlearn: 0.1170540\ttotal: 4.85s\tremaining: 1.12s\n812:\tlearn: 0.1168988\ttotal: 4.86s\tremaining: 1.12s\n813:\tlearn: 0.1167916\ttotal: 4.86s\tremaining: 1.11s\n814:\tlearn: 0.1166636\ttotal: 4.87s\tremaining: 1.1s\n815:\tlearn: 0.1165618\ttotal: 4.87s\tremaining: 1.1s\n816:\tlearn: 0.1162967\ttotal: 4.88s\tremaining: 1.09s\n817:\tlearn: 0.1161992\ttotal: 4.88s\tremaining: 1.09s\n818:\tlearn: 0.1159988\ttotal: 4.89s\tremaining: 1.08s\n819:\tlearn: 0.1158631\ttotal: 4.9s\tremaining: 1.07s\n820:\tlearn: 0.1157202\ttotal: 4.9s\tremaining: 1.07s\n821:\tlearn: 0.1155679\ttotal: 4.91s\tremaining: 1.06s\n822:\tlearn: 0.1153895\ttotal: 4.91s\tremaining: 1.06s\n823:\tlearn: 0.1152275\ttotal: 4.92s\tremaining: 1.05s\n824:\tlearn: 0.1151084\ttotal: 4.92s\tremaining: 1.04s\n825:\tlearn: 0.1149357\ttotal: 4.93s\tremaining: 1.04s\n826:\tlearn: 0.1147780\ttotal: 4.94s\tremaining: 1.03s\n827:\tlearn: 0.1145768\ttotal: 4.94s\tremaining: 1.03s\n828:\tlearn: 0.1143429\ttotal: 4.95s\tremaining: 1.02s\n829:\tlearn: 0.1141960\ttotal: 4.95s\tremaining: 1.01s\n830:\tlearn: 0.1141047\ttotal: 4.96s\tremaining: 1.01s\n831:\tlearn: 0.1139328\ttotal: 4.96s\tremaining: 1s\n832:\tlearn: 0.1138030\ttotal: 4.97s\tremaining: 997ms\n833:\tlearn: 0.1135620\ttotal: 4.98s\tremaining: 991ms\n834:\tlearn: 0.1134418\ttotal: 4.98s\tremaining: 985ms\n835:\tlearn: 0.1132930\ttotal: 4.99s\tremaining: 979ms\n836:\tlearn: 0.1130960\ttotal: 5s\tremaining: 973ms\n837:\tlearn: 0.1129160\ttotal: 5s\tremaining: 967ms\n838:\tlearn: 0.1128243\ttotal: 5.01s\tremaining: 961ms\n839:\tlearn: 0.1126654\ttotal: 5.01s\tremaining: 955ms\n840:\tlearn: 0.1125400\ttotal: 5.02s\tremaining: 949ms\n841:\tlearn: 0.1124234\ttotal: 5.02s\tremaining: 943ms\n842:\tlearn: 0.1122904\ttotal: 5.03s\tremaining: 937ms\n843:\tlearn: 0.1120240\ttotal: 5.04s\tremaining: 931ms\n844:\tlearn: 0.1118779\ttotal: 5.04s\tremaining: 925ms\n845:\tlearn: 0.1117894\ttotal: 5.05s\tremaining: 919ms\n846:\tlearn: 0.1115403\ttotal: 5.05s\tremaining: 913ms\n847:\tlearn: 0.1114461\ttotal: 5.06s\tremaining: 907ms\n848:\tlearn: 0.1112407\ttotal: 5.07s\tremaining: 901ms\n849:\tlearn: 0.1111565\ttotal: 5.07s\tremaining: 895ms\n850:\tlearn: 0.1110185\ttotal: 5.08s\tremaining: 889ms\n851:\tlearn: 0.1108195\ttotal: 5.08s\tremaining: 883ms\n852:\tlearn: 0.1106866\ttotal: 5.09s\tremaining: 877ms\n853:\tlearn: 0.1104963\ttotal: 5.1s\tremaining: 871ms\n854:\tlearn: 0.1104351\ttotal: 5.1s\tremaining: 865ms\n855:\tlearn: 0.1103508\ttotal: 5.11s\tremaining: 859ms\n856:\tlearn: 0.1102258\ttotal: 5.11s\tremaining: 853ms\n857:\tlearn: 0.1100568\ttotal: 5.12s\tremaining: 847ms\n858:\tlearn: 0.1099066\ttotal: 5.12s\tremaining: 841ms\n859:\tlearn: 0.1097744\ttotal: 5.13s\tremaining: 835ms\n860:\tlearn: 0.1096814\ttotal: 5.13s\tremaining: 829ms\n861:\tlearn: 0.1096094\ttotal: 5.14s\tremaining: 823ms\n862:\tlearn: 0.1094918\ttotal: 5.14s\tremaining: 817ms\n863:\tlearn: 0.1093296\ttotal: 5.15s\tremaining: 811ms\n864:\tlearn: 0.1091385\ttotal: 5.16s\tremaining: 805ms\n865:\tlearn: 0.1089582\ttotal: 5.16s\tremaining: 799ms\n866:\tlearn: 0.1088282\ttotal: 5.17s\tremaining: 793ms\n867:\tlearn: 0.1087142\ttotal: 5.18s\tremaining: 787ms\n868:\tlearn: 0.1085624\ttotal: 5.18s\tremaining: 781ms\n869:\tlearn: 0.1084399\ttotal: 5.19s\tremaining: 775ms\n870:\tlearn: 0.1083213\ttotal: 5.19s\tremaining: 769ms\n871:\tlearn: 0.1081709\ttotal: 5.2s\tremaining: 763ms\n872:\tlearn: 0.1080878\ttotal: 5.21s\tremaining: 758ms\n873:\tlearn: 0.1079336\ttotal: 5.21s\tremaining: 752ms\n874:\tlearn: 0.1077829\ttotal: 5.22s\tremaining: 746ms\n875:\tlearn: 0.1076741\ttotal: 5.23s\tremaining: 740ms\n876:\tlearn: 0.1076046\ttotal: 5.23s\tremaining: 734ms\n877:\tlearn: 0.1075206\ttotal: 5.24s\tremaining: 728ms\n878:\tlearn: 0.1073517\ttotal: 5.25s\tremaining: 722ms\n879:\tlearn: 0.1072304\ttotal: 5.25s\tremaining: 716ms\n880:\tlearn: 0.1071254\ttotal: 5.26s\tremaining: 710ms\n881:\tlearn: 0.1070200\ttotal: 5.26s\tremaining: 704ms\n882:\tlearn: 0.1067781\ttotal: 5.27s\tremaining: 699ms\n883:\tlearn: 0.1066074\ttotal: 5.28s\tremaining: 693ms\n884:\tlearn: 0.1064539\ttotal: 5.29s\tremaining: 687ms\n885:\tlearn: 0.1062820\ttotal: 5.29s\tremaining: 681ms\n886:\tlearn: 0.1062232\ttotal: 5.3s\tremaining: 675ms\n887:\tlearn: 0.1060488\ttotal: 5.3s\tremaining: 669ms\n888:\tlearn: 0.1059304\ttotal: 5.31s\tremaining: 663ms\n889:\tlearn: 0.1057177\ttotal: 5.32s\tremaining: 657ms\n890:\tlearn: 0.1055042\ttotal: 5.33s\tremaining: 651ms\n891:\tlearn: 0.1054056\ttotal: 5.33s\tremaining: 646ms\n892:\tlearn: 0.1053569\ttotal: 5.34s\tremaining: 640ms\n893:\tlearn: 0.1051649\ttotal: 5.34s\tremaining: 634ms\n894:\tlearn: 0.1050304\ttotal: 5.35s\tremaining: 628ms\n895:\tlearn: 0.1048940\ttotal: 5.36s\tremaining: 622ms\n896:\tlearn: 0.1047320\ttotal: 5.36s\tremaining: 616ms\n897:\tlearn: 0.1046670\ttotal: 5.37s\tremaining: 610ms\n898:\tlearn: 0.1045378\ttotal: 5.38s\tremaining: 604ms\n899:\tlearn: 0.1044444\ttotal: 5.38s\tremaining: 598ms\n900:\tlearn: 0.1043406\ttotal: 5.39s\tremaining: 592ms\n901:\tlearn: 0.1042053\ttotal: 5.39s\tremaining: 586ms\n902:\tlearn: 0.1041257\ttotal: 5.4s\tremaining: 580ms\n903:\tlearn: 0.1040174\ttotal: 5.41s\tremaining: 574ms\n904:\tlearn: 0.1038597\ttotal: 5.41s\tremaining: 568ms\n905:\tlearn: 0.1037334\ttotal: 5.42s\tremaining: 562ms\n906:\tlearn: 0.1036090\ttotal: 5.43s\tremaining: 556ms\n907:\tlearn: 0.1034862\ttotal: 5.43s\tremaining: 551ms\n908:\tlearn: 0.1032644\ttotal: 5.44s\tremaining: 545ms\n909:\tlearn: 0.1031770\ttotal: 5.45s\tremaining: 539ms\n910:\tlearn: 0.1031248\ttotal: 5.45s\tremaining: 533ms\n911:\tlearn: 0.1030253\ttotal: 5.46s\tremaining: 527ms\n912:\tlearn: 0.1028022\ttotal: 5.46s\tremaining: 521ms\n913:\tlearn: 0.1026091\ttotal: 5.47s\tremaining: 515ms\n914:\tlearn: 0.1025196\ttotal: 5.48s\tremaining: 509ms\n915:\tlearn: 0.1023467\ttotal: 5.48s\tremaining: 503ms\n916:\tlearn: 0.1022205\ttotal: 5.49s\tremaining: 497ms\n917:\tlearn: 0.1019899\ttotal: 5.5s\tremaining: 491ms\n918:\tlearn: 0.1018691\ttotal: 5.5s\tremaining: 485ms\n919:\tlearn: 0.1018042\ttotal: 5.51s\tremaining: 479ms\n920:\tlearn: 0.1015468\ttotal: 5.51s\tremaining: 473ms\n921:\tlearn: 0.1013514\ttotal: 5.52s\tremaining: 467ms\n922:\tlearn: 0.1012645\ttotal: 5.53s\tremaining: 461ms\n923:\tlearn: 0.1011424\ttotal: 5.53s\tremaining: 455ms\n924:\tlearn: 0.1010460\ttotal: 5.54s\tremaining: 449ms\n925:\tlearn: 0.1009628\ttotal: 5.55s\tremaining: 443ms\n926:\tlearn: 0.1007923\ttotal: 5.55s\tremaining: 437ms\n927:\tlearn: 0.1006089\ttotal: 5.56s\tremaining: 431ms\n928:\tlearn: 0.1004865\ttotal: 5.56s\tremaining: 425ms\n929:\tlearn: 0.1004035\ttotal: 5.57s\tremaining: 419ms\n930:\tlearn: 0.1003062\ttotal: 5.58s\tremaining: 413ms\n931:\tlearn: 0.1001269\ttotal: 5.58s\tremaining: 407ms\n932:\tlearn: 0.1000763\ttotal: 5.59s\tremaining: 401ms\n933:\tlearn: 0.0999852\ttotal: 5.59s\tremaining: 395ms\n934:\tlearn: 0.0998714\ttotal: 5.6s\tremaining: 389ms\n935:\tlearn: 0.0997845\ttotal: 5.61s\tremaining: 383ms\n936:\tlearn: 0.0996210\ttotal: 5.61s\tremaining: 377ms\n937:\tlearn: 0.0994984\ttotal: 5.62s\tremaining: 371ms\n938:\tlearn: 0.0993548\ttotal: 5.63s\tremaining: 365ms\n939:\tlearn: 0.0992328\ttotal: 5.63s\tremaining: 359ms\n940:\tlearn: 0.0991176\ttotal: 5.64s\tremaining: 354ms\n941:\tlearn: 0.0990288\ttotal: 5.64s\tremaining: 348ms\n942:\tlearn: 0.0989232\ttotal: 5.65s\tremaining: 342ms\n943:\tlearn: 0.0988139\ttotal: 5.66s\tremaining: 336ms\n944:\tlearn: 0.0986370\ttotal: 5.66s\tremaining: 330ms\n945:\tlearn: 0.0985646\ttotal: 5.67s\tremaining: 324ms\n946:\tlearn: 0.0984888\ttotal: 5.67s\tremaining: 318ms\n947:\tlearn: 0.0983911\ttotal: 5.68s\tremaining: 312ms\n948:\tlearn: 0.0982593\ttotal: 5.69s\tremaining: 306ms\n949:\tlearn: 0.0981750\ttotal: 5.69s\tremaining: 300ms\n950:\tlearn: 0.0980409\ttotal: 5.7s\tremaining: 294ms\n951:\tlearn: 0.0979070\ttotal: 5.71s\tremaining: 288ms\n952:\tlearn: 0.0978261\ttotal: 5.71s\tremaining: 282ms\n953:\tlearn: 0.0976610\ttotal: 5.72s\tremaining: 276ms\n954:\tlearn: 0.0975559\ttotal: 5.72s\tremaining: 270ms\n955:\tlearn: 0.0974674\ttotal: 5.73s\tremaining: 264ms\n956:\tlearn: 0.0973091\ttotal: 5.74s\tremaining: 258ms\n957:\tlearn: 0.0971679\ttotal: 5.74s\tremaining: 252ms\n958:\tlearn: 0.0970541\ttotal: 5.75s\tremaining: 246ms\n959:\tlearn: 0.0969893\ttotal: 5.76s\tremaining: 240ms\n960:\tlearn: 0.0968907\ttotal: 5.76s\tremaining: 234ms\n961:\tlearn: 0.0967837\ttotal: 5.77s\tremaining: 228ms\n962:\tlearn: 0.0966760\ttotal: 5.77s\tremaining: 222ms\n963:\tlearn: 0.0965818\ttotal: 5.78s\tremaining: 216ms\n964:\tlearn: 0.0964708\ttotal: 5.79s\tremaining: 210ms\n965:\tlearn: 0.0963500\ttotal: 5.79s\tremaining: 204ms\n966:\tlearn: 0.0962377\ttotal: 5.8s\tremaining: 198ms\n967:\tlearn: 0.0961712\ttotal: 5.8s\tremaining: 192ms\n968:\tlearn: 0.0960609\ttotal: 5.81s\tremaining: 186ms\n969:\tlearn: 0.0959905\ttotal: 5.82s\tremaining: 180ms\n970:\tlearn: 0.0958586\ttotal: 5.82s\tremaining: 174ms\n971:\tlearn: 0.0957708\ttotal: 5.83s\tremaining: 168ms\n972:\tlearn: 0.0956376\ttotal: 5.83s\tremaining: 162ms\n973:\tlearn: 0.0955645\ttotal: 5.84s\tremaining: 156ms\n974:\tlearn: 0.0954248\ttotal: 5.85s\tremaining: 150ms\n975:\tlearn: 0.0953048\ttotal: 5.85s\tremaining: 144ms\n976:\tlearn: 0.0951696\ttotal: 5.86s\tremaining: 138ms\n977:\tlearn: 0.0950209\ttotal: 5.87s\tremaining: 132ms\n978:\tlearn: 0.0949332\ttotal: 5.87s\tremaining: 126ms\n979:\tlearn: 0.0948320\ttotal: 5.88s\tremaining: 120ms\n980:\tlearn: 0.0946904\ttotal: 5.88s\tremaining: 114ms\n981:\tlearn: 0.0946564\ttotal: 5.89s\tremaining: 108ms\n982:\tlearn: 0.0945854\ttotal: 5.89s\tremaining: 102ms\n983:\tlearn: 0.0944801\ttotal: 5.9s\tremaining: 95.9ms\n984:\tlearn: 0.0943619\ttotal: 5.91s\tremaining: 89.9ms\n985:\tlearn: 0.0942939\ttotal: 5.91s\tremaining: 83.9ms\n986:\tlearn: 0.0941691\ttotal: 5.92s\tremaining: 77.9ms\n987:\tlearn: 0.0940701\ttotal: 5.92s\tremaining: 71.9ms\n988:\tlearn: 0.0939806\ttotal: 5.93s\tremaining: 65.9ms\n989:\tlearn: 0.0938168\ttotal: 5.93s\tremaining: 59.9ms\n990:\tlearn: 0.0937292\ttotal: 5.94s\tremaining: 54ms\n991:\tlearn: 0.0936202\ttotal: 5.95s\tremaining: 48ms\n992:\tlearn: 0.0934110\ttotal: 5.95s\tremaining: 42ms\n993:\tlearn: 0.0932825\ttotal: 5.96s\tremaining: 36ms\n994:\tlearn: 0.0931514\ttotal: 5.96s\tremaining: 30ms\n995:\tlearn: 0.0930141\ttotal: 5.97s\tremaining: 24ms\n996:\tlearn: 0.0929065\ttotal: 5.97s\tremaining: 18ms\n997:\tlearn: 0.0927805\ttotal: 5.98s\tremaining: 12ms\n998:\tlearn: 0.0926660\ttotal: 5.99s\tremaining: 5.99ms\n999:\tlearn: 0.0925391\ttotal: 5.99s\tremaining: 0us\n","output_type":"stream"},{"execution_count":177,"output_type":"execute_result","data":{"text/plain":"<catboost.core.CatBoostClassifier at 0x79ab04c38400>"},"metadata":{}}],"execution_count":177},{"cell_type":"code","source":"test_pred = clf.predict(test_embeds)\ntest_proba = clf.predict_proba(test_embeds)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2025-05-09T16:34:36.850346Z","iopub.execute_input":"2025-05-09T16:34:36.850656Z","iopub.status.idle":"2025-05-09T16:34:36.865699Z","shell.execute_reply.started":"2025-05-09T16:34:36.850631Z","shell.execute_reply":"2025-05-09T16:34:36.865006Z"},"trusted":true},"outputs":[],"execution_count":178},{"cell_type":"code","source":"print(\"Accuracy:\", accuracy_score(target_test, test_pred))\nprint(\"ROC-AUC:\", roc_auc_score(target_test, test_proba))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T16:34:38.588478Z","iopub.execute_input":"2025-05-09T16:34:38.588782Z","iopub.status.idle":"2025-05-09T16:34:38.598249Z","shell.execute_reply.started":"2025-05-09T16:34:38.588749Z","shell.execute_reply":"2025-05-09T16:34:38.597504Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.746\nROC-AUC: 0.8056207605510677\n","output_type":"stream"}],"execution_count":179},{"cell_type":"code","source":"arr = np.array([])\n\narr.mean(), arr.std()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:45:01.669985Z","iopub.execute_input":"2025-05-07T18:45:01.670277Z","iopub.status.idle":"2025-05-07T18:45:01.675727Z","shell.execute_reply.started":"2025-05-07T18:45:01.670254Z","shell.execute_reply":"2025-05-07T18:45:01.674889Z"}},"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"(0.8081408212052041, 0.006105603736099413)"},"metadata":{}}],"execution_count":106},{"cell_type":"markdown","source":"<!-- - CPC context embeds w/ Aug + Catboost (dim of trx embeds: 32):\n  - `Accuracy: 0.752`, `0.748`, `0.742`, avg: `0.7473 +- 0.0041`\n  - `ROC-AUC: 0.8051836622363244`, `0.8137313626135242`, `0.810639296757378`, avg: `0.8099 +- 0.0035`\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc + Catboost:\n  - `Accuracy: 0.746`, `0.726`, `0.742`, avg: `0.738 +- 0.0086`\n  - `ROC-AUC: 0.8005050913859253`, `0.8013630991889397`, `0.8061711806511145`, avg: `0.8027 +- 0.0025`\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc + Catboost (no look-ahead mask on inference):\n  - `Accuracy: 0.75`, `0.734`, `0.744`, avg: `0.7427 +- 0.0066`\n  - `ROC-AUC: 0.8118210810898319`, `0.7969759272150362`, `0.8073529649835682`, avg: `0.8054 +- 0.0062`\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc + Catboost (no look-ahead mask on inference + no start-end fusion):\n  - `Accuracy: 0.744`, `0.76`, `0.754`, avg: `0.7527 +- 0.0066`\n  - `ROC-AUC: 0.8094737012513963`, `0.809117547069013`, `0.807547230901232`, avg: `0.8087 +- 0.0008`\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc + ConvAgg (3 trx) + Catboost:\n  - `Accuracy: 0.734`, `0.742`, `0.744`, avg: `0.74 +- 0.0043`\n  - `ROC-AUC: 0.8022534846448982`, `0.818442311116867`, `0.8112059056838969`, avg: `0.8106 +- 0.0066`\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc + ConvAgg (5 trx) + Catboost:\n  - `Accuracy: 0.728`, `0.724`, `0.73`, avg: `0.7273 +- 0.0025`\n  - `ROC-AUC: 0.7972997037444756`, `0.803872367292095`, `0.7975425361415551`, avg: `0.7996 +- 0.003`\n\n---\n\nlr = 5e-5\n\n- `Accuracy: 0.746`, `0.748`, `0.738`, avg: `0.744 +- 0.0043`\n\n- `ROC-AUC: 0.8049893963186608`, `0.8117239481309999`, `0.8046008644833336`, avg: `0.8071 +- 0.0033`\n\nW/ look-ahead mask on inference:\n\n- `Accuracy: 0.756`, `0.736`, `0.726`, avg: `0.7393 +- 0.0125`\n\n- `ROC-AUC: 0.8037266678538473`, `0.8073853426365124`, `0.8133104531252531`, avg: `0.8081 +- 0.0039`\n\n---\n\n**W/ grad_norm_clipping**\n\n- `Accuracy: 0.742`, `0.748`, `0.752`, avg: `0.7473 +- 0.0041`\n- `ROC-AUC: 0.8075148532482881`, `0.8072882096776806`, `0.8169205614285021`, avg: `0.8106 +- 0.0045`\n\nW/ look-ahead mask on inference:\n\n- `Accuracy: 0.752`, `0.722`, `0.74`, avg: `0.738 +- 0.8022858622978419`\n- `ROC-AUC: 0.8165644072461188`, `0.8055721940716518`, `0.8022858622978419`, avg: `0.8081 +- 0.0061`\n\n---\n\nSmaller Window (4 trx -> 2 trx):\n\n- `Accuracy: 0.74`, ``, ``\n- `ROC-AUC: 0.8044389762186138`, ``, ``\n\nW/ look-ahead mask on inference:\n\n- `Accuracy: 0.728`, ``, ``\n- `ROC-AUC: 0.8029010377037767`, ``, ``\n\n---\n\nSmaller Window (4 trx -> 2 trx), changed lr scheduler:\n\n- `Accuracy: 0.746`, `0.734`, `0.732`, avg: ``\n\n- `ROC-AUC: 0.8099917436984992`, `0.8003432031212058`, `0.8128895436369817`, avg: ``\n\nW/ look-ahead mask on inference:\n\n- `Accuracy: 0.746`, `0.744`, `0.752`, avg: ``\n\n- `ROC-AUC: 0.8107849961956259`, `0.8039695002509268`, `0.813828495572356`, avg: ``\n\n---\n\nConvAgg (3 trx):\n\n- `Accuracy: 0.734`, `0.734`, `0.74`, avg: `0.736 +- 0.0028`\n- `ROC-AUC: 0.8006184131712293`, `0.8067539784041056`, `0.8040666332097586`, avg: `0.8038 +- 0.0025`\n\n---\n\nConvAgg (5 trx):\n\n- `Accuracy: 0.724`, `0.718`, `0.728`, avg: `0.7233 +- 0.0041`\n- `ROC-AUC: 0.7985786210357612`, `0.7871654983730229`, `0.7939486166647779`, avg: `0.7932 +- 0.0047`\n\n---\n\nNo RNN for feat agg:\n  - `Accuracy: 0.686`\n  - `ROC-AUC: 0.7611338654060966`\n\n**Вывод:** SWIN без RNN не стоит использовать. \n\n---\n\n**Вывод:** агрегация с помощью свёрток независимо от размера окна заметно улучшает ROC-AUC, при этом accuracy также c увеличением окна несколько вырастает, но не так существенно. Что интересно, с увеличением окна ROC-AUC только увеличивается, хотя, кажется, что рано или поздно результат выйдет на плато и затем начнёт ухудшаться.\n\n**Лучший результат:**\n\n- CPC context embeds + ConvAgg (7 trx) + Catboost:\n  - `Accuracy: 0.744`, `0.758`, `0.75`, avg: `0.7507 +- 0.0057`\n  - `ROC-AUC: 0.8150912240371695`, `0.82635864726166`, `0.8121610464457432`, avg: `0.8179 +- 0.0061`\n\n---\n\n**Результаты для CPC с меньшей размерностью embed_dim (8):**\n\n- CPC context embeds w/ Aug + Catboost (dim of trx embeds: 8):\n  - `Accuracy: 0.754`, `0.742`, `0.744`, avg: `0.7467 +- 0.0052`\n  - `ROC-AUC: 0.8175195480079649`, `0.8197697948875686`, `0.8122096129251591`, avg: `0.8165 +- 0.0032` -->","metadata":{}},{"cell_type":"markdown","source":"- CPC context embeds w/ Aug + Catboost (dim of trx embeds: 32):\n  - `Accuracy: 0.752`, `0.748`, `0.742`, avg: `0.7473 +- 0.0041`\n  - `ROC-AUC: 0.8051836622363244`, `0.8137313626135242`, `0.810639296757378`, avg: `0.8099 +- 0.0035`\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc + Catboost:\n    - `Accuracy: 0.744`, `0.746`, `0.75`, avg: `0.7467 +- 0.0025`\n    - `ROC-AUC: 0.8105745414514903`, `0.8218419646759807`, `0.8159006653607681`, avg: `0.8161 +- 0.0046`\n\n<!-- ---\n\n- Smaller model:\n    - `Accuracy: 0.74`, `0.746`, `0.74`, avg: `0.742 +- 0.0028`\n    - `ROC-AUC: 0.8124362564957667`, `0.8266986126175713`, `0.8113192274692007`, avg: `0.8168 +- 0.007` -->\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc (w/ look-ahead mask) + Catboost:\n    - `Accuracy: 0.744`, `0.746`, `0.738`, avg: `0.7427 +- 0.0034`\n    - `ROC-AUC: 0.8105745414514901`, `0.8208058797817748`, `0.8069320554952971`, avg: `0.8128 +- 0.0059`\n\n<!-- ---\n\n- Smaller model:\n    - `Accuracy: 0.744`, `0.766`, `0.736`, avg: `0.7487 +- 0.0127`\n    - `ROC-AUC: 0.8064625795276101`, `0.8253711288468699`, `0.8082271616130545`, avg: `0.8134 +- 0.0085` -->\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc (smaller windows) + Catboost:\n    - `Accuracy: 0.754`, `0.748`, `0.742`, avg: `0.748 +- 0.0049`\n    - `ROC-AUC: 0.8113030386427289`, `0.8168881837755582`, `0.8082919169189426`, avg: `0.8122 +- 0.0036`\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc & ConvAgg (3 trx) + Catboost:\n    - `Accuracy: 0.746`, `0.736`, `0.734`, avg: `0.7387 +- 0.0052`\n    - `ROC-AUC: 0.8182804228521475`, `0.7999546712858785`, `0.806494957180554`, avg: `0.8082 +- 0.0076`\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc (smaller windows) & ConvAgg (3 trx) + Catboost:\n    - `Accuracy: 0.72`, `0.742`, `0.746`, avg: `0.736 +- 0.0114`\n    - `ROC-AUC: 0.8085509381424941`, `0.8239950785967526`, `0.8056207605510677`, avg: `0.8127 +- 0.0081`\n\n---\n\n**Вывод:** CPC с SWIN-трансформером в качестве seq_encoder'а демонстрирует сравнимое с бейзлайном значение accuracy и гораздо более высокий ROC-AUC по сравнению с ним. На тестовой выборке пробовались сетапы с наличием look-ahead маски, не дающей \"смотреть\" модели в будущее (это обязательно для обучения, так как CPC должен работать с будущими сэмплами и они не должны при этом попадать в контекст такой модели) и без неё. Без такой маски на тесте оказалось значительно лучше - как по accuracy, так и по ROC-AUC. \n\nПо умолчанию стартовое окно покрывает 4 транзакции, также пробовался сетап с размером окна в 2 транзакции, он демонстрировал accuracy чуть лучше, чем при стандартном размере окна и чем у бейзлайна, но также - достаточно большую просадку по ROC-AUC (при этом такое значение ROC-AUC всё ещё лучше, чем для бейзлайна).\n\nТакже пробовалась архитектура (Conv Aggregator в качестве trx_enc +  SWIN + RNN SeqEncoder). Размер свёртки Conv Aggregator'а брался как минимальный (3) из рассматриваемых, чтобы не увеличивать слишком сильно за счёт свёртки - и далее - SWIN-трансформера - receptive field. Такой вариант продемонстрировал качество хуже, чем для SWIN-трансформера с обычным trx_encoder'ом (сильные просадки по accuracy и - почти во всех случаях - ROC-AUC хуже, чем у базового варианта). При этом по ROC-AUC такие методы всё ещё лучше или сравнимы с бейзлайном.   \n\n**Лучшие результаты:**\n\n- CPC context embeds w/ SWIN_Agg seq_enc + Catboost:\n    - `Accuracy: 0.744`, `0.746`, `0.75`, avg: `0.7467 +- 0.0025`\n    - `ROC-AUC: 0.8105745414514903`, `0.8218419646759807`, `0.8159006653607681`, avg: `0.8161 +- 0.0046`\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc (smaller window size) + Catboost:\n    - `Accuracy: 0.754`, `0.748`, `0.742`, avg: `0.748 +- 0.0049`\n    - `ROC-AUC: 0.8113030386427289`, `0.8168881837755582`, `0.8082919169189426`, avg: `0.8122 +- 0.0036`","metadata":{}},{"cell_type":"markdown","source":"# Итоги.","metadata":{}},{"cell_type":"markdown","source":"| Method                                 |    Accuracy           | ROC-AUC         |\n|----------------------------------------|-----------------------|-----------------|\n| **Flattened Sequences**                | 0.67 ± 0.0046         | 0.7536 ± 0.003  |\n| **GRU (+ MLP)**                        | 0.746 ± 0.0076        | 0.8148 ± 0.0037 |\n| **CoLES**                              | 0.726 ± 0.0071        | 0.8076 ± 0.0025 |\n| **COLES embeds + SWIN Agg encoder & ConvAgg (3 trx)** | 0.7527 ± 0.0093       | 0.81 ± 0.0052 |\n| **CPC Modeling (emb_dim=32)**          | 0.747 ± 0.0041        | 0.8099 ± 0.0035 |\n| **CPC Modeling (emb_dim=32) w/ SWIN Agg encoder** | 0.7467 ± 0.0025       | 0.8161 ± 0.0046 |\n| **CPC Modeling (emb_dim=32) w/ SWIN Agg encoder (smaller win size)** | 0.748 ± 0.0049       | 0.8122 ± 0.0036 |","metadata":{}}]}