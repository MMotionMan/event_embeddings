{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyNf/d19uVxFxPMfFmtk3j7z"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Импортируем необходимые библиотеки","metadata":{}},{"cell_type":"code","source":"!pip install pytorch-lifestream\n!pip install comet_ml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:50:08.092841Z","iopub.execute_input":"2025-05-11T10:50:08.093176Z","iopub.status.idle":"2025-05-11T10:50:41.733947Z","shell.execute_reply.started":"2025-05-11T10:50:08.093137Z","shell.execute_reply":"2025-05-11T10:50:41.732763Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting pytorch-lifestream\n  Downloading pytorch-lifestream-0.6.0.tar.gz (163 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.4/163.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting duckdb (from pytorch-lifestream)\n  Downloading duckdb-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (966 bytes)\nCollecting hydra-core>=1.1.2 (from pytorch-lifestream)\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: numpy>=1.21.5 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream) (1.26.4)\nCollecting omegaconf (from pytorch-lifestream)\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: pandas>=1.3.5 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream) (2.2.3)\nRequirement already satisfied: pyarrow>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream) (17.0.0)\nRequirement already satisfied: pytorch-lightning>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream) (2.4.0)\nRequirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream) (1.2.2)\nRequirement already satisfied: torch>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream) (2.4.0)\nRequirement already satisfied: torchmetrics>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream) (1.6.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from pytorch-lifestream) (4.46.3)\nCollecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1.2->pytorch-lifestream)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from hydra-core>=1.1.2->pytorch-lifestream) (21.3)\nRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from omegaconf->pytorch-lifestream) (6.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->pytorch-lifestream) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->pytorch-lifestream) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.5->pytorch-lifestream) (2024.1)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning>=1.6.0->pytorch-lifestream) (4.66.4)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (2024.6.0)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning>=1.6.0->pytorch-lifestream) (4.12.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning>=1.6.0->pytorch-lifestream) (0.11.9)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.2->pytorch-lifestream) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.2->pytorch-lifestream) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0.2->pytorch-lifestream) (3.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch-lifestream) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch-lifestream) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch-lifestream) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch-lifestream) (3.1.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers->pytorch-lifestream) (0.26.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->pytorch-lifestream) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->pytorch-lifestream) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->pytorch-lifestream) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->pytorch-lifestream) (0.4.5)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (3.9.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (70.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->hydra-core>=1.1.2->pytorch-lifestream) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-lifestream) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12.0->pytorch-lifestream) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->pytorch-lifestream) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->pytorch-lifestream) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->pytorch-lifestream) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->pytorch-lifestream) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12.0->pytorch-lifestream) (1.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.6.0->pytorch-lifestream) (4.0.3)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading duckdb-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pytorch-lifestream, antlr4-python3-runtime\n  Building wheel for pytorch-lifestream (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorch-lifestream: filename=pytorch_lifestream-0.6.0-py3-none-any.whl size=274670 sha256=89ac81f9ed80e1a3bf63b8d6d94e1ff84169e81ec0bbcb8fe465c3ddb8a0b3f5\n  Stored in directory: /root/.cache/pip/wheels/90/76/b4/0a944bc7c5a69201e4d757cc54886971117a2a581740e7f11d\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=b7ddedb576419b42b08b481601a0c1df158a9c688c7f179eea6c8831372510ef\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\nSuccessfully built pytorch-lifestream antlr4-python3-runtime\nInstalling collected packages: antlr4-python3-runtime, omegaconf, duckdb, hydra-core, pytorch-lifestream\nSuccessfully installed antlr4-python3-runtime-4.9.3 duckdb-1.2.2 hydra-core-1.3.2 omegaconf-2.3.0 pytorch-lifestream-0.6.0\nCollecting comet_ml\n  Downloading comet_ml-3.49.9-py3-none-any.whl.metadata (4.1 kB)\nCollecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n  Downloading dulwich-0.22.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (4.22.0)\nRequirement already satisfied: psutil>=5.6.3 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (5.9.3)\nCollecting python-box<7.0.0 (from comet_ml)\n  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\nRequirement already satisfied: requests-toolbelt>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (0.10.1)\nRequirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (2.32.3)\nRequirement already satisfied: rich>=13.3.2 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (13.7.1)\nCollecting semantic-version>=2.8.0 (from comet_ml)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: sentry-sdk>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (2.19.0)\nCollecting simplejson (from comet_ml)\n  Downloading simplejson-3.20.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.26.18)\nRequirement already satisfied: wrapt>=1.11.2 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.16.0)\nRequirement already satisfied: wurlitzer>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (3.1.1)\nCollecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.18.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.18.4->comet_ml) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.18.4->comet_ml) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.18.4->comet_ml) (2024.6.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=13.3.2->comet_ml) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=13.3.2->comet_ml) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\nDownloading comet_ml-3.49.9-py3-none-any.whl (726 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.0/727.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dulwich-0.22.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\nDownloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading simplejson-3.20.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\nInstalling collected packages: everett, simplejson, semantic-version, python-box, dulwich, configobj, comet_ml\nSuccessfully installed comet_ml-3.49.9 configobj-5.0.9 dulwich-0.22.8 everett-3.1.0 python-box-6.1.0 semantic-version-2.10.0 simplejson-3.20.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# data preprocessing\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\n\n# misc\nfrom tqdm import tqdm\nfrom functools import partial\n\n# logging\nimport comet_ml\n\n# classical ML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom catboost import CatBoostClassifier\n\n# basic deep learning libs\nimport torch\nimport pytorch_lightning as pl\nimport torchmetrics\n\n# ptls\nfrom ptls.nn import TrxEncoder, RnnSeqEncoder, TransformerEncoder, GptEncoder, Head\nfrom ptls.frames import PtlsDataModule\nfrom ptls.frames.coles import CoLESModule\nfrom ptls.frames.coles import ColesDataset\nfrom ptls.frames.coles.split_strategy import SampleSlices\nfrom ptls.frames.cpc import CpcModule\nfrom ptls.frames.cpc import CpcDataset\nfrom ptls.frames.gpt import GptDataset\nfrom ptls.frames.supervised import SeqToTargetDataset, SequenceToTarget\nfrom ptls.data_load.datasets import MemoryMapDataset\nfrom ptls.data_load.datasets import inference_data_loader\nfrom ptls.frames.inference_module import InferenceModule\nfrom ptls.data_load.iterable_processing import SeqLenFilter\nfrom ptls.preprocessing import PandasDataPreprocessor\nfrom ptls.data_load.utils import collate_feature_dict\nfrom ptls.frames.inference_module import InferenceModule\nfrom ptls.frames.coles.losses.softmax_loss import SoftmaxLoss","metadata":{"id":"4KxO1qd6hUTG","executionInfo":{"status":"ok","timestamp":1732826830664,"user_tz":-180,"elapsed":233,"user":{"displayName":"Антон Коротков","userId":"12465883653538096788"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:50:56.873914Z","iopub.execute_input":"2025-05-11T10:50:56.874711Z","iopub.status.idle":"2025-05-11T10:51:16.446859Z","shell.execute_reply.started":"2025-05-11T10:50:56.874655Z","shell.execute_reply":"2025-05-11T10:51:16.445940Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def seed_everything(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:51:22.943211Z","iopub.execute_input":"2025-05-11T10:51:22.944401Z","iopub.status.idle":"2025-05-11T10:51:22.949512Z","shell.execute_reply.started":"2025-05-11T10:51:22.944350Z","shell.execute_reply":"2025-05-11T10:51:22.948546Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"comet_ml.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:51:45.339097Z","iopub.execute_input":"2025-05-11T10:51:45.339927Z","iopub.status.idle":"2025-05-11T10:51:45.659313Z","shell.execute_reply.started":"2025-05-11T10:51:45.339890Z","shell.execute_reply":"2025-05-11T10:51:45.658219Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from pytorch_lightning.loggers import CometLogger","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:51:47.330536Z","iopub.execute_input":"2025-05-11T10:51:47.331500Z","iopub.status.idle":"2025-05-11T10:51:47.335907Z","shell.execute_reply.started":"2025-05-11T10:51:47.331450Z","shell.execute_reply":"2025-05-11T10:51:47.334971Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**SWIN1D_Encoder (orig. implementation by Yukara Ikemiya):**","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------------------------------------------------------\n# Based on https://github.com/yukara-ikemiya/Swin-Transformer-1d/tree/main and adapted to pytorch-lifestream\n#------------------------------------------------------------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nfrom ptls.data_load.padded_batch import PaddedBatch\n\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n    # copied from timm/models/layers/drop.py\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:\n        random_tensor.div_(keep_prob)\n    return x * random_tensor\n\n\nclass DropPath(nn.Module):\n    # copied from timm/models/layers/drop.py\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None, scale_by_keep=True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, L, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, C)\n    \"\"\"\n    B, L, C = x.shape\n    x = x.view(B, L // window_size, window_size, C)\n    windows = x.contiguous().view(-1, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, L):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, C)\n        window_size (int): Window size\n        L (int): Length of data\n\n    Returns:\n        x: (B, L, C)\n    \"\"\"\n    B = int(windows.shape[0] / (L / window_size))\n    x = windows.view(B, L // window_size, window_size, -1)\n    x = x.contiguous().view(B, L, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (int): The width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim: int, window_size: int, num_heads: int,\n                 qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(2 * window_size - 1, num_heads))  # 2*window_size - 1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_w = torch.arange(self.window_size)\n        relative_coords = coords_w[:, None] - coords_w[None, :]  # W, W\n        relative_coords[:, :] += self.window_size - 1  # shift to start from 0\n\n        # relative_position_index | example\n        # [2, 1, 0]\n        # [3, 2, 1]\n        # [4, 3, 2]\n        self.register_buffer(\"relative_position_index\", relative_coords)  # (W, W): range of 0 -- 2*(W-1)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        torch.nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask_add, mask_mult):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, W, C)\n            mask: (0/-inf) mask with shape of (num_windows, W, W) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size, self.window_size, -1)  # W, W, nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, W, W\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        nW = mask_add.shape[1]\n        attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask_add\n        attn = attn.view(-1, self.num_heads, N, N)\n        attn = self.softmax(attn)\n        attn = attn * mask_mult\n        \n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        \n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n        decoder (bool, optional): Flag that shows whether this block is decoder-like (hence, attn_mask should prevent from seeing future tokens). True => decoder-like; False => encoder-like. Default: False\n        start_end_fusion (bool, optional): Flag that shows if the last and the first half-windows should merge (True) or not (False).\n    \"\"\"\n\n    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 decoder=False, start_end_fusion=True):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=self.window_size, num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        attn_mask = None\n        self.register_buffer(\"attn_mask\", attn_mask)\n\n        self.decoder = decoder\n        self.start_end_fusion = start_end_fusion\n\n    def forward(self, x):\n        seq_lens = x.seq_lens\n        x = x.payload\n        \n        B, L, C = x.shape\n\n        # define seq_len_mask\n        mask = torch.arange(L, device=x.device)[None, :] + torch.ones((B, L), device=x.device)\n        mask[mask > seq_lens[:, None]] = 0.\n        mask[mask > 0.] = 1.\n        mask = mask[:, :, None]\n\n        # make new max seq_len `L` divisible by `self.window_size` by adding 'zero' samples\n        num_samples_to_add = self.window_size - (L % self.window_size)\n        \n        if num_samples_to_add < self.window_size:\n            additional_samples = torch.zeros((B, num_samples_to_add, C), device=x.device)\n            x = torch.cat((x, additional_samples), dim=1)\n            mask_additional_samples = torch.zeros((B, num_samples_to_add, mask.shape[2]), device=mask.device)\n            mask = torch.cat((mask, mask_additional_samples), dim=1)\n            L += num_samples_to_add\n\n        # zero out padding transactions\n        x = x * mask\n        \n        assert L >= self.window_size, f'input length ({L}) must be >= window size ({self.window_size})'\n        assert L % self.window_size == 0, f'input length ({L}) must be divisible by window size ({self.window_size})'\n\n        shortcut = x\n        x = self.norm1(x)\n\n        # shift\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=-self.shift_size, dims=1) # cyclic shift \n            if not self.start_end_fusion:\n                shifted_x[:, -self.shift_size:] = 0. # zero out invalid embs\n            mask = torch.roll(mask, shifts=-self.shift_size, dims=1) # cyclic shift of the mask\n            if not self.start_end_fusion:\n                mask[:, -self.shift_size:] = 0.\n        else:\n            shifted_x = x\n        \n        # partition\n        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, C\n        mask = window_partition(mask, self.window_size) # nW*B, window_size, 1\n        \n        # calculate attn_mask\n        attn_mask = (mask @ mask.transpose(-2, -1)) # nW*B, window_size, window_size\n        \n        if self.decoder:\n            no_look_ahead_attn_mask = 1. - torch.triu(torch.ones_like(attn_mask), diagonal=1)\n            attn_mask *= no_look_ahead_attn_mask\n        \n        attn_mask_real = attn_mask.clone().detach()\n        attn_mask_real = attn_mask_real.view(attn_mask_real.shape[0], self.window_size, self.window_size).unsqueeze(1).expand(-1, self.num_heads, -1, -1) # B*nW, nH, window_size, window_size\n        \n        attn_mask[attn_mask == 0.] = -torch.inf\n        attn_mask[attn_mask == 1.] = 0.\n        attn_mask[:, torch.arange(attn_mask.shape[-1]), torch.arange(attn_mask.shape[-1])] = 0.\n        attn_mask = attn_mask.view(B, attn_mask.shape[0] // B, self.window_size, self.window_size).unsqueeze(2).expand(-1, -1, self.num_heads, -1, -1) # B, nW, nH, window_size, window_size\n        \n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask_add=attn_mask, mask_mult=attn_mask_real)  # nW*B, window_size, C\n        \n        # merge windows\n        shifted_x = window_reverse(attn_windows, self.window_size, L)  # (B, L, C)\n\n        # reverse zero-padding shift\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=self.shift_size, dims=1) # cyclic shift\n            if not self.start_end_fusion:\n                x[:, :self.shift_size] = 0. # zero out invalid embs\n        else:\n            x = shifted_x\n\n        x = shortcut + self.drop_path(x)\n\n        # FFN\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        \n        return PaddedBatch(x, seq_lens)\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n\n\nclass SwinTransformerLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        decoder (bool, optional): Flag that shows whether blocks in this layer are decoder-like. True => decoder-like; False => encoder-like. Default: False\n        start_end_fusion (bool, optional): Flag that shows if the last and the first half-windows should merge (True) or not (False).\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        depth: int,\n        num_heads: int,\n        window_size: int,\n        mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n        drop_path=0., norm_layer=nn.LayerNorm,\n        decoder=False, start_end_fusion=True\n    ):\n        super().__init__()\n        self.dim = dim\n        self.depth = depth\n        self.num_heads = num_heads\n        self.window_size = window_size\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer,\n                                 decoder=decoder,\n                                 start_end_fusion=start_end_fusion)\n            for i in range(depth)])\n\n    def forward(self, x):\n        for blk in self.blocks:\n            x = blk(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, depth={self.depth}, num_heads={self.num_heads}, window_size={self.window_size}\"\n\n\nclass SwinTransformerBackbone(nn.Module):\n    \"\"\" Swin Transformer Backbone (4 stages as in orig. 2D impl.).\n\n    Args:\n        dim (int): Number of input channels.\n        depths (list[int]): Numbers of blocks in stages.\n        num_heads (int): Number of attention heads in W-MSA layers.\n        start_window_size (int): Local window size of stage 1.\n        window_size_mult (int): the number by which the `window_size` is being multiplied when moving to another stage\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        decoder (bool, optional): Flag that shows whether blocks in this backbone are decoder-like. True => decoder-like; False => encoder-like. Default: False\n        start_end_fusion (bool, optional): Flag that shows if the last and the first half-windows should merge (True) or not (False).\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        depths: list[int],\n        num_heads,\n        start_window_size: int,\n        window_size_mult: int = 1,\n        mlp_ratio=4.,\n        qkv_bias=True,\n        qk_scale=None,\n        drop=0.,\n        attn_drop=0.,\n        drop_path=0.,\n        norm_layer=nn.LayerNorm,\n        decoder=False,\n        start_end_fusion=True\n    ):\n        super().__init__()\n        self.dim = dim\n        self.depths = depths\n        \n        if type(num_heads) == int:\n            self.num_heads = [num_heads] * len(depths)\n        else:\n            self.num_heads = num_heads\n        \n        self.window_sizes = [start_window_size]\n        \n        for i in range(len(self.depths) - 1):\n            self.window_sizes += [self.window_sizes[-1] * window_size_mult]\n\n        # build model\n        self.backbone = nn.ModuleList([\n            SwinTransformerLayer(dim=self.dim,\n                                 depth=self.depths[i],\n                                 num_heads=self.num_heads[i],\n                                 window_size=self.window_sizes[i],\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias,\n                                 qk_scale=qk_scale,\n                                 drop=drop,\n                                 attn_drop=attn_drop,\n                                 drop_path=drop_path,\n                                 norm_layer=norm_layer,\n                                 decoder=decoder,\n                                 start_end_fusion=start_end_fusion)\n            for i in range(len(self.depths))])\n\n    def forward(self, x):\n        for layer in self.backbone:\n            x = layer(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:51:50.289191Z","iopub.execute_input":"2025-05-11T10:51:50.289618Z","iopub.status.idle":"2025-05-11T10:51:50.330258Z","shell.execute_reply.started":"2025-05-11T10:51:50.289574Z","shell.execute_reply":"2025-05-11T10:51:50.329528Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def change_to_enc(swin_model):\n    for i in range(len(swin_model.backbone)):\n        for j in range(len(swin_model.backbone[i].blocks)):\n            swin_model.backbone[i].blocks[j].decoder = False\n\ndef change_to_dec(swin_model):\n    for i in range(len(swin_model.backbone)):\n        for j in range(len(swin_model.backbone[i].blocks)):\n            swin_model.backbone[i].blocks[j].decoder = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:52:03.058453Z","iopub.execute_input":"2025-05-11T10:52:03.059411Z","iopub.status.idle":"2025-05-11T10:52:03.065413Z","shell.execute_reply.started":"2025-05-11T10:52:03.059362Z","shell.execute_reply":"2025-05-11T10:52:03.064338Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Эксперименты.","metadata":{}},{"cell_type":"markdown","source":"**Данные:**","metadata":{}},{"cell_type":"code","source":"path_data = \"https://huggingface.co/datasets/dllllb/age-group-prediction/resolve/main/transactions_train.csv.gz?download=true\"\ndata = pd.read_csv(path_data, compression=\"gzip\")\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:52:05.192376Z","iopub.execute_input":"2025-05-11T10:52:05.193058Z","iopub.status.idle":"2025-05-11T10:52:15.228231Z","shell.execute_reply.started":"2025-05-11T10:52:05.193024Z","shell.execute_reply":"2025-05-11T10:52:15.227302Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"          client_id  trans_date  small_group  amount_rur\n0             33172           6            4      71.463\n1             33172           6           35      45.017\n2             33172           8           11      13.887\n3             33172           9           11      15.983\n4             33172          10           11      21.341\n...             ...         ...          ...         ...\n26450572      43300         727           25       7.602\n26450573      43300         727           15       3.709\n26450574      43300         727            1       6.448\n26450575      43300         727           11      24.669\n26450576      43300         729            3      19.408\n\n[26450577 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>client_id</th>\n      <th>trans_date</th>\n      <th>small_group</th>\n      <th>amount_rur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33172</td>\n      <td>6</td>\n      <td>4</td>\n      <td>71.463</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>33172</td>\n      <td>6</td>\n      <td>35</td>\n      <td>45.017</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33172</td>\n      <td>8</td>\n      <td>11</td>\n      <td>13.887</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>33172</td>\n      <td>9</td>\n      <td>11</td>\n      <td>15.983</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33172</td>\n      <td>10</td>\n      <td>11</td>\n      <td>21.341</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>26450572</th>\n      <td>43300</td>\n      <td>727</td>\n      <td>25</td>\n      <td>7.602</td>\n    </tr>\n    <tr>\n      <th>26450573</th>\n      <td>43300</td>\n      <td>727</td>\n      <td>15</td>\n      <td>3.709</td>\n    </tr>\n    <tr>\n      <th>26450574</th>\n      <td>43300</td>\n      <td>727</td>\n      <td>1</td>\n      <td>6.448</td>\n    </tr>\n    <tr>\n      <th>26450575</th>\n      <td>43300</td>\n      <td>727</td>\n      <td>11</td>\n      <td>24.669</td>\n    </tr>\n    <tr>\n      <th>26450576</th>\n      <td>43300</td>\n      <td>729</td>\n      <td>3</td>\n      <td>19.408</td>\n    </tr>\n  </tbody>\n</table>\n<p>26450577 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"path_target = \"https://huggingface.co/datasets/dllllb/age-group-prediction/resolve/main/train_target.csv?download=true\"\ntarget = pd.read_csv(path_target)\ntarget","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:52:34.480715Z","iopub.execute_input":"2025-05-11T10:52:34.481088Z","iopub.status.idle":"2025-05-11T10:52:34.863733Z","shell.execute_reply.started":"2025-05-11T10:52:34.481058Z","shell.execute_reply":"2025-05-11T10:52:34.862943Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"       client_id  bins\n0          24662     2\n1           1046     0\n2          34089     2\n3          34848     1\n4          47076     3\n...          ...   ...\n29995      14303     1\n29996      22301     2\n29997      25731     0\n29998      16820     3\n29999       5265     0\n\n[30000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>client_id</th>\n      <th>bins</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24662</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1046</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34089</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>34848</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>47076</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>29995</th>\n      <td>14303</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29996</th>\n      <td>22301</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>29997</th>\n      <td>25731</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29998</th>\n      <td>16820</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>29999</th>\n      <td>5265</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>30000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"target_train, target_test = train_test_split(target, test_size=0.1, stratify=target[\"bins\"], random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:52:38.117637Z","iopub.execute_input":"2025-05-11T10:52:38.118002Z","iopub.status.idle":"2025-05-11T10:52:38.135904Z","shell.execute_reply.started":"2025-05-11T10:52:38.117971Z","shell.execute_reply":"2025-05-11T10:52:38.134962Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"trx_data_train = pd.merge(data, target_train[\"client_id\"], on=\"client_id\", how=\"inner\")\ntrx_data_test = pd.merge(data, target_test[\"client_id\"], on=\"client_id\", how=\"inner\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:52:41.114772Z","iopub.execute_input":"2025-05-11T10:52:41.115549Z","iopub.status.idle":"2025-05-11T10:52:44.100611Z","shell.execute_reply.started":"2025-05-11T10:52:41.115514Z","shell.execute_reply":"2025-05-11T10:52:44.099854Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Квантизация непрерывных признаков (опциональный шаг, нужен только для GPT):**","metadata":{}},{"cell_type":"code","source":"def digitize(input_array: np.array, q_count: int = 1, bins: np.array = None):\n    \"\"\"Quantile-based discretization function.\n\n    Parameters:\n    -------\n    input_array (np.array): Input array.\n    q_count (int): Amount of quantiles. Used only if input parameter `bins` is None.\n    bins (np.array):\n        If None, then calculate bins as quantiles of input array,\n        otherwise only apply bins to input_array. Default: None\n\n    Returns\n    -------\n    out_array (np.array of ints): discretized input_array\n    bins (np.array of floats):\n        Returned only if input parameter `bins` is None.\n    \"\"\"\n\n    if bins is None:\n        return_bins = True\n        bins = np.quantile(input_array, q=[i / q_count for i in range(1, q_count)], axis=0)\n    else:\n        return_bins = False\n\n    out_array = np.digitize(input_array, bins)\n\n    if return_bins:\n        return out_array, bins\n    else:\n        return out_array","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:59:52.139167Z","iopub.execute_input":"2025-04-24T13:59:52.139542Z","iopub.status.idle":"2025-04-24T13:59:52.144441Z","shell.execute_reply.started":"2025-04-24T13:59:52.139488Z","shell.execute_reply":"2025-04-24T13:59:52.143531Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"BINS_NUM = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:59:54.881958Z","iopub.execute_input":"2025-04-24T13:59:54.882244Z","iopub.status.idle":"2025-04-24T13:59:54.885782Z","shell.execute_reply.started":"2025-04-24T13:59:54.882222Z","shell.execute_reply":"2025-04-24T13:59:54.884830Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"numeric_features = [\"amount_rur\"]\n\nfor feat in numeric_features:\n    trx_data_train[feat], bins = digitize(trx_data_train[feat], q_count=BINS_NUM)\n    trx_data_test[feat] = digitize(trx_data_test[feat], bins=bins)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T13:59:56.530752Z","iopub.execute_input":"2025-04-24T13:59:56.531033Z","iopub.status.idle":"2025-04-24T13:59:59.023989Z","shell.execute_reply.started":"2025-04-24T13:59:56.531011Z","shell.execute_reply":"2025-04-24T13:59:59.023308Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import gc\n\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T14:00:01.265630Z","iopub.execute_input":"2025-04-24T14:00:01.265926Z","iopub.status.idle":"2025-04-24T14:00:01.613844Z","shell.execute_reply.started":"2025-04-24T14:00:01.265904Z","shell.execute_reply":"2025-04-24T14:00:01.613082Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"147"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"preprocessor = PandasDataPreprocessor(\n    col_id=\"client_id\",\n    col_event_time=\"trans_date\",\n    event_time_transformation=\"none\",\n    cols_category=[\"small_group\"],\n    cols_numerical=[\"amount_rur\"],\n    return_records=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:53:20.147113Z","iopub.execute_input":"2025-05-11T10:53:20.147848Z","iopub.status.idle":"2025-05-11T10:53:20.152169Z","shell.execute_reply.started":"2025-05-11T10:53:20.147784Z","shell.execute_reply":"2025-05-11T10:53:20.151094Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"data_train = preprocessor.fit_transform(trx_data_train)\ndata_test = preprocessor.transform(trx_data_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:53:23.055045Z","iopub.execute_input":"2025-05-11T10:53:23.055614Z","iopub.status.idle":"2025-05-11T10:54:04.652358Z","shell.execute_reply.started":"2025-05-11T10:53:23.055578Z","shell.execute_reply":"2025-05-11T10:54:04.651236Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"target_train.rename(columns={\"bins\": \"target\"}, inplace=True)\ntarget_test.rename(columns={\"bins\": \"target\"}, inplace=True)\ntarget_train.sort_values(by=\"client_id\", inplace=True)\ntarget_test.sort_values(by=\"client_id\", inplace=True)\ntarget_train = target_train[\"target\"]\ntarget_test = target_test[\"target\"]\ntarget_train.reset_index(drop=True, inplace=True)\ntarget_test.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:54:27.992334Z","iopub.execute_input":"2025-05-11T10:54:27.993004Z","iopub.status.idle":"2025-05-11T10:54:28.001834Z","shell.execute_reply.started":"2025-05-11T10:54:27.992967Z","shell.execute_reply":"2025-05-11T10:54:28.000935Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"data_train = data_train.to_dict(orient=\"records\")\ndata_test = data_test.to_dict(orient=\"records\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:54:30.377415Z","iopub.execute_input":"2025-05-11T10:54:30.377879Z","iopub.status.idle":"2025-05-11T10:54:30.485416Z","shell.execute_reply.started":"2025-05-11T10:54:30.377842Z","shell.execute_reply":"2025-05-11T10:54:30.484445Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**SWIN-RNN Seq Encoder:**","metadata":{}},{"cell_type":"code","source":"from ptls.nn.seq_encoder.rnn_encoder import RnnEncoder\nfrom ptls.nn.seq_encoder.containers import SeqEncoderContainer\n\n\nclass SWIN_RNN_SeqEncoder(SeqEncoderContainer):\n    \"\"\"SeqEncoderContainer with SWIN transformer backbone for features hierarchic fusion and RnnEncoder for feature aggregation.\n    \n    Parameters\n        trx_encoder:\n            TrxEncoder object\n        input_size:\n            input_size parameter for RnnEncoder\n            If None: input_size = trx_encoder.output_size\n            Set input_size explicitly or use None if your trx_encoder object has output_size attribute\n        is_reduce_sequence:\n            False - returns PaddedBatch with all transactions embeddings\n            True - returns one embedding for sequence based on CLS token\n        swin_depths: Numbers of blocks in stages (SWIN backbone).\n        swin_num_heads: Number of attention heads in W-MSA layers (SWIN backbone).\n        swin_start_window_size: Local window size of stage 1 (SWIN backbone).\n        swin_window_size_mult (int): the number by which the `window_size` is being multiplied when moving to another stage (SWIN backbone).\n        swin_drop: Dropout rate (SWIN backbone). Default: 0.0\n        swin_attn_drop: Attention dropout rate (SWIN backbone). Default: 0.0\n        swin_drop_path: Stochastic depth rate (SWIN backbone). Default: 0.0\n        swin_decoder: Flag that shows whether blocks in SWIN backbone are decoder-like. True => decoder-like; False => encoder-like. Default: False\n        swin_start_end_fusion: Flag that shows if the last and the first half-windows should merge (True) or not (False). Must be False for CPC and GPT.\n        **rnn_seq_encoder_params:\n            RnnEncoder params\n    \"\"\"\n    def __init__(self,\n                 trx_encoder=None,\n                 input_size=None,\n                 is_reduce_sequence=True,\n                 swin_depths=[],\n                 swin_num_heads=4,\n                 swin_start_window_size=4,\n                 swin_window_size_mult=1,\n                 swin_drop=0.,\n                 swin_attn_drop=0.,\n                 swin_drop_path=0.,\n                 swin_decoder=False,\n                 swin_start_end_fusion=True,\n                 **rnn_seq_encoder_params\n                 ):\n        super().__init__(\n            trx_encoder=trx_encoder,\n            seq_encoder_cls=RnnEncoder,\n            input_size=input_size,\n            seq_encoder_params=rnn_seq_encoder_params,\n            is_reduce_sequence=is_reduce_sequence,\n        )\n        self.swin_fusion = SwinTransformerBackbone(\n                               dim=trx_encoder.output_size,\n                               depths=swin_depths,\n                               num_heads=swin_num_heads,\n                               start_window_size=swin_start_window_size,\n                               window_size_mult=swin_window_size_mult,\n                               drop=swin_drop,\n                               attn_drop=swin_attn_drop,\n                               drop_path=swin_drop_path,\n                               decoder=swin_decoder,\n                               start_end_fusion=swin_start_end_fusion \n                              )\n\n    def forward(self, x, names=None, seq_len=None, h_0=None):\n        x = self.trx_encoder(x)\n        x = self.swin_fusion(x)\n        x = self.seq_encoder(x, h_0)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:54:32.712909Z","iopub.execute_input":"2025-05-11T10:54:32.713243Z","iopub.status.idle":"2025-05-11T10:54:32.721454Z","shell.execute_reply.started":"2025-05-11T10:54:32.713215Z","shell.execute_reply":"2025-05-11T10:54:32.720528Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"**Convolution Aggregator Class:**","metadata":{}},{"cell_type":"code","source":"from ptls.data_load.padded_batch import PaddedBatch\nimport torch.nn as nn\n\n\nclass ConvAggregator(TrxEncoder):\n    \"\"\"The NN layer, a combination of TrxEncoder and Conv Layer (a window of #`agg_samples` transactions) \n       (works like nn.Sequential([TrxEncoder, Conv Window Aggregation])).\n       \n       The types of the input and output are `PaddedBatch` of shapes (B, L, T) and (B, L', T) respectively, where \n       B means batch_size,\n       L/L' means the max length of a sequence of transactions in a batch (the length is the same as #trx)\n       T means the dimension of a single transaction.\n\n       Parameters\n        agg_samples (int):\n            The number of transactions in a sliding aggregation window (conv layer).\n\n        use_window_attention (bool):\n            If True, the attention layer will be applied to transactions in a sliding window before pooling.\n            \n        embeddings:\n            You can find info about this param in TrxEncoder desc.\n        \n        numeric_values:\n            You can find info about this param in TrxEncoder desc.\n\n        embeddings_noise:\n            You can find info about this param in TrxEncoder desc.\n            \n        emb_dropout:\n            You can find info about this param in TrxEncoder desc.\n            \n        spatial_dropout:\n            You can find info about this param in TrxEncoder desc.\n\n        use_batch_norm:\n            You can find info about this param in TrxEncoder desc.\n\n        orthogonal_init:\n            You can find info about this param in TrxEncoder desc.\n            \n        linear_projection_size:\n            You can find info about this param in TrxEncoder desc.\n\n        out_of_index:\n            You can find info about this param in TrxEncoder desc.\n\n        norm_embeddings:\n            Keep default value for this parameter\n        \n        clip_replace_value:\n            Not used. Keep default value for this parameter\n        \n        positions: \n            Not used. Keep default value for this parameter\n       \"\"\"\n\n    def __init__(self,\n                 agg_samples=3,\n                 use_window_attention=False,\n                 embeddings=None,\n                 numeric_values=None,\n                 custom_embeddings=None,\n                 time_values=None,\n                 embeddings_noise: float = 0,\n                 norm_embeddings=None,\n                 use_batch_norm=False,\n                 use_batch_norm_with_lens=False,\n                 clip_replace_value=None,\n                 positions=None,\n                 emb_dropout=0,\n                 spatial_dropout=False,\n                 orthogonal_init=False,\n                 linear_projection_size=0,\n                 out_of_index: str = 'clip',\n                ):\n        \n        super().__init__(\n            embeddings=embeddings,\n            numeric_values=numeric_values,\n            custom_embeddings=custom_embeddings,\n            embeddings_noise=embeddings_noise,\n            norm_embeddings=norm_embeddings,\n            use_batch_norm=use_batch_norm,\n            use_batch_norm_with_lens=use_batch_norm_with_lens,\n            clip_replace_value=clip_replace_value,\n            positions=positions,\n            emb_dropout=emb_dropout,\n            spatial_dropout=spatial_dropout,\n            orthogonal_init=orthogonal_init,\n            linear_projection_size=linear_projection_size,\n            out_of_index=out_of_index,\n        )\n\n        self.agg_samples = agg_samples\n\n        channels = super().output_size\n\n        self.conv = nn.Conv1d(in_channels=channels, out_channels=channels, kernel_size=self.agg_samples, padding=(self.agg_samples - 1), bias=False) # (B, T, L)\n\n        self.use_window_attention = use_window_attention\n        if self.use_window_attention:\n            pass # Not Implemented\n\n    def forward(self, pb: PaddedBatch):\n        embeds = super().forward(pb)\n\n        mask = torch.arange(embeds.payload.shape[1], device=embeds.device)[None, :] + torch.ones((embeds.seq_lens.shape[0], embeds.payload.shape[1]), device=embeds.device)\n        mask[mask > embeds.seq_lens[:, None]] = 0.\n        mask[mask > 0.] = 1.\n        mask = mask[:, :, None]\n    \n        masked_embeds = embeds.payload * mask\n    \n        if self.use_window_attention:\n            pass # Not Implemented\n    \n        agg_embeds = torch.transpose(self.conv(torch.transpose(masked_embeds, 1, 2)), 1, 2)\n\n        new_seq_lens = embeds.seq_lens + self.agg_samples - 1\n\n        return PaddedBatch(agg_embeds, new_seq_lens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:54:37.040779Z","iopub.execute_input":"2025-05-11T10:54:37.041159Z","iopub.status.idle":"2025-05-11T10:54:37.051520Z","shell.execute_reply.started":"2025-05-11T10:54:37.041128Z","shell.execute_reply":"2025-05-11T10:54:37.050658Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"seed_everything(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:35:17.488795Z","iopub.execute_input":"2025-05-09T19:35:17.489173Z","iopub.status.idle":"2025-05-09T19:35:17.495100Z","shell.execute_reply.started":"2025-05-09T19:35:17.489140Z","shell.execute_reply":"2025-05-09T19:35:17.494246Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"device = \"cuda:0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:35:19.045522Z","iopub.execute_input":"2025-05-09T19:35:19.046112Z","iopub.status.idle":"2025-05-09T19:35:19.049745Z","shell.execute_reply.started":"2025-05-09T19:35:19.046078Z","shell.execute_reply":"2025-05-09T19:35:19.048870Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"trx_encoder_params = dict(\n    embeddings_noise=0.003,\n    numeric_values={\"amount_rur\": \"log\"},\n    embeddings={\n        \"trans_date\": {\"in\": 800, \"out\": 16},\n        \"small_group\": {\"in\": 250, \"out\": 16},\n    },\n    linear_projection_size=64\n)\n\ntrx_encoder = TrxEncoder(**trx_encoder_params).to(device)\n\nseq_encoder = SWIN_RNN_SeqEncoder(\n    trx_encoder=trx_encoder,\n    swin_depths=[2, 2, 6, 2],\n    swin_num_heads=[2, 4, 8, 16],\n    swin_start_window_size=4,\n    swin_window_size_mult=2,\n    swin_drop=0.1,\n    swin_attn_drop=0.1,\n    swin_drop_path=0.1,\n    swin_decoder=True,\n    swin_start_end_fusion=False,\n    hidden_size=512,\n    type=\"gru\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:36:56.382035Z","iopub.execute_input":"2025-05-09T19:36:56.382779Z","iopub.status.idle":"2025-05-09T19:36:56.419957Z","shell.execute_reply.started":"2025-05-09T19:36:56.382747Z","shell.execute_reply":"2025-05-09T19:36:56.419310Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"trx_encoder.eval()\n\ntrain_loader = inference_data_loader(data_train, num_workers=0, batch_size=64)\n\nfor i, batch in tqdm(enumerate(train_loader)):\n    batch = batch.to(device)\n    embeds = seq_encoder(batch)\n\n    # if i == 0:\n    #     print(batch.payload)\n    #     print(batch.seq_lens)\n    #     print()\n    #     print(embeds)\n    #     print(embeds.shape)\n    #     print(embeds.seq_lens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:38:08.130938Z","iopub.execute_input":"2025-05-09T19:38:08.131744Z","iopub.status.idle":"2025-05-09T19:39:25.515570Z","shell.execute_reply.started":"2025-05-09T19:38:08.131712Z","shell.execute_reply":"2025-05-09T19:39:25.514652Z"}},"outputs":[{"name":"stderr","text":"422it [01:17,  5.45it/s]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Train sequences lengths check:**","metadata":{}},{"cell_type":"code","source":"trx_encoder_params = dict(\n    embeddings_noise=0.003,\n    numeric_values={\"amount_rur\": \"log\"},\n    embeddings={\n        \"trans_date\": {\"in\": 800, \"out\": 16},\n        \"small_group\": {\"in\": 250, \"out\": 16},\n    },\n    linear_projection_size=64\n)\n\ntrx_encoder = TrxEncoder(**trx_encoder_params)\ntrx_encoder.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:40:56.954496Z","iopub.execute_input":"2025-05-09T19:40:56.954832Z","iopub.status.idle":"2025-05-09T19:40:56.964655Z","shell.execute_reply.started":"2025-05-09T19:40:56.954803Z","shell.execute_reply":"2025-05-09T19:40:56.963682Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrxEncoder(\n  (embeddings): ModuleDict(\n    (trans_date): NoisyEmbedding(\n      800, 16, padding_idx=0\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (small_group): NoisyEmbedding(\n      250, 16, padding_idx=0\n      (dropout): Dropout(p=0, inplace=False)\n    )\n  )\n  (custom_embeddings): ModuleDict(\n    (amount_rur): LogScaler()\n  )\n  (custom_embedding_batch_norm): RBatchNorm(\n    (bn): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (linear_projection_head): Linear(in_features=33, out_features=64, bias=True)\n)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"train_loader = inference_data_loader(data_train, num_workers=0, batch_size=64)\n\ntrx_encoder.eval()\n\nseq_lens = []\n\nfor batch in tqdm(train_loader):\n    embeds_batch = trx_encoder(batch.to(\"cuda\"))\n    seq_lens += [embeds_batch.seq_lens.detach().cpu().numpy()]\n\nseq_lens = np.concatenate(seq_lens)\n\nthreshold = int(np.quantile(seq_lens, 0.75) * 0.7)\n\nprint(\"Max Length:\", threshold)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T19:41:04.643112Z","iopub.execute_input":"2025-05-09T19:41:04.643779Z","iopub.status.idle":"2025-05-09T19:41:06.665992Z","shell.execute_reply.started":"2025-05-09T19:41:04.643747Z","shell.execute_reply":"2025-05-09T19:41:06.665100Z"}},"outputs":[{"name":"stderr","text":"422it [00:02, 209.81it/s]","output_type":"stream"},{"name":"stdout","text":"Max Length: 683\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# SWIN Aggregation","metadata":{}},{"cell_type":"markdown","source":"- **COLES:**","metadata":{}},{"cell_type":"code","source":"# import gc\n\n# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:37:25.676152Z","iopub.execute_input":"2025-05-03T13:37:25.676467Z","iopub.status.idle":"2025-05-03T13:37:26.235117Z","shell.execute_reply.started":"2025-05-03T13:37:25.676444Z","shell.execute_reply":"2025-05-03T13:37:26.234394Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"seed_everything(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:14:31.786560Z","iopub.execute_input":"2025-05-10T11:14:31.787386Z","iopub.status.idle":"2025-05-10T11:14:31.797175Z","shell.execute_reply.started":"2025-05-10T11:14:31.787351Z","shell.execute_reply":"2025-05-10T11:14:31.796310Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"**DataLoaders:**","metadata":{}},{"cell_type":"code","source":"data = PtlsDataModule(\n    train_data=ColesDataset(\n        MemoryMapDataset(\n            data=data_train,\n            i_filters=[SeqLenFilter(min_seq_len=30)],\n        ),\n        splitter=SampleSlices(\n            split_count=5,\n            cnt_min=30,\n            cnt_max=683,\n        ),\n    ),\n    train_num_workers=4,\n    train_batch_size=32,\n    valid_data=ColesDataset(\n        MemoryMapDataset(\n            data=data_test,\n            i_filters=[SeqLenFilter(min_seq_len=30)],\n        ),\n        splitter=SampleSlices(\n            split_count=5,\n            cnt_min=30,\n            cnt_max=683,\n        ),\n    ),\n    valid_num_workers=4,\n    valid_batch_size=32\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:14:48.421067Z","iopub.execute_input":"2025-05-10T11:14:48.421796Z","iopub.status.idle":"2025-05-10T11:14:48.473675Z","shell.execute_reply.started":"2025-05-10T11:14:48.421760Z","shell.execute_reply":"2025-05-10T11:14:48.473058Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"**Модель:**","metadata":{}},{"cell_type":"code","source":"N_EPOCHS = 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:14:53.189228Z","iopub.execute_input":"2025-05-10T11:14:53.190018Z","iopub.status.idle":"2025-05-10T11:14:53.193592Z","shell.execute_reply.started":"2025-05-10T11:14:53.189987Z","shell.execute_reply":"2025-05-10T11:14:53.192646Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"trx_encoder_params = dict(\n    embeddings_noise=0.003,\n    numeric_values={\"amount_rur\": \"log\"},\n    embeddings={\n        \"trans_date\": {\"in\": 800, \"out\": 16},\n        \"small_group\": {\"in\": 250, \"out\": 16},\n    },\n    linear_projection_size=64,\n    agg_samples=3, # 3, 5, 7, 9\n    use_window_attention=False\n)\n\n#trx_encoder = TrxEncoder(**trx_encoder_params)\ntrx_encoder = ConvAggregator(**trx_encoder_params)\n\nseq_encoder = SWIN_RNN_SeqEncoder(\n    trx_encoder=trx_encoder,\n    swin_depths=[2, 2, 6, 2],\n    swin_num_heads=[2, 4, 8, 16],\n    swin_start_window_size=4,\n    swin_window_size_mult=2,\n    swin_drop=0.1,\n    swin_attn_drop=0.1,\n    swin_drop_path=0.1,\n    swin_decoder=False,\n    swin_start_end_fusion=False,\n    hidden_size=512,\n    type=\"gru\"\n)\n\ncoles = CoLESModule(\n    seq_encoder=seq_encoder,\n    #loss=SoftmaxLoss(),\n    optimizer_partial=partial(torch.optim.Adam, lr=3e-3, weight_decay=5e-4),\n    lr_scheduler_partial=partial(torch.optim.lr_scheduler.CosineAnnealingLR, T_max=N_EPOCHS, eta_min=1e-6)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:17:33.826283Z","iopub.execute_input":"2025-05-10T11:17:33.826615Z","iopub.status.idle":"2025-05-10T11:17:33.959042Z","shell.execute_reply.started":"2025-05-10T11:17:33.826589Z","shell.execute_reply":"2025-05-10T11:17:33.958146Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"**Обучение:**","metadata":{}},{"cell_type":"code","source":"logger = CometLogger(project_name=\"EvS_SSL\", experiment_name=\"CoLES_SWIN_agg (w/ conv_agg, 3trx)\")\n\ntrainer = pl.Trainer(\n    logger=logger,\n    max_epochs=N_EPOCHS,\n    accelerator=\"gpu\",\n    devices=1,\n    enable_progress_bar=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:17:53.816592Z","iopub.execute_input":"2025-05-10T11:17:53.817452Z","iopub.status.idle":"2025-05-10T11:17:53.889819Z","shell.execute_reply.started":"2025-05-10T11:17:53.817417Z","shell.execute_reply":"2025-05-10T11:17:53.889185Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"trainer.fit(coles, data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:18:00.222608Z","iopub.execute_input":"2025-05-10T11:18:00.223410Z","iopub.status.idle":"2025-05-10T14:34:45.081051Z","shell.execute_reply.started":"2025-05-10T11:18:00.223377Z","shell.execute_reply":"2025-05-10T14:34:45.080339Z"}},"outputs":[{"name":"stderr","text":"\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/askoro/evs-ssl/691f007f6de14fd7811a52366541fb43\n\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/kaggle/working' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65bcb0b3c6d44694aacc7cea6008d39a"}},"metadata":{}},{"name":"stderr","text":"\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Error retrieving Conda package as an explicit file\n\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Command '['conda', 'list', '--explicit', '--md5']' returned non-zero exit status 109.\nThe following arguments were not expected: --md5 --explicit\nRun with --help for more information.\n\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : CoLES_SWIN_agg (w/ conv_agg, 3trx)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/askoro/evs-ssl/691f007f6de14fd7811a52366541fb43\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [2025]             : (12.434327125549316, 256.6272277832031)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seq_len [337]           : (321.9937438964844, 396.76251220703125)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid/recall_top_k [20] : (0.7303524613380432, 0.9741855263710022)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : CoLES_SWIN_agg (w/ conv_agg, 3trx)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_batch_size   : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_drop_last    : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_num_workers  : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_batch_size  : 32\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_drop_last   : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_num_workers : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid_batch_size  : 32\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid_drop_last   : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid_num_workers : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"trainer.logged_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:38:40.042092Z","iopub.execute_input":"2025-05-10T14:38:40.043014Z","iopub.status.idle":"2025-05-10T14:38:40.057550Z","shell.execute_reply.started":"2025-05-10T14:38:40.042976Z","shell.execute_reply":"2025-05-10T14:38:40.056678Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'loss': tensor(14.5103),\n 'seq_len': tensor(344.6250),\n 'valid/recall_top_k': tensor(0.9742)}"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"torch.save(seq_encoder.state_dict(), \"coles_enc_win_agg.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T01:14:58.099065Z","iopub.execute_input":"2025-01-24T01:14:58.099414Z","iopub.status.idle":"2025-01-24T01:14:58.111115Z","shell.execute_reply.started":"2025-01-24T01:14:58.099387Z","shell.execute_reply":"2025-01-24T01:14:58.110156Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"**Измерим качество на тесте (catboost поверх эмбеддингов):**","metadata":{}},{"cell_type":"code","source":"encoder = coles.seq_encoder\n\ndevice = \"cuda:0\"\n\nencoder.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:38:42.733031Z","iopub.execute_input":"2025-05-10T14:38:42.733692Z","iopub.status.idle":"2025-05-10T14:38:42.750931Z","shell.execute_reply.started":"2025-05-10T14:38:42.733660Z","shell.execute_reply":"2025-05-10T14:38:42.750145Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"SWIN_RNN_SeqEncoder(\n  (trx_encoder): ConvAggregator(\n    (embeddings): ModuleDict(\n      (trans_date): NoisyEmbedding(\n        800, 16, padding_idx=0\n        (dropout): Dropout(p=0, inplace=False)\n      )\n      (small_group): NoisyEmbedding(\n        250, 16, padding_idx=0\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (custom_embeddings): ModuleDict(\n      (amount_rur): LogScaler()\n    )\n    (linear_projection_head): Linear(in_features=33, out_features=64, bias=True)\n    (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), bias=False)\n  )\n  (seq_encoder): RnnEncoder(\n    (rnn): GRU(64, 512, batch_first=True)\n    (reducer): LastStepEncoder()\n  )\n  (swin_fusion): SwinTransformerBackbone(\n    (backbone): ModuleList(\n      (0): SwinTransformerLayer(\n        dim=64, depth=2, num_heads=2, window_size=4\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=64, num_heads=2, window_size=4, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=4, num_heads=2\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=64, num_heads=2, window_size=4, shift_size=2, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=4, num_heads=2\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (1): SwinTransformerLayer(\n        dim=64, depth=2, num_heads=4, window_size=8\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=64, num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=8, num_heads=4\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=64, num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=8, num_heads=4\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (2): SwinTransformerLayer(\n        dim=64, depth=6, num_heads=8, window_size=16\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=64, num_heads=8, window_size=16, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=16, num_heads=8\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=64, num_heads=8, window_size=16, shift_size=8, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=16, num_heads=8\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): SwinTransformerBlock(\n            dim=64, num_heads=8, window_size=16, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=16, num_heads=8\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): SwinTransformerBlock(\n            dim=64, num_heads=8, window_size=16, shift_size=8, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=16, num_heads=8\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): SwinTransformerBlock(\n            dim=64, num_heads=8, window_size=16, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=16, num_heads=8\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): SwinTransformerBlock(\n            dim=64, num_heads=8, window_size=16, shift_size=8, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=16, num_heads=8\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (3): SwinTransformerLayer(\n        dim=64, depth=2, num_heads=16, window_size=32\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=64, num_heads=16, window_size=32, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=32, num_heads=16\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=64, num_heads=16, window_size=32, shift_size=16, mlp_ratio=4.0\n            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=64, window_size=32, num_heads=16\n              (qkv): Linear(in_features=64, out_features=192, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=64, out_features=64, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=64, out_features=256, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=256, out_features=64, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"from tqdm import tqdm\n\nseed_everything(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:39:02.365034Z","iopub.execute_input":"2025-05-10T14:39:02.365703Z","iopub.status.idle":"2025-05-10T14:39:02.370927Z","shell.execute_reply.started":"2025-05-10T14:39:02.365672Z","shell.execute_reply":"2025-05-10T14:39:02.370103Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"train_loader = inference_data_loader(data_train, num_workers=0, batch_size=32)\nencoder.eval()\ntrain_embeds = None\n\nfor i, batch in tqdm(enumerate(train_loader)):\n    train_embeds_batch = encoder(batch.to(device))\n    if i == 0:\n        train_embeds = train_embeds_batch.detach().cpu().numpy()\n    else:\n        train_embeds = np.concatenate([train_embeds, train_embeds_batch.detach().cpu().numpy()], axis=0)\n    \ntrain_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:39:12.074812Z","iopub.execute_input":"2025-05-10T14:39:12.075683Z","iopub.status.idle":"2025-05-10T14:40:46.505703Z","shell.execute_reply.started":"2025-05-10T14:39:12.075649Z","shell.execute_reply":"2025-05-10T14:40:46.504796Z"}},"outputs":[{"name":"stderr","text":"844it [01:34,  8.94it/s]\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"array([[ 0.99999994, -0.9999983 ,  0.9999999 , ...,  0.07011775,\n        -0.99997383,  0.9089313 ],\n       [ 0.99999994, -0.99999917,  0.99999994, ...,  0.6910359 ,\n        -0.999971  ,  0.27540633],\n       [ 1.        , -0.9999976 ,  0.99999994, ...,  0.6189189 ,\n        -0.9999723 , -0.13676685],\n       ...,\n       [ 1.        , -0.99999875,  1.        , ..., -0.8627192 ,\n        -0.9999666 , -0.50936544],\n       [ 1.        , -0.99999726,  0.99999994, ..., -0.00435414,\n        -0.99995846,  0.28849235],\n       [ 1.        , -0.9999988 ,  1.        , ...,  0.262862  ,\n        -0.99998564, -0.26877946]], dtype=float32)"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"test_loader = inference_data_loader(data_test, num_workers=0, batch_size=32)\nencoder.eval()\ntest_embeds = None\n\nfor i, batch in tqdm(enumerate(test_loader)):\n    test_embeds_batch = encoder(batch.to(device))\n    if i == 0:\n        test_embeds = test_embeds_batch.detach().cpu().numpy()\n    else:\n        test_embeds = np.concatenate([test_embeds, test_embeds_batch.detach().cpu().numpy()], axis=0)\n    \ntest_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:41:01.060550Z","iopub.execute_input":"2025-05-10T14:41:01.061368Z","iopub.status.idle":"2025-05-10T14:41:10.761634Z","shell.execute_reply.started":"2025-05-10T14:41:01.061321Z","shell.execute_reply":"2025-05-10T14:41:10.760748Z"}},"outputs":[{"name":"stderr","text":"94it [00:09,  9.70it/s]\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"array([[ 0.99999994, -0.999999  ,  1.        , ...,  0.548387  ,\n        -0.99997306,  0.94794637],\n       [ 0.99999994, -0.99999917,  0.9999998 , ...,  0.40317556,\n        -0.9999732 , -0.59722704],\n       [ 1.        , -0.9999987 ,  1.        , ...,  0.7880338 ,\n        -0.9999577 , -0.18983312],\n       ...,\n       [ 0.99999976, -0.9999977 ,  1.        , ...,  0.704449  ,\n        -0.9999575 , -0.76203877],\n       [ 0.99999994, -0.9999965 ,  1.        , ...,  0.7353311 ,\n        -0.99998206, -0.19870564],\n       [ 1.        , -0.99999946,  0.99999994, ...,  0.7427735 ,\n        -0.999995  , -0.76541483]], dtype=float32)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"clf = CatBoostClassifier(loss_function='MultiClass', task_type=\"GPU\", devices='0', random_state=0)\n\nclf.fit(train_embeds, target_train, plot_file=\"catboost_log.html\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:41:18.038505Z","iopub.execute_input":"2025-05-10T14:41:18.039347Z","iopub.status.idle":"2025-05-10T14:41:46.538956Z","shell.execute_reply.started":"2025-05-10T14:41:18.039316Z","shell.execute_reply":"2025-05-10T14:41:46.538064Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 8197.125 Total: 16269.25\n","output_type":"stream"},{"name":"stdout","text":"Learning rate set to 0.12714\n0:\tlearn: 1.2954761\ttotal: 16s\tremaining: 4h 26m 50s\n1:\tlearn: 1.2301989\ttotal: 16s\tremaining: 2h 13m 22s\n2:\tlearn: 1.1790849\ttotal: 16s\tremaining: 1h 28m 53s\n3:\tlearn: 1.1373166\ttotal: 16.1s\tremaining: 1h 6m 38s\n4:\tlearn: 1.1046293\ttotal: 16.1s\tremaining: 53m 17s\n5:\tlearn: 1.0768421\ttotal: 16.1s\tremaining: 44m 24s\n6:\tlearn: 1.0533809\ttotal: 16.1s\tremaining: 38m 2s\n7:\tlearn: 1.0330715\ttotal: 16.1s\tremaining: 33m 16s\n8:\tlearn: 1.0160692\ttotal: 16.1s\tremaining: 29m 34s\n9:\tlearn: 1.0013699\ttotal: 16.1s\tremaining: 26m 36s\n10:\tlearn: 0.9889747\ttotal: 16.1s\tremaining: 24m 10s\n11:\tlearn: 0.9783094\ttotal: 16.1s\tremaining: 22m 9s\n12:\tlearn: 0.9684621\ttotal: 16.2s\tremaining: 20m 26s\n13:\tlearn: 0.9594464\ttotal: 16.2s\tremaining: 18m 58s\n14:\tlearn: 0.9520535\ttotal: 16.2s\tremaining: 17m 42s\n15:\tlearn: 0.9449457\ttotal: 16.2s\tremaining: 16m 35s\n16:\tlearn: 0.9390123\ttotal: 16.2s\tremaining: 15m 36s\n17:\tlearn: 0.9335706\ttotal: 16.2s\tremaining: 14m 44s\n18:\tlearn: 0.9289168\ttotal: 16.2s\tremaining: 13m 57s\n19:\tlearn: 0.9241490\ttotal: 16.2s\tremaining: 13m 15s\n20:\tlearn: 0.9204653\ttotal: 16.2s\tremaining: 12m 36s\n21:\tlearn: 0.9168026\ttotal: 16.2s\tremaining: 12m 2s\n22:\tlearn: 0.9137465\ttotal: 16.3s\tremaining: 11m 30s\n23:\tlearn: 0.9105825\ttotal: 16.3s\tremaining: 11m 1s\n24:\tlearn: 0.9075987\ttotal: 16.3s\tremaining: 10m 34s\n25:\tlearn: 0.9045540\ttotal: 16.3s\tremaining: 10m 10s\n26:\tlearn: 0.9021469\ttotal: 16.3s\tremaining: 9m 47s\n27:\tlearn: 0.8998732\ttotal: 16.3s\tremaining: 9m 25s\n28:\tlearn: 0.8975146\ttotal: 16.3s\tremaining: 9m 6s\n29:\tlearn: 0.8951777\ttotal: 16.3s\tremaining: 8m 47s\n30:\tlearn: 0.8931652\ttotal: 16.3s\tremaining: 8m 30s\n31:\tlearn: 0.8910284\ttotal: 16.3s\tremaining: 8m 14s\n32:\tlearn: 0.8892669\ttotal: 16.4s\tremaining: 7m 59s\n33:\tlearn: 0.8872250\ttotal: 16.4s\tremaining: 7m 44s\n34:\tlearn: 0.8854533\ttotal: 16.4s\tremaining: 7m 31s\n35:\tlearn: 0.8835586\ttotal: 16.4s\tremaining: 7m 18s\n36:\tlearn: 0.8818011\ttotal: 16.4s\tremaining: 7m 6s\n37:\tlearn: 0.8803037\ttotal: 16.4s\tremaining: 6m 55s\n38:\tlearn: 0.8788936\ttotal: 16.4s\tremaining: 6m 44s\n39:\tlearn: 0.8774279\ttotal: 16.4s\tremaining: 6m 34s\n40:\tlearn: 0.8760160\ttotal: 16.4s\tremaining: 6m 24s\n41:\tlearn: 0.8743328\ttotal: 16.4s\tremaining: 6m 14s\n42:\tlearn: 0.8728938\ttotal: 16.4s\tremaining: 6m 6s\n43:\tlearn: 0.8716803\ttotal: 16.5s\tremaining: 5m 57s\n44:\tlearn: 0.8704179\ttotal: 16.5s\tremaining: 5m 49s\n45:\tlearn: 0.8690391\ttotal: 16.5s\tremaining: 5m 41s\n46:\tlearn: 0.8680265\ttotal: 16.5s\tremaining: 5m 34s\n47:\tlearn: 0.8664661\ttotal: 16.5s\tremaining: 5m 27s\n48:\tlearn: 0.8653099\ttotal: 16.5s\tremaining: 5m 20s\n49:\tlearn: 0.8639384\ttotal: 16.5s\tremaining: 5m 13s\n50:\tlearn: 0.8626354\ttotal: 16.5s\tremaining: 5m 7s\n51:\tlearn: 0.8613864\ttotal: 16.5s\tremaining: 5m 1s\n52:\tlearn: 0.8601093\ttotal: 16.5s\tremaining: 4m 55s\n53:\tlearn: 0.8591438\ttotal: 16.6s\tremaining: 4m 50s\n54:\tlearn: 0.8581670\ttotal: 16.6s\tremaining: 4m 44s\n55:\tlearn: 0.8571223\ttotal: 16.6s\tremaining: 4m 39s\n56:\tlearn: 0.8561905\ttotal: 16.6s\tremaining: 4m 34s\n57:\tlearn: 0.8555020\ttotal: 16.6s\tremaining: 4m 29s\n58:\tlearn: 0.8546983\ttotal: 16.6s\tremaining: 4m 24s\n59:\tlearn: 0.8535881\ttotal: 16.6s\tremaining: 4m 20s\n60:\tlearn: 0.8527088\ttotal: 16.6s\tremaining: 4m 15s\n61:\tlearn: 0.8519793\ttotal: 16.6s\tremaining: 4m 11s\n62:\tlearn: 0.8509277\ttotal: 16.6s\tremaining: 4m 7s\n63:\tlearn: 0.8500318\ttotal: 16.6s\tremaining: 4m 3s\n64:\tlearn: 0.8489748\ttotal: 16.7s\tremaining: 3m 59s\n65:\tlearn: 0.8480686\ttotal: 16.7s\tremaining: 3m 55s\n66:\tlearn: 0.8473961\ttotal: 16.7s\tremaining: 3m 52s\n67:\tlearn: 0.8463615\ttotal: 16.7s\tremaining: 3m 48s\n68:\tlearn: 0.8456581\ttotal: 16.7s\tremaining: 3m 45s\n69:\tlearn: 0.8448573\ttotal: 16.7s\tremaining: 3m 41s\n70:\tlearn: 0.8442497\ttotal: 16.7s\tremaining: 3m 38s\n71:\tlearn: 0.8435200\ttotal: 16.7s\tremaining: 3m 35s\n72:\tlearn: 0.8423924\ttotal: 16.7s\tremaining: 3m 32s\n73:\tlearn: 0.8415732\ttotal: 16.7s\tremaining: 3m 29s\n74:\tlearn: 0.8404219\ttotal: 16.8s\tremaining: 3m 26s\n75:\tlearn: 0.8397114\ttotal: 16.8s\tremaining: 3m 23s\n76:\tlearn: 0.8390674\ttotal: 16.8s\tremaining: 3m 21s\n77:\tlearn: 0.8383406\ttotal: 16.8s\tremaining: 3m 18s\n78:\tlearn: 0.8368962\ttotal: 16.8s\tremaining: 3m 15s\n79:\tlearn: 0.8361897\ttotal: 16.8s\tremaining: 3m 13s\n80:\tlearn: 0.8354321\ttotal: 16.8s\tremaining: 3m 10s\n81:\tlearn: 0.8346953\ttotal: 16.8s\tremaining: 3m 8s\n82:\tlearn: 0.8337983\ttotal: 16.8s\tremaining: 3m 5s\n83:\tlearn: 0.8330019\ttotal: 16.8s\tremaining: 3m 3s\n84:\tlearn: 0.8320357\ttotal: 16.8s\tremaining: 3m 1s\n85:\tlearn: 0.8313158\ttotal: 16.9s\tremaining: 2m 59s\n86:\tlearn: 0.8305350\ttotal: 16.9s\tremaining: 2m 56s\n87:\tlearn: 0.8297736\ttotal: 16.9s\tremaining: 2m 54s\n88:\tlearn: 0.8292178\ttotal: 16.9s\tremaining: 2m 52s\n89:\tlearn: 0.8284223\ttotal: 16.9s\tremaining: 2m 50s\n90:\tlearn: 0.8279013\ttotal: 16.9s\tremaining: 2m 48s\n91:\tlearn: 0.8273474\ttotal: 16.9s\tremaining: 2m 46s\n92:\tlearn: 0.8268543\ttotal: 16.9s\tremaining: 2m 44s\n93:\tlearn: 0.8260689\ttotal: 16.9s\tremaining: 2m 43s\n94:\tlearn: 0.8255036\ttotal: 16.9s\tremaining: 2m 41s\n95:\tlearn: 0.8249177\ttotal: 16.9s\tremaining: 2m 39s\n96:\tlearn: 0.8240173\ttotal: 17s\tremaining: 2m 37s\n97:\tlearn: 0.8234168\ttotal: 17s\tremaining: 2m 36s\n98:\tlearn: 0.8226866\ttotal: 17s\tremaining: 2m 34s\n99:\tlearn: 0.8223050\ttotal: 17s\tremaining: 2m 32s\n100:\tlearn: 0.8213444\ttotal: 17s\tremaining: 2m 31s\n101:\tlearn: 0.8205835\ttotal: 17s\tremaining: 2m 29s\n102:\tlearn: 0.8200788\ttotal: 17s\tremaining: 2m 28s\n103:\tlearn: 0.8193715\ttotal: 17s\tremaining: 2m 26s\n104:\tlearn: 0.8189069\ttotal: 17s\tremaining: 2m 25s\n105:\tlearn: 0.8183185\ttotal: 17s\tremaining: 2m 23s\n106:\tlearn: 0.8174264\ttotal: 17s\tremaining: 2m 22s\n107:\tlearn: 0.8166415\ttotal: 17.1s\tremaining: 2m 20s\n108:\tlearn: 0.8159380\ttotal: 17.1s\tremaining: 2m 19s\n109:\tlearn: 0.8151359\ttotal: 17.1s\tremaining: 2m 18s\n110:\tlearn: 0.8144003\ttotal: 17.1s\tremaining: 2m 16s\n111:\tlearn: 0.8138060\ttotal: 17.1s\tremaining: 2m 15s\n112:\tlearn: 0.8133069\ttotal: 17.1s\tremaining: 2m 14s\n113:\tlearn: 0.8125658\ttotal: 17.1s\tremaining: 2m 13s\n114:\tlearn: 0.8116443\ttotal: 17.1s\tremaining: 2m 11s\n115:\tlearn: 0.8110722\ttotal: 17.1s\tremaining: 2m 10s\n116:\tlearn: 0.8102161\ttotal: 17.1s\tremaining: 2m 9s\n117:\tlearn: 0.8095344\ttotal: 17.1s\tremaining: 2m 8s\n118:\tlearn: 0.8090681\ttotal: 17.2s\tremaining: 2m 7s\n119:\tlearn: 0.8085000\ttotal: 17.2s\tremaining: 2m 5s\n120:\tlearn: 0.8078397\ttotal: 17.2s\tremaining: 2m 4s\n121:\tlearn: 0.8073283\ttotal: 17.2s\tremaining: 2m 3s\n122:\tlearn: 0.8066213\ttotal: 17.2s\tremaining: 2m 2s\n123:\tlearn: 0.8060085\ttotal: 17.2s\tremaining: 2m 1s\n124:\tlearn: 0.8054096\ttotal: 17.2s\tremaining: 2m\n125:\tlearn: 0.8046008\ttotal: 17.2s\tremaining: 1m 59s\n126:\tlearn: 0.8038994\ttotal: 17.2s\tremaining: 1m 58s\n127:\tlearn: 0.8034303\ttotal: 17.2s\tremaining: 1m 57s\n128:\tlearn: 0.8030131\ttotal: 17.3s\tremaining: 1m 56s\n129:\tlearn: 0.8025993\ttotal: 17.3s\tremaining: 1m 55s\n130:\tlearn: 0.8019949\ttotal: 17.3s\tremaining: 1m 54s\n131:\tlearn: 0.8013168\ttotal: 17.3s\tremaining: 1m 53s\n132:\tlearn: 0.8006893\ttotal: 17.3s\tremaining: 1m 52s\n133:\tlearn: 0.7999993\ttotal: 17.3s\tremaining: 1m 51s\n134:\tlearn: 0.7990845\ttotal: 17.3s\tremaining: 1m 50s\n135:\tlearn: 0.7982112\ttotal: 17.3s\tremaining: 1m 50s\n136:\tlearn: 0.7975864\ttotal: 17.3s\tremaining: 1m 49s\n137:\tlearn: 0.7972464\ttotal: 17.3s\tremaining: 1m 48s\n138:\tlearn: 0.7964869\ttotal: 17.3s\tremaining: 1m 47s\n139:\tlearn: 0.7958140\ttotal: 17.4s\tremaining: 1m 46s\n140:\tlearn: 0.7951546\ttotal: 17.4s\tremaining: 1m 45s\n141:\tlearn: 0.7947312\ttotal: 17.4s\tremaining: 1m 44s\n142:\tlearn: 0.7939352\ttotal: 17.4s\tremaining: 1m 44s\n143:\tlearn: 0.7932656\ttotal: 17.4s\tremaining: 1m 43s\n144:\tlearn: 0.7927817\ttotal: 17.4s\tremaining: 1m 42s\n145:\tlearn: 0.7920475\ttotal: 17.4s\tremaining: 1m 41s\n146:\tlearn: 0.7914528\ttotal: 17.4s\tremaining: 1m 41s\n147:\tlearn: 0.7909984\ttotal: 17.4s\tremaining: 1m 40s\n148:\tlearn: 0.7904094\ttotal: 17.4s\tremaining: 1m 39s\n149:\tlearn: 0.7900236\ttotal: 17.4s\tremaining: 1m 38s\n150:\tlearn: 0.7896387\ttotal: 17.5s\tremaining: 1m 38s\n151:\tlearn: 0.7890469\ttotal: 17.5s\tremaining: 1m 37s\n152:\tlearn: 0.7887138\ttotal: 17.5s\tremaining: 1m 36s\n153:\tlearn: 0.7880756\ttotal: 17.5s\tremaining: 1m 36s\n154:\tlearn: 0.7874750\ttotal: 17.5s\tremaining: 1m 35s\n155:\tlearn: 0.7865718\ttotal: 17.5s\tremaining: 1m 34s\n156:\tlearn: 0.7860499\ttotal: 17.5s\tremaining: 1m 34s\n157:\tlearn: 0.7857512\ttotal: 17.5s\tremaining: 1m 33s\n158:\tlearn: 0.7854204\ttotal: 17.5s\tremaining: 1m 32s\n159:\tlearn: 0.7849388\ttotal: 17.5s\tremaining: 1m 32s\n160:\tlearn: 0.7843822\ttotal: 17.5s\tremaining: 1m 31s\n161:\tlearn: 0.7839137\ttotal: 17.6s\tremaining: 1m 30s\n162:\tlearn: 0.7831257\ttotal: 17.6s\tremaining: 1m 30s\n163:\tlearn: 0.7825978\ttotal: 17.6s\tremaining: 1m 29s\n164:\tlearn: 0.7820590\ttotal: 17.6s\tremaining: 1m 28s\n165:\tlearn: 0.7815270\ttotal: 17.6s\tremaining: 1m 28s\n166:\tlearn: 0.7808706\ttotal: 17.6s\tremaining: 1m 27s\n167:\tlearn: 0.7807138\ttotal: 17.6s\tremaining: 1m 27s\n168:\tlearn: 0.7803208\ttotal: 17.6s\tremaining: 1m 26s\n169:\tlearn: 0.7797781\ttotal: 17.6s\tremaining: 1m 26s\n170:\tlearn: 0.7793406\ttotal: 17.6s\tremaining: 1m 25s\n171:\tlearn: 0.7785906\ttotal: 17.6s\tremaining: 1m 24s\n172:\tlearn: 0.7782671\ttotal: 17.7s\tremaining: 1m 24s\n173:\tlearn: 0.7778760\ttotal: 17.7s\tremaining: 1m 23s\n174:\tlearn: 0.7771839\ttotal: 17.7s\tremaining: 1m 23s\n175:\tlearn: 0.7766729\ttotal: 17.7s\tremaining: 1m 22s\n176:\tlearn: 0.7759815\ttotal: 17.7s\tremaining: 1m 22s\n177:\tlearn: 0.7752982\ttotal: 17.7s\tremaining: 1m 21s\n178:\tlearn: 0.7749533\ttotal: 17.7s\tremaining: 1m 21s\n179:\tlearn: 0.7744554\ttotal: 17.7s\tremaining: 1m 20s\n180:\tlearn: 0.7736555\ttotal: 17.7s\tremaining: 1m 20s\n181:\tlearn: 0.7731950\ttotal: 17.7s\tremaining: 1m 19s\n182:\tlearn: 0.7727821\ttotal: 17.7s\tremaining: 1m 19s\n183:\tlearn: 0.7719576\ttotal: 17.8s\tremaining: 1m 18s\n184:\tlearn: 0.7715990\ttotal: 17.8s\tremaining: 1m 18s\n185:\tlearn: 0.7710898\ttotal: 17.8s\tremaining: 1m 17s\n186:\tlearn: 0.7703916\ttotal: 17.8s\tremaining: 1m 17s\n187:\tlearn: 0.7700441\ttotal: 17.8s\tremaining: 1m 16s\n188:\tlearn: 0.7694405\ttotal: 17.8s\tremaining: 1m 16s\n189:\tlearn: 0.7688480\ttotal: 17.8s\tremaining: 1m 15s\n190:\tlearn: 0.7682185\ttotal: 17.8s\tremaining: 1m 15s\n191:\tlearn: 0.7679692\ttotal: 17.8s\tremaining: 1m 15s\n192:\tlearn: 0.7675799\ttotal: 17.8s\tremaining: 1m 14s\n193:\tlearn: 0.7669917\ttotal: 17.9s\tremaining: 1m 14s\n194:\tlearn: 0.7663422\ttotal: 17.9s\tremaining: 1m 13s\n195:\tlearn: 0.7655172\ttotal: 17.9s\tremaining: 1m 13s\n196:\tlearn: 0.7649157\ttotal: 17.9s\tremaining: 1m 12s\n197:\tlearn: 0.7643828\ttotal: 17.9s\tremaining: 1m 12s\n198:\tlearn: 0.7639204\ttotal: 17.9s\tremaining: 1m 12s\n199:\tlearn: 0.7632661\ttotal: 17.9s\tremaining: 1m 11s\n200:\tlearn: 0.7627445\ttotal: 17.9s\tremaining: 1m 11s\n201:\tlearn: 0.7620323\ttotal: 17.9s\tremaining: 1m 10s\n202:\tlearn: 0.7617449\ttotal: 17.9s\tremaining: 1m 10s\n203:\tlearn: 0.7614780\ttotal: 17.9s\tremaining: 1m 10s\n204:\tlearn: 0.7608733\ttotal: 18s\tremaining: 1m 9s\n205:\tlearn: 0.7602266\ttotal: 18s\tremaining: 1m 9s\n206:\tlearn: 0.7596563\ttotal: 18s\tremaining: 1m 8s\n207:\tlearn: 0.7592585\ttotal: 18s\tremaining: 1m 8s\n208:\tlearn: 0.7588331\ttotal: 18s\tremaining: 1m 8s\n209:\tlearn: 0.7585336\ttotal: 18s\tremaining: 1m 7s\n210:\tlearn: 0.7581620\ttotal: 18s\tremaining: 1m 7s\n211:\tlearn: 0.7578107\ttotal: 18s\tremaining: 1m 6s\n212:\tlearn: 0.7573434\ttotal: 18s\tremaining: 1m 6s\n213:\tlearn: 0.7569656\ttotal: 18s\tremaining: 1m 6s\n214:\tlearn: 0.7564065\ttotal: 18s\tremaining: 1m 5s\n215:\tlearn: 0.7558709\ttotal: 18.1s\tremaining: 1m 5s\n216:\tlearn: 0.7553981\ttotal: 18.1s\tremaining: 1m 5s\n217:\tlearn: 0.7546978\ttotal: 18.1s\tremaining: 1m 4s\n218:\tlearn: 0.7539867\ttotal: 18.1s\tremaining: 1m 4s\n219:\tlearn: 0.7535509\ttotal: 18.1s\tremaining: 1m 4s\n220:\tlearn: 0.7530825\ttotal: 18.1s\tremaining: 1m 3s\n221:\tlearn: 0.7524375\ttotal: 18.1s\tremaining: 1m 3s\n222:\tlearn: 0.7519865\ttotal: 18.1s\tremaining: 1m 3s\n223:\tlearn: 0.7515911\ttotal: 18.1s\tremaining: 1m 2s\n224:\tlearn: 0.7511652\ttotal: 18.1s\tremaining: 1m 2s\n225:\tlearn: 0.7506727\ttotal: 18.1s\tremaining: 1m 2s\n226:\tlearn: 0.7502143\ttotal: 18.2s\tremaining: 1m 1s\n227:\tlearn: 0.7498804\ttotal: 18.2s\tremaining: 1m 1s\n228:\tlearn: 0.7492578\ttotal: 18.2s\tremaining: 1m 1s\n229:\tlearn: 0.7488244\ttotal: 18.2s\tremaining: 1m\n230:\tlearn: 0.7483143\ttotal: 18.2s\tremaining: 1m\n231:\tlearn: 0.7477446\ttotal: 18.2s\tremaining: 1m\n232:\tlearn: 0.7473038\ttotal: 18.2s\tremaining: 59.9s\n233:\tlearn: 0.7468284\ttotal: 18.2s\tremaining: 59.6s\n234:\tlearn: 0.7463562\ttotal: 18.2s\tremaining: 59.3s\n235:\tlearn: 0.7457868\ttotal: 18.2s\tremaining: 59.1s\n236:\tlearn: 0.7451926\ttotal: 18.3s\tremaining: 58.8s\n237:\tlearn: 0.7448499\ttotal: 18.3s\tremaining: 58.5s\n238:\tlearn: 0.7441298\ttotal: 18.3s\tremaining: 58.2s\n239:\tlearn: 0.7437159\ttotal: 18.3s\tremaining: 57.9s\n240:\tlearn: 0.7431675\ttotal: 18.3s\tremaining: 57.6s\n241:\tlearn: 0.7425765\ttotal: 18.3s\tremaining: 57.3s\n242:\tlearn: 0.7421192\ttotal: 18.3s\tremaining: 57s\n243:\tlearn: 0.7414635\ttotal: 18.3s\tremaining: 56.8s\n244:\tlearn: 0.7409721\ttotal: 18.3s\tremaining: 56.5s\n245:\tlearn: 0.7402598\ttotal: 18.3s\tremaining: 56.2s\n246:\tlearn: 0.7395945\ttotal: 18.3s\tremaining: 55.9s\n247:\tlearn: 0.7394481\ttotal: 18.4s\tremaining: 55.7s\n248:\tlearn: 0.7388122\ttotal: 18.4s\tremaining: 55.4s\n249:\tlearn: 0.7380387\ttotal: 18.4s\tremaining: 55.1s\n250:\tlearn: 0.7374965\ttotal: 18.4s\tremaining: 54.9s\n251:\tlearn: 0.7367114\ttotal: 18.4s\tremaining: 54.6s\n252:\tlearn: 0.7359962\ttotal: 18.4s\tremaining: 54.3s\n253:\tlearn: 0.7355318\ttotal: 18.4s\tremaining: 54.1s\n254:\tlearn: 0.7349593\ttotal: 18.4s\tremaining: 53.8s\n255:\tlearn: 0.7345613\ttotal: 18.4s\tremaining: 53.6s\n256:\tlearn: 0.7340170\ttotal: 18.4s\tremaining: 53.3s\n257:\tlearn: 0.7337760\ttotal: 18.5s\tremaining: 53.1s\n258:\tlearn: 0.7335304\ttotal: 18.5s\tremaining: 52.8s\n259:\tlearn: 0.7330356\ttotal: 18.5s\tremaining: 52.6s\n260:\tlearn: 0.7328128\ttotal: 18.5s\tremaining: 52.3s\n261:\tlearn: 0.7324809\ttotal: 18.5s\tremaining: 52.1s\n262:\tlearn: 0.7321426\ttotal: 18.5s\tremaining: 51.8s\n263:\tlearn: 0.7314851\ttotal: 18.5s\tremaining: 51.6s\n264:\tlearn: 0.7307969\ttotal: 18.5s\tremaining: 51.4s\n265:\tlearn: 0.7305920\ttotal: 18.5s\tremaining: 51.1s\n266:\tlearn: 0.7302906\ttotal: 18.5s\tremaining: 50.9s\n267:\tlearn: 0.7299425\ttotal: 18.5s\tremaining: 50.7s\n268:\tlearn: 0.7294494\ttotal: 18.6s\tremaining: 50.4s\n269:\tlearn: 0.7289577\ttotal: 18.6s\tremaining: 50.2s\n270:\tlearn: 0.7282652\ttotal: 18.6s\tremaining: 50s\n271:\tlearn: 0.7279439\ttotal: 18.6s\tremaining: 49.7s\n272:\tlearn: 0.7273788\ttotal: 18.6s\tremaining: 49.5s\n273:\tlearn: 0.7270885\ttotal: 18.6s\tremaining: 49.3s\n274:\tlearn: 0.7264904\ttotal: 18.6s\tremaining: 49.1s\n275:\tlearn: 0.7260111\ttotal: 18.6s\tremaining: 48.9s\n276:\tlearn: 0.7257582\ttotal: 18.6s\tremaining: 48.6s\n277:\tlearn: 0.7254342\ttotal: 18.6s\tremaining: 48.4s\n278:\tlearn: 0.7250048\ttotal: 18.6s\tremaining: 48.2s\n279:\tlearn: 0.7246890\ttotal: 18.7s\tremaining: 48s\n280:\tlearn: 0.7242661\ttotal: 18.7s\tremaining: 47.8s\n281:\tlearn: 0.7238201\ttotal: 18.7s\tremaining: 47.6s\n282:\tlearn: 0.7237005\ttotal: 18.7s\tremaining: 47.3s\n283:\tlearn: 0.7232378\ttotal: 18.7s\tremaining: 47.1s\n284:\tlearn: 0.7227303\ttotal: 18.7s\tremaining: 46.9s\n285:\tlearn: 0.7222172\ttotal: 18.7s\tremaining: 46.7s\n286:\tlearn: 0.7217731\ttotal: 18.7s\tremaining: 46.5s\n287:\tlearn: 0.7212831\ttotal: 18.7s\tremaining: 46.3s\n288:\tlearn: 0.7210917\ttotal: 18.7s\tremaining: 46.1s\n289:\tlearn: 0.7207433\ttotal: 18.8s\tremaining: 45.9s\n290:\tlearn: 0.7203249\ttotal: 18.8s\tremaining: 45.7s\n291:\tlearn: 0.7199175\ttotal: 18.8s\tremaining: 45.5s\n292:\tlearn: 0.7194274\ttotal: 18.8s\tremaining: 45.3s\n293:\tlearn: 0.7192152\ttotal: 18.8s\tremaining: 45.1s\n294:\tlearn: 0.7186385\ttotal: 18.8s\tremaining: 44.9s\n295:\tlearn: 0.7181614\ttotal: 18.8s\tremaining: 44.7s\n296:\tlearn: 0.7175789\ttotal: 18.8s\tremaining: 44.5s\n297:\tlearn: 0.7171443\ttotal: 18.8s\tremaining: 44.3s\n298:\tlearn: 0.7167257\ttotal: 18.8s\tremaining: 44.2s\n299:\tlearn: 0.7162912\ttotal: 18.8s\tremaining: 44s\n300:\tlearn: 0.7157672\ttotal: 18.9s\tremaining: 43.8s\n301:\tlearn: 0.7153785\ttotal: 18.9s\tremaining: 43.6s\n302:\tlearn: 0.7147734\ttotal: 18.9s\tremaining: 43.4s\n303:\tlearn: 0.7143435\ttotal: 18.9s\tremaining: 43.2s\n304:\tlearn: 0.7138236\ttotal: 18.9s\tremaining: 43s\n305:\tlearn: 0.7134358\ttotal: 18.9s\tremaining: 42.9s\n306:\tlearn: 0.7132422\ttotal: 18.9s\tremaining: 42.7s\n307:\tlearn: 0.7125584\ttotal: 18.9s\tremaining: 42.5s\n308:\tlearn: 0.7120944\ttotal: 18.9s\tremaining: 42.3s\n309:\tlearn: 0.7116094\ttotal: 18.9s\tremaining: 42.2s\n310:\tlearn: 0.7112695\ttotal: 18.9s\tremaining: 42s\n311:\tlearn: 0.7104803\ttotal: 19s\tremaining: 41.8s\n312:\tlearn: 0.7099414\ttotal: 19s\tremaining: 41.6s\n313:\tlearn: 0.7095678\ttotal: 19s\tremaining: 41.5s\n314:\tlearn: 0.7093163\ttotal: 19s\tremaining: 41.3s\n315:\tlearn: 0.7088563\ttotal: 19s\tremaining: 41.1s\n316:\tlearn: 0.7082284\ttotal: 19s\tremaining: 40.9s\n317:\tlearn: 0.7077937\ttotal: 19s\tremaining: 40.8s\n318:\tlearn: 0.7075000\ttotal: 19s\tremaining: 40.6s\n319:\tlearn: 0.7070076\ttotal: 19s\tremaining: 40.4s\n320:\tlearn: 0.7064076\ttotal: 19s\tremaining: 40.3s\n321:\tlearn: 0.7058754\ttotal: 19s\tremaining: 40.1s\n322:\tlearn: 0.7055411\ttotal: 19.1s\tremaining: 39.9s\n323:\tlearn: 0.7052094\ttotal: 19.1s\tremaining: 39.8s\n324:\tlearn: 0.7049010\ttotal: 19.1s\tremaining: 39.6s\n325:\tlearn: 0.7045679\ttotal: 19.1s\tremaining: 39.5s\n326:\tlearn: 0.7041678\ttotal: 19.1s\tremaining: 39.3s\n327:\tlearn: 0.7035422\ttotal: 19.1s\tremaining: 39.1s\n328:\tlearn: 0.7032595\ttotal: 19.1s\tremaining: 39s\n329:\tlearn: 0.7028312\ttotal: 19.1s\tremaining: 38.8s\n330:\tlearn: 0.7025461\ttotal: 19.1s\tremaining: 38.7s\n331:\tlearn: 0.7021140\ttotal: 19.1s\tremaining: 38.5s\n332:\tlearn: 0.7016896\ttotal: 19.1s\tremaining: 38.4s\n333:\tlearn: 0.7012545\ttotal: 19.2s\tremaining: 38.2s\n334:\tlearn: 0.7010353\ttotal: 19.2s\tremaining: 38.1s\n335:\tlearn: 0.7006611\ttotal: 19.2s\tremaining: 37.9s\n336:\tlearn: 0.7004038\ttotal: 19.2s\tremaining: 37.7s\n337:\tlearn: 0.6999189\ttotal: 19.2s\tremaining: 37.6s\n338:\tlearn: 0.6996225\ttotal: 19.2s\tremaining: 37.4s\n339:\tlearn: 0.6993478\ttotal: 19.2s\tremaining: 37.3s\n340:\tlearn: 0.6989887\ttotal: 19.2s\tremaining: 37.2s\n341:\tlearn: 0.6985511\ttotal: 19.2s\tremaining: 37s\n342:\tlearn: 0.6980000\ttotal: 19.2s\tremaining: 36.9s\n343:\tlearn: 0.6976522\ttotal: 19.3s\tremaining: 36.7s\n344:\tlearn: 0.6972161\ttotal: 19.3s\tremaining: 36.6s\n345:\tlearn: 0.6967526\ttotal: 19.3s\tremaining: 36.4s\n346:\tlearn: 0.6961148\ttotal: 19.3s\tremaining: 36.3s\n347:\tlearn: 0.6958388\ttotal: 19.3s\tremaining: 36.1s\n348:\tlearn: 0.6953629\ttotal: 19.3s\tremaining: 36s\n349:\tlearn: 0.6948158\ttotal: 19.3s\tremaining: 35.9s\n350:\tlearn: 0.6942311\ttotal: 19.3s\tremaining: 35.7s\n351:\tlearn: 0.6940035\ttotal: 19.3s\tremaining: 35.6s\n352:\tlearn: 0.6933736\ttotal: 19.3s\tremaining: 35.4s\n353:\tlearn: 0.6929746\ttotal: 19.3s\tremaining: 35.3s\n354:\tlearn: 0.6928056\ttotal: 19.4s\tremaining: 35.2s\n355:\tlearn: 0.6923903\ttotal: 19.4s\tremaining: 35s\n356:\tlearn: 0.6918249\ttotal: 19.4s\tremaining: 34.9s\n357:\tlearn: 0.6915089\ttotal: 19.4s\tremaining: 34.8s\n358:\tlearn: 0.6910768\ttotal: 19.4s\tremaining: 34.6s\n359:\tlearn: 0.6907271\ttotal: 19.4s\tremaining: 34.5s\n360:\tlearn: 0.6903611\ttotal: 19.4s\tremaining: 34.4s\n361:\tlearn: 0.6898770\ttotal: 19.4s\tremaining: 34.2s\n362:\tlearn: 0.6893992\ttotal: 19.4s\tremaining: 34.1s\n363:\tlearn: 0.6888469\ttotal: 19.4s\tremaining: 34s\n364:\tlearn: 0.6881317\ttotal: 19.4s\tremaining: 33.8s\n365:\tlearn: 0.6878196\ttotal: 19.5s\tremaining: 33.7s\n366:\tlearn: 0.6873915\ttotal: 19.5s\tremaining: 33.6s\n367:\tlearn: 0.6870217\ttotal: 19.5s\tremaining: 33.4s\n368:\tlearn: 0.6865711\ttotal: 19.5s\tremaining: 33.3s\n369:\tlearn: 0.6861112\ttotal: 19.5s\tremaining: 33.2s\n370:\tlearn: 0.6857611\ttotal: 19.5s\tremaining: 33.1s\n371:\tlearn: 0.6855124\ttotal: 19.5s\tremaining: 32.9s\n372:\tlearn: 0.6850041\ttotal: 19.5s\tremaining: 32.8s\n373:\tlearn: 0.6845723\ttotal: 19.5s\tremaining: 32.7s\n374:\tlearn: 0.6841764\ttotal: 19.5s\tremaining: 32.6s\n375:\tlearn: 0.6838315\ttotal: 19.6s\tremaining: 32.4s\n376:\tlearn: 0.6831736\ttotal: 19.6s\tremaining: 32.3s\n377:\tlearn: 0.6829668\ttotal: 19.6s\tremaining: 32.2s\n378:\tlearn: 0.6827532\ttotal: 19.6s\tremaining: 32.1s\n379:\tlearn: 0.6821899\ttotal: 19.6s\tremaining: 32s\n380:\tlearn: 0.6816478\ttotal: 19.6s\tremaining: 31.8s\n381:\tlearn: 0.6811882\ttotal: 19.6s\tremaining: 31.7s\n382:\tlearn: 0.6808864\ttotal: 19.6s\tremaining: 31.6s\n383:\tlearn: 0.6804002\ttotal: 19.6s\tremaining: 31.5s\n384:\tlearn: 0.6800973\ttotal: 19.6s\tremaining: 31.4s\n385:\tlearn: 0.6796533\ttotal: 19.6s\tremaining: 31.3s\n386:\tlearn: 0.6793196\ttotal: 19.7s\tremaining: 31.1s\n387:\tlearn: 0.6788390\ttotal: 19.7s\tremaining: 31s\n388:\tlearn: 0.6784329\ttotal: 19.7s\tremaining: 30.9s\n389:\tlearn: 0.6780044\ttotal: 19.7s\tremaining: 30.8s\n390:\tlearn: 0.6775722\ttotal: 19.7s\tremaining: 30.7s\n391:\tlearn: 0.6773522\ttotal: 19.7s\tremaining: 30.6s\n392:\tlearn: 0.6771317\ttotal: 19.7s\tremaining: 30.5s\n393:\tlearn: 0.6766189\ttotal: 19.7s\tremaining: 30.4s\n394:\tlearn: 0.6761465\ttotal: 19.7s\tremaining: 30.2s\n395:\tlearn: 0.6758156\ttotal: 19.8s\tremaining: 30.1s\n396:\tlearn: 0.6753463\ttotal: 19.8s\tremaining: 30s\n397:\tlearn: 0.6750499\ttotal: 19.8s\tremaining: 29.9s\n398:\tlearn: 0.6747855\ttotal: 19.8s\tremaining: 29.8s\n399:\tlearn: 0.6743777\ttotal: 19.8s\tremaining: 29.7s\n400:\tlearn: 0.6741097\ttotal: 19.8s\tremaining: 29.6s\n401:\tlearn: 0.6739259\ttotal: 19.8s\tremaining: 29.5s\n402:\tlearn: 0.6735040\ttotal: 19.8s\tremaining: 29.4s\n403:\tlearn: 0.6731441\ttotal: 19.8s\tremaining: 29.2s\n404:\tlearn: 0.6729307\ttotal: 19.8s\tremaining: 29.1s\n405:\tlearn: 0.6726215\ttotal: 19.8s\tremaining: 29s\n406:\tlearn: 0.6722098\ttotal: 19.8s\tremaining: 28.9s\n407:\tlearn: 0.6717292\ttotal: 19.9s\tremaining: 28.8s\n408:\tlearn: 0.6714164\ttotal: 19.9s\tremaining: 28.7s\n409:\tlearn: 0.6709612\ttotal: 19.9s\tremaining: 28.6s\n410:\tlearn: 0.6704514\ttotal: 19.9s\tremaining: 28.5s\n411:\tlearn: 0.6702933\ttotal: 19.9s\tremaining: 28.4s\n412:\tlearn: 0.6698416\ttotal: 19.9s\tremaining: 28.3s\n413:\tlearn: 0.6695894\ttotal: 19.9s\tremaining: 28.2s\n414:\tlearn: 0.6692389\ttotal: 19.9s\tremaining: 28.1s\n415:\tlearn: 0.6687303\ttotal: 19.9s\tremaining: 28s\n416:\tlearn: 0.6684231\ttotal: 19.9s\tremaining: 27.9s\n417:\tlearn: 0.6681380\ttotal: 19.9s\tremaining: 27.8s\n418:\tlearn: 0.6677352\ttotal: 20s\tremaining: 27.7s\n419:\tlearn: 0.6673796\ttotal: 20s\tremaining: 27.6s\n420:\tlearn: 0.6669376\ttotal: 20s\tremaining: 27.5s\n421:\tlearn: 0.6664292\ttotal: 20s\tremaining: 27.4s\n422:\tlearn: 0.6660095\ttotal: 20s\tremaining: 27.3s\n423:\tlearn: 0.6656781\ttotal: 20s\tremaining: 27.2s\n424:\tlearn: 0.6652195\ttotal: 20s\tremaining: 27.1s\n425:\tlearn: 0.6648862\ttotal: 20s\tremaining: 27s\n426:\tlearn: 0.6645086\ttotal: 20s\tremaining: 26.9s\n427:\tlearn: 0.6641017\ttotal: 20s\tremaining: 26.8s\n428:\tlearn: 0.6636445\ttotal: 20.1s\tremaining: 26.7s\n429:\tlearn: 0.6630741\ttotal: 20.1s\tremaining: 26.6s\n430:\tlearn: 0.6626465\ttotal: 20.1s\tremaining: 26.5s\n431:\tlearn: 0.6622322\ttotal: 20.1s\tremaining: 26.4s\n432:\tlearn: 0.6618375\ttotal: 20.1s\tremaining: 26.3s\n433:\tlearn: 0.6614611\ttotal: 20.1s\tremaining: 26.2s\n434:\tlearn: 0.6611922\ttotal: 20.1s\tremaining: 26.1s\n435:\tlearn: 0.6609331\ttotal: 20.1s\tremaining: 26s\n436:\tlearn: 0.6604899\ttotal: 20.1s\tremaining: 25.9s\n437:\tlearn: 0.6598555\ttotal: 20.1s\tremaining: 25.8s\n438:\tlearn: 0.6593377\ttotal: 20.1s\tremaining: 25.7s\n439:\tlearn: 0.6591101\ttotal: 20.2s\tremaining: 25.7s\n440:\tlearn: 0.6587682\ttotal: 20.2s\tremaining: 25.6s\n441:\tlearn: 0.6585001\ttotal: 20.2s\tremaining: 25.5s\n442:\tlearn: 0.6578641\ttotal: 20.2s\tremaining: 25.4s\n443:\tlearn: 0.6575275\ttotal: 20.2s\tremaining: 25.3s\n444:\tlearn: 0.6570482\ttotal: 20.2s\tremaining: 25.2s\n445:\tlearn: 0.6565919\ttotal: 20.2s\tremaining: 25.1s\n446:\tlearn: 0.6563425\ttotal: 20.2s\tremaining: 25s\n447:\tlearn: 0.6559671\ttotal: 20.2s\tremaining: 24.9s\n448:\tlearn: 0.6553921\ttotal: 20.2s\tremaining: 24.8s\n449:\tlearn: 0.6549915\ttotal: 20.2s\tremaining: 24.7s\n450:\tlearn: 0.6546519\ttotal: 20.3s\tremaining: 24.7s\n451:\tlearn: 0.6543261\ttotal: 20.3s\tremaining: 24.6s\n452:\tlearn: 0.6539777\ttotal: 20.3s\tremaining: 24.5s\n453:\tlearn: 0.6535816\ttotal: 20.3s\tremaining: 24.4s\n454:\tlearn: 0.6530560\ttotal: 20.3s\tremaining: 24.3s\n455:\tlearn: 0.6527939\ttotal: 20.3s\tremaining: 24.2s\n456:\tlearn: 0.6524032\ttotal: 20.3s\tremaining: 24.1s\n457:\tlearn: 0.6517997\ttotal: 20.3s\tremaining: 24.1s\n458:\tlearn: 0.6514646\ttotal: 20.3s\tremaining: 24s\n459:\tlearn: 0.6511250\ttotal: 20.3s\tremaining: 23.9s\n460:\tlearn: 0.6508175\ttotal: 20.4s\tremaining: 23.8s\n461:\tlearn: 0.6504430\ttotal: 20.4s\tremaining: 23.7s\n462:\tlearn: 0.6500898\ttotal: 20.4s\tremaining: 23.6s\n463:\tlearn: 0.6496192\ttotal: 20.4s\tremaining: 23.5s\n464:\tlearn: 0.6490159\ttotal: 20.4s\tremaining: 23.5s\n465:\tlearn: 0.6487130\ttotal: 20.4s\tremaining: 23.4s\n466:\tlearn: 0.6482017\ttotal: 20.4s\tremaining: 23.3s\n467:\tlearn: 0.6475404\ttotal: 20.4s\tremaining: 23.2s\n468:\tlearn: 0.6470640\ttotal: 20.4s\tremaining: 23.1s\n469:\tlearn: 0.6467941\ttotal: 20.4s\tremaining: 23s\n470:\tlearn: 0.6463724\ttotal: 20.4s\tremaining: 23s\n471:\tlearn: 0.6458820\ttotal: 20.5s\tremaining: 22.9s\n472:\tlearn: 0.6456751\ttotal: 20.5s\tremaining: 22.8s\n473:\tlearn: 0.6455500\ttotal: 20.5s\tremaining: 22.7s\n474:\tlearn: 0.6451834\ttotal: 20.5s\tremaining: 22.6s\n475:\tlearn: 0.6449081\ttotal: 20.5s\tremaining: 22.6s\n476:\tlearn: 0.6446258\ttotal: 20.5s\tremaining: 22.5s\n477:\tlearn: 0.6441062\ttotal: 20.5s\tremaining: 22.4s\n478:\tlearn: 0.6437266\ttotal: 20.5s\tremaining: 22.3s\n479:\tlearn: 0.6434252\ttotal: 20.5s\tremaining: 22.2s\n480:\tlearn: 0.6430505\ttotal: 20.5s\tremaining: 22.2s\n481:\tlearn: 0.6426821\ttotal: 20.5s\tremaining: 22.1s\n482:\tlearn: 0.6423246\ttotal: 20.5s\tremaining: 22s\n483:\tlearn: 0.6419434\ttotal: 20.6s\tremaining: 21.9s\n484:\tlearn: 0.6417666\ttotal: 20.6s\tremaining: 21.8s\n485:\tlearn: 0.6413696\ttotal: 20.6s\tremaining: 21.8s\n486:\tlearn: 0.6411704\ttotal: 20.6s\tremaining: 21.7s\n487:\tlearn: 0.6409743\ttotal: 20.6s\tremaining: 21.6s\n488:\tlearn: 0.6407259\ttotal: 20.6s\tremaining: 21.5s\n489:\tlearn: 0.6402725\ttotal: 20.6s\tremaining: 21.5s\n490:\tlearn: 0.6397593\ttotal: 20.6s\tremaining: 21.4s\n491:\tlearn: 0.6392823\ttotal: 20.6s\tremaining: 21.3s\n492:\tlearn: 0.6389350\ttotal: 20.6s\tremaining: 21.2s\n493:\tlearn: 0.6384193\ttotal: 20.6s\tremaining: 21.2s\n494:\tlearn: 0.6377786\ttotal: 20.7s\tremaining: 21.1s\n495:\tlearn: 0.6375113\ttotal: 20.7s\tremaining: 21s\n496:\tlearn: 0.6372563\ttotal: 20.7s\tremaining: 20.9s\n497:\tlearn: 0.6368176\ttotal: 20.7s\tremaining: 20.9s\n498:\tlearn: 0.6365425\ttotal: 20.7s\tremaining: 20.8s\n499:\tlearn: 0.6359747\ttotal: 20.7s\tremaining: 20.7s\n500:\tlearn: 0.6356908\ttotal: 20.7s\tremaining: 20.6s\n501:\tlearn: 0.6353032\ttotal: 20.7s\tremaining: 20.6s\n502:\tlearn: 0.6347254\ttotal: 20.7s\tremaining: 20.5s\n503:\tlearn: 0.6344467\ttotal: 20.7s\tremaining: 20.4s\n504:\tlearn: 0.6341670\ttotal: 20.7s\tremaining: 20.3s\n505:\tlearn: 0.6338435\ttotal: 20.8s\tremaining: 20.3s\n506:\tlearn: 0.6334439\ttotal: 20.8s\tremaining: 20.2s\n507:\tlearn: 0.6331065\ttotal: 20.8s\tremaining: 20.1s\n508:\tlearn: 0.6326296\ttotal: 20.8s\tremaining: 20.1s\n509:\tlearn: 0.6321725\ttotal: 20.8s\tremaining: 20s\n510:\tlearn: 0.6317538\ttotal: 20.8s\tremaining: 19.9s\n511:\tlearn: 0.6312533\ttotal: 20.8s\tremaining: 19.8s\n512:\tlearn: 0.6308335\ttotal: 20.8s\tremaining: 19.8s\n513:\tlearn: 0.6305623\ttotal: 20.8s\tremaining: 19.7s\n514:\tlearn: 0.6302255\ttotal: 20.8s\tremaining: 19.6s\n515:\tlearn: 0.6298844\ttotal: 20.9s\tremaining: 19.6s\n516:\tlearn: 0.6296123\ttotal: 20.9s\tremaining: 19.5s\n517:\tlearn: 0.6292261\ttotal: 20.9s\tremaining: 19.4s\n518:\tlearn: 0.6288617\ttotal: 20.9s\tremaining: 19.4s\n519:\tlearn: 0.6284357\ttotal: 20.9s\tremaining: 19.3s\n520:\tlearn: 0.6280974\ttotal: 20.9s\tremaining: 19.2s\n521:\tlearn: 0.6276038\ttotal: 20.9s\tremaining: 19.1s\n522:\tlearn: 0.6272776\ttotal: 20.9s\tremaining: 19.1s\n523:\tlearn: 0.6266674\ttotal: 20.9s\tremaining: 19s\n524:\tlearn: 0.6264714\ttotal: 20.9s\tremaining: 18.9s\n525:\tlearn: 0.6260113\ttotal: 20.9s\tremaining: 18.9s\n526:\tlearn: 0.6256697\ttotal: 21s\tremaining: 18.8s\n527:\tlearn: 0.6252908\ttotal: 21s\tremaining: 18.7s\n528:\tlearn: 0.6250431\ttotal: 21s\tremaining: 18.7s\n529:\tlearn: 0.6246668\ttotal: 21s\tremaining: 18.6s\n530:\tlearn: 0.6242143\ttotal: 21s\tremaining: 18.5s\n531:\tlearn: 0.6239765\ttotal: 21s\tremaining: 18.5s\n532:\tlearn: 0.6235435\ttotal: 21s\tremaining: 18.4s\n533:\tlearn: 0.6234360\ttotal: 21s\tremaining: 18.3s\n534:\tlearn: 0.6230571\ttotal: 21s\tremaining: 18.3s\n535:\tlearn: 0.6226351\ttotal: 21s\tremaining: 18.2s\n536:\tlearn: 0.6223328\ttotal: 21s\tremaining: 18.1s\n537:\tlearn: 0.6220624\ttotal: 21.1s\tremaining: 18.1s\n538:\tlearn: 0.6217717\ttotal: 21.1s\tremaining: 18s\n539:\tlearn: 0.6213122\ttotal: 21.1s\tremaining: 18s\n540:\tlearn: 0.6208864\ttotal: 21.1s\tremaining: 17.9s\n541:\tlearn: 0.6206019\ttotal: 21.1s\tremaining: 17.8s\n542:\tlearn: 0.6202156\ttotal: 21.1s\tremaining: 17.8s\n543:\tlearn: 0.6198462\ttotal: 21.1s\tremaining: 17.7s\n544:\tlearn: 0.6196032\ttotal: 21.1s\tremaining: 17.6s\n545:\tlearn: 0.6191878\ttotal: 21.1s\tremaining: 17.6s\n546:\tlearn: 0.6188677\ttotal: 21.1s\tremaining: 17.5s\n547:\tlearn: 0.6185191\ttotal: 21.1s\tremaining: 17.4s\n548:\tlearn: 0.6178062\ttotal: 21.2s\tremaining: 17.4s\n549:\tlearn: 0.6175160\ttotal: 21.2s\tremaining: 17.3s\n550:\tlearn: 0.6171780\ttotal: 21.2s\tremaining: 17.3s\n551:\tlearn: 0.6169601\ttotal: 21.2s\tremaining: 17.2s\n552:\tlearn: 0.6166141\ttotal: 21.2s\tremaining: 17.1s\n553:\tlearn: 0.6163185\ttotal: 21.2s\tremaining: 17.1s\n554:\tlearn: 0.6160292\ttotal: 21.2s\tremaining: 17s\n555:\tlearn: 0.6155912\ttotal: 21.2s\tremaining: 16.9s\n556:\tlearn: 0.6153874\ttotal: 21.2s\tremaining: 16.9s\n557:\tlearn: 0.6150980\ttotal: 21.2s\tremaining: 16.8s\n558:\tlearn: 0.6149196\ttotal: 21.2s\tremaining: 16.8s\n559:\tlearn: 0.6144234\ttotal: 21.3s\tremaining: 16.7s\n560:\tlearn: 0.6139285\ttotal: 21.3s\tremaining: 16.6s\n561:\tlearn: 0.6137459\ttotal: 21.3s\tremaining: 16.6s\n562:\tlearn: 0.6133029\ttotal: 21.3s\tremaining: 16.5s\n563:\tlearn: 0.6128516\ttotal: 21.3s\tremaining: 16.5s\n564:\tlearn: 0.6123937\ttotal: 21.3s\tremaining: 16.4s\n565:\tlearn: 0.6119575\ttotal: 21.3s\tremaining: 16.3s\n566:\tlearn: 0.6115368\ttotal: 21.3s\tremaining: 16.3s\n567:\tlearn: 0.6110050\ttotal: 21.3s\tremaining: 16.2s\n568:\tlearn: 0.6106805\ttotal: 21.3s\tremaining: 16.2s\n569:\tlearn: 0.6102906\ttotal: 21.4s\tremaining: 16.1s\n570:\tlearn: 0.6100393\ttotal: 21.4s\tremaining: 16s\n571:\tlearn: 0.6096863\ttotal: 21.4s\tremaining: 16s\n572:\tlearn: 0.6093261\ttotal: 21.4s\tremaining: 15.9s\n573:\tlearn: 0.6090260\ttotal: 21.4s\tremaining: 15.9s\n574:\tlearn: 0.6086232\ttotal: 21.4s\tremaining: 15.8s\n575:\tlearn: 0.6080916\ttotal: 21.4s\tremaining: 15.8s\n576:\tlearn: 0.6077002\ttotal: 21.4s\tremaining: 15.7s\n577:\tlearn: 0.6074096\ttotal: 21.4s\tremaining: 15.6s\n578:\tlearn: 0.6070978\ttotal: 21.4s\tremaining: 15.6s\n579:\tlearn: 0.6066867\ttotal: 21.4s\tremaining: 15.5s\n580:\tlearn: 0.6062886\ttotal: 21.5s\tremaining: 15.5s\n581:\tlearn: 0.6058458\ttotal: 21.5s\tremaining: 15.4s\n582:\tlearn: 0.6055120\ttotal: 21.5s\tremaining: 15.4s\n583:\tlearn: 0.6049701\ttotal: 21.5s\tremaining: 15.3s\n584:\tlearn: 0.6045346\ttotal: 21.5s\tremaining: 15.2s\n585:\tlearn: 0.6043354\ttotal: 21.5s\tremaining: 15.2s\n586:\tlearn: 0.6040409\ttotal: 21.5s\tremaining: 15.1s\n587:\tlearn: 0.6037772\ttotal: 21.5s\tremaining: 15.1s\n588:\tlearn: 0.6033435\ttotal: 21.5s\tremaining: 15s\n589:\tlearn: 0.6029240\ttotal: 21.5s\tremaining: 15s\n590:\tlearn: 0.6026111\ttotal: 21.5s\tremaining: 14.9s\n591:\tlearn: 0.6023887\ttotal: 21.6s\tremaining: 14.9s\n592:\tlearn: 0.6020893\ttotal: 21.6s\tremaining: 14.8s\n593:\tlearn: 0.6015411\ttotal: 21.6s\tremaining: 14.7s\n594:\tlearn: 0.6011061\ttotal: 21.6s\tremaining: 14.7s\n595:\tlearn: 0.6006897\ttotal: 21.6s\tremaining: 14.6s\n596:\tlearn: 0.6001991\ttotal: 21.6s\tremaining: 14.6s\n597:\tlearn: 0.5997204\ttotal: 21.6s\tremaining: 14.5s\n598:\tlearn: 0.5990995\ttotal: 21.6s\tremaining: 14.5s\n599:\tlearn: 0.5986359\ttotal: 21.6s\tremaining: 14.4s\n600:\tlearn: 0.5982101\ttotal: 21.6s\tremaining: 14.4s\n601:\tlearn: 0.5978823\ttotal: 21.7s\tremaining: 14.3s\n602:\tlearn: 0.5976358\ttotal: 21.7s\tremaining: 14.3s\n603:\tlearn: 0.5973401\ttotal: 21.7s\tremaining: 14.2s\n604:\tlearn: 0.5968446\ttotal: 21.7s\tremaining: 14.2s\n605:\tlearn: 0.5967246\ttotal: 21.7s\tremaining: 14.1s\n606:\tlearn: 0.5963980\ttotal: 21.7s\tremaining: 14.1s\n607:\tlearn: 0.5960852\ttotal: 21.7s\tremaining: 14s\n608:\tlearn: 0.5957313\ttotal: 21.7s\tremaining: 13.9s\n609:\tlearn: 0.5954397\ttotal: 21.7s\tremaining: 13.9s\n610:\tlearn: 0.5951638\ttotal: 21.7s\tremaining: 13.8s\n611:\tlearn: 0.5946351\ttotal: 21.8s\tremaining: 13.8s\n612:\tlearn: 0.5943678\ttotal: 21.8s\tremaining: 13.7s\n613:\tlearn: 0.5940761\ttotal: 21.8s\tremaining: 13.7s\n614:\tlearn: 0.5935022\ttotal: 21.8s\tremaining: 13.6s\n615:\tlearn: 0.5931377\ttotal: 21.8s\tremaining: 13.6s\n616:\tlearn: 0.5927762\ttotal: 21.8s\tremaining: 13.5s\n617:\tlearn: 0.5925095\ttotal: 21.8s\tremaining: 13.5s\n618:\tlearn: 0.5923341\ttotal: 21.8s\tremaining: 13.4s\n619:\tlearn: 0.5920671\ttotal: 21.8s\tremaining: 13.4s\n620:\tlearn: 0.5916744\ttotal: 21.8s\tremaining: 13.3s\n621:\tlearn: 0.5914437\ttotal: 21.8s\tremaining: 13.3s\n622:\tlearn: 0.5912524\ttotal: 21.9s\tremaining: 13.2s\n623:\tlearn: 0.5907515\ttotal: 21.9s\tremaining: 13.2s\n624:\tlearn: 0.5903304\ttotal: 21.9s\tremaining: 13.1s\n625:\tlearn: 0.5899708\ttotal: 21.9s\tremaining: 13.1s\n626:\tlearn: 0.5895620\ttotal: 21.9s\tremaining: 13s\n627:\tlearn: 0.5890658\ttotal: 21.9s\tremaining: 13s\n628:\tlearn: 0.5887490\ttotal: 21.9s\tremaining: 12.9s\n629:\tlearn: 0.5883997\ttotal: 21.9s\tremaining: 12.9s\n630:\tlearn: 0.5878910\ttotal: 21.9s\tremaining: 12.8s\n631:\tlearn: 0.5874506\ttotal: 21.9s\tremaining: 12.8s\n632:\tlearn: 0.5869351\ttotal: 22s\tremaining: 12.7s\n633:\tlearn: 0.5867033\ttotal: 22s\tremaining: 12.7s\n634:\tlearn: 0.5864156\ttotal: 22s\tremaining: 12.6s\n635:\tlearn: 0.5861731\ttotal: 22s\tremaining: 12.6s\n636:\tlearn: 0.5858383\ttotal: 22s\tremaining: 12.5s\n637:\tlearn: 0.5855929\ttotal: 22s\tremaining: 12.5s\n638:\tlearn: 0.5852561\ttotal: 22s\tremaining: 12.4s\n639:\tlearn: 0.5850103\ttotal: 22s\tremaining: 12.4s\n640:\tlearn: 0.5845859\ttotal: 22s\tremaining: 12.3s\n641:\tlearn: 0.5843606\ttotal: 22s\tremaining: 12.3s\n642:\tlearn: 0.5839176\ttotal: 22s\tremaining: 12.2s\n643:\tlearn: 0.5835458\ttotal: 22.1s\tremaining: 12.2s\n644:\tlearn: 0.5830812\ttotal: 22.1s\tremaining: 12.1s\n645:\tlearn: 0.5829277\ttotal: 22.1s\tremaining: 12.1s\n646:\tlearn: 0.5825807\ttotal: 22.1s\tremaining: 12s\n647:\tlearn: 0.5821982\ttotal: 22.1s\tremaining: 12s\n648:\tlearn: 0.5818040\ttotal: 22.1s\tremaining: 12s\n649:\tlearn: 0.5815783\ttotal: 22.1s\tremaining: 11.9s\n650:\tlearn: 0.5813942\ttotal: 22.1s\tremaining: 11.9s\n651:\tlearn: 0.5812405\ttotal: 22.1s\tremaining: 11.8s\n652:\tlearn: 0.5809548\ttotal: 22.1s\tremaining: 11.8s\n653:\tlearn: 0.5806199\ttotal: 22.1s\tremaining: 11.7s\n654:\tlearn: 0.5802262\ttotal: 22.2s\tremaining: 11.7s\n655:\tlearn: 0.5799335\ttotal: 22.2s\tremaining: 11.6s\n656:\tlearn: 0.5796844\ttotal: 22.2s\tremaining: 11.6s\n657:\tlearn: 0.5795183\ttotal: 22.2s\tremaining: 11.5s\n658:\tlearn: 0.5793107\ttotal: 22.2s\tremaining: 11.5s\n659:\tlearn: 0.5790948\ttotal: 22.2s\tremaining: 11.4s\n660:\tlearn: 0.5787853\ttotal: 22.2s\tremaining: 11.4s\n661:\tlearn: 0.5784669\ttotal: 22.2s\tremaining: 11.3s\n662:\tlearn: 0.5781740\ttotal: 22.2s\tremaining: 11.3s\n663:\tlearn: 0.5778487\ttotal: 22.2s\tremaining: 11.3s\n664:\tlearn: 0.5775255\ttotal: 22.3s\tremaining: 11.2s\n665:\tlearn: 0.5771762\ttotal: 22.3s\tremaining: 11.2s\n666:\tlearn: 0.5770638\ttotal: 22.3s\tremaining: 11.1s\n667:\tlearn: 0.5766834\ttotal: 22.3s\tremaining: 11.1s\n668:\tlearn: 0.5762054\ttotal: 22.3s\tremaining: 11s\n669:\tlearn: 0.5758117\ttotal: 22.3s\tremaining: 11s\n670:\tlearn: 0.5755501\ttotal: 22.3s\tremaining: 10.9s\n671:\tlearn: 0.5751611\ttotal: 22.3s\tremaining: 10.9s\n672:\tlearn: 0.5750341\ttotal: 22.3s\tremaining: 10.8s\n673:\tlearn: 0.5748275\ttotal: 22.3s\tremaining: 10.8s\n674:\tlearn: 0.5743067\ttotal: 22.3s\tremaining: 10.8s\n675:\tlearn: 0.5739780\ttotal: 22.4s\tremaining: 10.7s\n676:\tlearn: 0.5736919\ttotal: 22.4s\tremaining: 10.7s\n677:\tlearn: 0.5732925\ttotal: 22.4s\tremaining: 10.6s\n678:\tlearn: 0.5729598\ttotal: 22.4s\tremaining: 10.6s\n679:\tlearn: 0.5726353\ttotal: 22.4s\tremaining: 10.5s\n680:\tlearn: 0.5723288\ttotal: 22.4s\tremaining: 10.5s\n681:\tlearn: 0.5719975\ttotal: 22.4s\tremaining: 10.4s\n682:\tlearn: 0.5717080\ttotal: 22.4s\tremaining: 10.4s\n683:\tlearn: 0.5714291\ttotal: 22.4s\tremaining: 10.4s\n684:\tlearn: 0.5710954\ttotal: 22.4s\tremaining: 10.3s\n685:\tlearn: 0.5707394\ttotal: 22.4s\tremaining: 10.3s\n686:\tlearn: 0.5704026\ttotal: 22.5s\tremaining: 10.2s\n687:\tlearn: 0.5701660\ttotal: 22.5s\tremaining: 10.2s\n688:\tlearn: 0.5700391\ttotal: 22.5s\tremaining: 10.1s\n689:\tlearn: 0.5696155\ttotal: 22.5s\tremaining: 10.1s\n690:\tlearn: 0.5692959\ttotal: 22.5s\tremaining: 10.1s\n691:\tlearn: 0.5690370\ttotal: 22.5s\tremaining: 10s\n692:\tlearn: 0.5687372\ttotal: 22.5s\tremaining: 9.97s\n693:\tlearn: 0.5684655\ttotal: 22.5s\tremaining: 9.93s\n694:\tlearn: 0.5683243\ttotal: 22.5s\tremaining: 9.89s\n695:\tlearn: 0.5678993\ttotal: 22.5s\tremaining: 9.84s\n696:\tlearn: 0.5673983\ttotal: 22.5s\tremaining: 9.8s\n697:\tlearn: 0.5669592\ttotal: 22.6s\tremaining: 9.76s\n698:\tlearn: 0.5667393\ttotal: 22.6s\tremaining: 9.72s\n699:\tlearn: 0.5664230\ttotal: 22.6s\tremaining: 9.68s\n700:\tlearn: 0.5661633\ttotal: 22.6s\tremaining: 9.63s\n701:\tlearn: 0.5659402\ttotal: 22.6s\tremaining: 9.59s\n702:\tlearn: 0.5656093\ttotal: 22.6s\tremaining: 9.55s\n703:\tlearn: 0.5652269\ttotal: 22.6s\tremaining: 9.51s\n704:\tlearn: 0.5648721\ttotal: 22.6s\tremaining: 9.47s\n705:\tlearn: 0.5642917\ttotal: 22.6s\tremaining: 9.43s\n706:\tlearn: 0.5641098\ttotal: 22.6s\tremaining: 9.38s\n707:\tlearn: 0.5638853\ttotal: 22.7s\tremaining: 9.34s\n708:\tlearn: 0.5635685\ttotal: 22.7s\tremaining: 9.3s\n709:\tlearn: 0.5631007\ttotal: 22.7s\tremaining: 9.26s\n710:\tlearn: 0.5626816\ttotal: 22.7s\tremaining: 9.22s\n711:\tlearn: 0.5623672\ttotal: 22.7s\tremaining: 9.18s\n712:\tlearn: 0.5619698\ttotal: 22.7s\tremaining: 9.14s\n713:\tlearn: 0.5619053\ttotal: 22.7s\tremaining: 9.1s\n714:\tlearn: 0.5616067\ttotal: 22.7s\tremaining: 9.06s\n715:\tlearn: 0.5612477\ttotal: 22.7s\tremaining: 9.02s\n716:\tlearn: 0.5607476\ttotal: 22.7s\tremaining: 8.97s\n717:\tlearn: 0.5604447\ttotal: 22.7s\tremaining: 8.93s\n718:\tlearn: 0.5600461\ttotal: 22.8s\tremaining: 8.89s\n719:\tlearn: 0.5596341\ttotal: 22.8s\tremaining: 8.85s\n720:\tlearn: 0.5592179\ttotal: 22.8s\tremaining: 8.81s\n721:\tlearn: 0.5588705\ttotal: 22.8s\tremaining: 8.77s\n722:\tlearn: 0.5585250\ttotal: 22.8s\tremaining: 8.73s\n723:\tlearn: 0.5582162\ttotal: 22.8s\tremaining: 8.69s\n724:\tlearn: 0.5578987\ttotal: 22.8s\tremaining: 8.65s\n725:\tlearn: 0.5575292\ttotal: 22.8s\tremaining: 8.61s\n726:\tlearn: 0.5570718\ttotal: 22.8s\tremaining: 8.57s\n727:\tlearn: 0.5568688\ttotal: 22.8s\tremaining: 8.54s\n728:\tlearn: 0.5566602\ttotal: 22.9s\tremaining: 8.49s\n729:\tlearn: 0.5564512\ttotal: 22.9s\tremaining: 8.46s\n730:\tlearn: 0.5561102\ttotal: 22.9s\tremaining: 8.42s\n731:\tlearn: 0.5559802\ttotal: 22.9s\tremaining: 8.38s\n732:\tlearn: 0.5556083\ttotal: 22.9s\tremaining: 8.34s\n733:\tlearn: 0.5554282\ttotal: 22.9s\tremaining: 8.3s\n734:\tlearn: 0.5552235\ttotal: 22.9s\tremaining: 8.26s\n735:\tlearn: 0.5548008\ttotal: 22.9s\tremaining: 8.22s\n736:\tlearn: 0.5544043\ttotal: 22.9s\tremaining: 8.18s\n737:\tlearn: 0.5540926\ttotal: 22.9s\tremaining: 8.14s\n738:\tlearn: 0.5535533\ttotal: 22.9s\tremaining: 8.11s\n739:\tlearn: 0.5532460\ttotal: 23s\tremaining: 8.07s\n740:\tlearn: 0.5529850\ttotal: 23s\tremaining: 8.03s\n741:\tlearn: 0.5525981\ttotal: 23s\tremaining: 7.99s\n742:\tlearn: 0.5524198\ttotal: 23s\tremaining: 7.95s\n743:\tlearn: 0.5524013\ttotal: 23s\tremaining: 7.91s\n744:\tlearn: 0.5519517\ttotal: 23s\tremaining: 7.87s\n745:\tlearn: 0.5515926\ttotal: 23s\tremaining: 7.83s\n746:\tlearn: 0.5513971\ttotal: 23s\tremaining: 7.8s\n747:\tlearn: 0.5510739\ttotal: 23s\tremaining: 7.76s\n748:\tlearn: 0.5504297\ttotal: 23s\tremaining: 7.72s\n749:\tlearn: 0.5500986\ttotal: 23.1s\tremaining: 7.68s\n750:\tlearn: 0.5498122\ttotal: 23.1s\tremaining: 7.65s\n751:\tlearn: 0.5496548\ttotal: 23.1s\tremaining: 7.61s\n752:\tlearn: 0.5494930\ttotal: 23.1s\tremaining: 7.57s\n753:\tlearn: 0.5492858\ttotal: 23.1s\tremaining: 7.53s\n754:\tlearn: 0.5489310\ttotal: 23.1s\tremaining: 7.5s\n755:\tlearn: 0.5486269\ttotal: 23.1s\tremaining: 7.46s\n756:\tlearn: 0.5483146\ttotal: 23.1s\tremaining: 7.42s\n757:\tlearn: 0.5479780\ttotal: 23.1s\tremaining: 7.38s\n758:\tlearn: 0.5477117\ttotal: 23.1s\tremaining: 7.35s\n759:\tlearn: 0.5474018\ttotal: 23.1s\tremaining: 7.31s\n760:\tlearn: 0.5470127\ttotal: 23.2s\tremaining: 7.27s\n761:\tlearn: 0.5468016\ttotal: 23.2s\tremaining: 7.24s\n762:\tlearn: 0.5465613\ttotal: 23.2s\tremaining: 7.2s\n763:\tlearn: 0.5461888\ttotal: 23.2s\tremaining: 7.16s\n764:\tlearn: 0.5459122\ttotal: 23.2s\tremaining: 7.12s\n765:\tlearn: 0.5457036\ttotal: 23.2s\tremaining: 7.09s\n766:\tlearn: 0.5453969\ttotal: 23.2s\tremaining: 7.05s\n767:\tlearn: 0.5451707\ttotal: 23.2s\tremaining: 7.01s\n768:\tlearn: 0.5448925\ttotal: 23.2s\tremaining: 6.98s\n769:\tlearn: 0.5446983\ttotal: 23.2s\tremaining: 6.94s\n770:\tlearn: 0.5444639\ttotal: 23.2s\tremaining: 6.9s\n771:\tlearn: 0.5441277\ttotal: 23.3s\tremaining: 6.87s\n772:\tlearn: 0.5439317\ttotal: 23.3s\tremaining: 6.83s\n773:\tlearn: 0.5436243\ttotal: 23.3s\tremaining: 6.8s\n774:\tlearn: 0.5434313\ttotal: 23.3s\tremaining: 6.76s\n775:\tlearn: 0.5432140\ttotal: 23.3s\tremaining: 6.72s\n776:\tlearn: 0.5428946\ttotal: 23.3s\tremaining: 6.69s\n777:\tlearn: 0.5426695\ttotal: 23.3s\tremaining: 6.65s\n778:\tlearn: 0.5423388\ttotal: 23.3s\tremaining: 6.62s\n779:\tlearn: 0.5420298\ttotal: 23.3s\tremaining: 6.58s\n780:\tlearn: 0.5415080\ttotal: 23.3s\tremaining: 6.54s\n781:\tlearn: 0.5411156\ttotal: 23.3s\tremaining: 6.51s\n782:\tlearn: 0.5408964\ttotal: 23.4s\tremaining: 6.47s\n783:\tlearn: 0.5406627\ttotal: 23.4s\tremaining: 6.44s\n784:\tlearn: 0.5401523\ttotal: 23.4s\tremaining: 6.4s\n785:\tlearn: 0.5399028\ttotal: 23.4s\tremaining: 6.37s\n786:\tlearn: 0.5396149\ttotal: 23.4s\tremaining: 6.33s\n787:\tlearn: 0.5393427\ttotal: 23.4s\tremaining: 6.3s\n788:\tlearn: 0.5391366\ttotal: 23.4s\tremaining: 6.26s\n789:\tlearn: 0.5387140\ttotal: 23.4s\tremaining: 6.23s\n790:\tlearn: 0.5383587\ttotal: 23.4s\tremaining: 6.19s\n791:\tlearn: 0.5379933\ttotal: 23.4s\tremaining: 6.16s\n792:\tlearn: 0.5375821\ttotal: 23.5s\tremaining: 6.12s\n793:\tlearn: 0.5373792\ttotal: 23.5s\tremaining: 6.09s\n794:\tlearn: 0.5369847\ttotal: 23.5s\tremaining: 6.05s\n795:\tlearn: 0.5365671\ttotal: 23.5s\tremaining: 6.02s\n796:\tlearn: 0.5365059\ttotal: 23.5s\tremaining: 5.98s\n797:\tlearn: 0.5362847\ttotal: 23.5s\tremaining: 5.95s\n798:\tlearn: 0.5359732\ttotal: 23.5s\tremaining: 5.91s\n799:\tlearn: 0.5356868\ttotal: 23.5s\tremaining: 5.88s\n800:\tlearn: 0.5354865\ttotal: 23.5s\tremaining: 5.84s\n801:\tlearn: 0.5351205\ttotal: 23.5s\tremaining: 5.81s\n802:\tlearn: 0.5347847\ttotal: 23.5s\tremaining: 5.78s\n803:\tlearn: 0.5344998\ttotal: 23.6s\tremaining: 5.74s\n804:\tlearn: 0.5340676\ttotal: 23.6s\tremaining: 5.71s\n805:\tlearn: 0.5337716\ttotal: 23.6s\tremaining: 5.67s\n806:\tlearn: 0.5333228\ttotal: 23.6s\tremaining: 5.64s\n807:\tlearn: 0.5330893\ttotal: 23.6s\tremaining: 5.61s\n808:\tlearn: 0.5329306\ttotal: 23.6s\tremaining: 5.57s\n809:\tlearn: 0.5326607\ttotal: 23.6s\tremaining: 5.54s\n810:\tlearn: 0.5323817\ttotal: 23.6s\tremaining: 5.5s\n811:\tlearn: 0.5321655\ttotal: 23.6s\tremaining: 5.47s\n812:\tlearn: 0.5318023\ttotal: 23.6s\tremaining: 5.44s\n813:\tlearn: 0.5313748\ttotal: 23.7s\tremaining: 5.4s\n814:\tlearn: 0.5309758\ttotal: 23.7s\tremaining: 5.37s\n815:\tlearn: 0.5306941\ttotal: 23.7s\tremaining: 5.34s\n816:\tlearn: 0.5305163\ttotal: 23.7s\tremaining: 5.3s\n817:\tlearn: 0.5301971\ttotal: 23.7s\tremaining: 5.27s\n818:\tlearn: 0.5298921\ttotal: 23.7s\tremaining: 5.24s\n819:\tlearn: 0.5295407\ttotal: 23.7s\tremaining: 5.2s\n820:\tlearn: 0.5292023\ttotal: 23.7s\tremaining: 5.17s\n821:\tlearn: 0.5287747\ttotal: 23.7s\tremaining: 5.14s\n822:\tlearn: 0.5284271\ttotal: 23.7s\tremaining: 5.11s\n823:\tlearn: 0.5280580\ttotal: 23.8s\tremaining: 5.07s\n824:\tlearn: 0.5278171\ttotal: 23.8s\tremaining: 5.04s\n825:\tlearn: 0.5275279\ttotal: 23.8s\tremaining: 5.01s\n826:\tlearn: 0.5272053\ttotal: 23.8s\tremaining: 4.97s\n827:\tlearn: 0.5269904\ttotal: 23.8s\tremaining: 4.94s\n828:\tlearn: 0.5265856\ttotal: 23.8s\tremaining: 4.91s\n829:\tlearn: 0.5262917\ttotal: 23.8s\tremaining: 4.88s\n830:\tlearn: 0.5259041\ttotal: 23.8s\tremaining: 4.84s\n831:\tlearn: 0.5255409\ttotal: 23.8s\tremaining: 4.81s\n832:\tlearn: 0.5253911\ttotal: 23.8s\tremaining: 4.78s\n833:\tlearn: 0.5252129\ttotal: 23.8s\tremaining: 4.75s\n834:\tlearn: 0.5249985\ttotal: 23.9s\tremaining: 4.71s\n835:\tlearn: 0.5249239\ttotal: 23.9s\tremaining: 4.68s\n836:\tlearn: 0.5247018\ttotal: 23.9s\tremaining: 4.65s\n837:\tlearn: 0.5244997\ttotal: 23.9s\tremaining: 4.62s\n838:\tlearn: 0.5242228\ttotal: 23.9s\tremaining: 4.58s\n839:\tlearn: 0.5240792\ttotal: 23.9s\tremaining: 4.55s\n840:\tlearn: 0.5237798\ttotal: 23.9s\tremaining: 4.52s\n841:\tlearn: 0.5234574\ttotal: 23.9s\tremaining: 4.49s\n842:\tlearn: 0.5230901\ttotal: 23.9s\tremaining: 4.46s\n843:\tlearn: 0.5228931\ttotal: 23.9s\tremaining: 4.42s\n844:\tlearn: 0.5226783\ttotal: 23.9s\tremaining: 4.39s\n845:\tlearn: 0.5223470\ttotal: 24s\tremaining: 4.36s\n846:\tlearn: 0.5220165\ttotal: 24s\tremaining: 4.33s\n847:\tlearn: 0.5219071\ttotal: 24s\tremaining: 4.3s\n848:\tlearn: 0.5215786\ttotal: 24s\tremaining: 4.26s\n849:\tlearn: 0.5214292\ttotal: 24s\tremaining: 4.23s\n850:\tlearn: 0.5212262\ttotal: 24s\tremaining: 4.2s\n851:\tlearn: 0.5210140\ttotal: 24s\tremaining: 4.17s\n852:\tlearn: 0.5207526\ttotal: 24s\tremaining: 4.14s\n853:\tlearn: 0.5203979\ttotal: 24s\tremaining: 4.11s\n854:\tlearn: 0.5201886\ttotal: 24s\tremaining: 4.08s\n855:\tlearn: 0.5199166\ttotal: 24s\tremaining: 4.04s\n856:\tlearn: 0.5196257\ttotal: 24.1s\tremaining: 4.01s\n857:\tlearn: 0.5193609\ttotal: 24.1s\tremaining: 3.98s\n858:\tlearn: 0.5191557\ttotal: 24.1s\tremaining: 3.95s\n859:\tlearn: 0.5188288\ttotal: 24.1s\tremaining: 3.92s\n860:\tlearn: 0.5185686\ttotal: 24.1s\tremaining: 3.89s\n861:\tlearn: 0.5183629\ttotal: 24.1s\tremaining: 3.86s\n862:\tlearn: 0.5180079\ttotal: 24.1s\tremaining: 3.83s\n863:\tlearn: 0.5178051\ttotal: 24.1s\tremaining: 3.8s\n864:\tlearn: 0.5173896\ttotal: 24.1s\tremaining: 3.77s\n865:\tlearn: 0.5170193\ttotal: 24.1s\tremaining: 3.73s\n866:\tlearn: 0.5167952\ttotal: 24.1s\tremaining: 3.7s\n867:\tlearn: 0.5164410\ttotal: 24.2s\tremaining: 3.67s\n868:\tlearn: 0.5160659\ttotal: 24.2s\tremaining: 3.64s\n869:\tlearn: 0.5158498\ttotal: 24.2s\tremaining: 3.61s\n870:\tlearn: 0.5155653\ttotal: 24.2s\tremaining: 3.58s\n871:\tlearn: 0.5150909\ttotal: 24.2s\tremaining: 3.55s\n872:\tlearn: 0.5150060\ttotal: 24.2s\tremaining: 3.52s\n873:\tlearn: 0.5146907\ttotal: 24.2s\tremaining: 3.49s\n874:\tlearn: 0.5143136\ttotal: 24.2s\tremaining: 3.46s\n875:\tlearn: 0.5141170\ttotal: 24.2s\tremaining: 3.43s\n876:\tlearn: 0.5137954\ttotal: 24.2s\tremaining: 3.4s\n877:\tlearn: 0.5135388\ttotal: 24.3s\tremaining: 3.37s\n878:\tlearn: 0.5132911\ttotal: 24.3s\tremaining: 3.34s\n879:\tlearn: 0.5130582\ttotal: 24.3s\tremaining: 3.31s\n880:\tlearn: 0.5127622\ttotal: 24.3s\tremaining: 3.28s\n881:\tlearn: 0.5125432\ttotal: 24.3s\tremaining: 3.25s\n882:\tlearn: 0.5123889\ttotal: 24.3s\tremaining: 3.22s\n883:\tlearn: 0.5121738\ttotal: 24.3s\tremaining: 3.19s\n884:\tlearn: 0.5119055\ttotal: 24.3s\tremaining: 3.16s\n885:\tlearn: 0.5117368\ttotal: 24.3s\tremaining: 3.13s\n886:\tlearn: 0.5113967\ttotal: 24.3s\tremaining: 3.1s\n887:\tlearn: 0.5111963\ttotal: 24.3s\tremaining: 3.07s\n888:\tlearn: 0.5109172\ttotal: 24.4s\tremaining: 3.04s\n889:\tlearn: 0.5106964\ttotal: 24.4s\tremaining: 3.01s\n890:\tlearn: 0.5105530\ttotal: 24.4s\tremaining: 2.98s\n891:\tlearn: 0.5101224\ttotal: 24.4s\tremaining: 2.95s\n892:\tlearn: 0.5098537\ttotal: 24.4s\tremaining: 2.92s\n893:\tlearn: 0.5096117\ttotal: 24.4s\tremaining: 2.89s\n894:\tlearn: 0.5093120\ttotal: 24.4s\tremaining: 2.86s\n895:\tlearn: 0.5090965\ttotal: 24.4s\tremaining: 2.83s\n896:\tlearn: 0.5089710\ttotal: 24.4s\tremaining: 2.81s\n897:\tlearn: 0.5086834\ttotal: 24.4s\tremaining: 2.78s\n898:\tlearn: 0.5083765\ttotal: 24.4s\tremaining: 2.75s\n899:\tlearn: 0.5080028\ttotal: 24.5s\tremaining: 2.72s\n900:\tlearn: 0.5077837\ttotal: 24.5s\tremaining: 2.69s\n901:\tlearn: 0.5076204\ttotal: 24.5s\tremaining: 2.66s\n902:\tlearn: 0.5074468\ttotal: 24.5s\tremaining: 2.63s\n903:\tlearn: 0.5072004\ttotal: 24.5s\tremaining: 2.6s\n904:\tlearn: 0.5070360\ttotal: 24.5s\tremaining: 2.57s\n905:\tlearn: 0.5067595\ttotal: 24.5s\tremaining: 2.54s\n906:\tlearn: 0.5065320\ttotal: 24.5s\tremaining: 2.51s\n907:\tlearn: 0.5062054\ttotal: 24.5s\tremaining: 2.48s\n908:\tlearn: 0.5058668\ttotal: 24.5s\tremaining: 2.46s\n909:\tlearn: 0.5054907\ttotal: 24.5s\tremaining: 2.43s\n910:\tlearn: 0.5052077\ttotal: 24.6s\tremaining: 2.4s\n911:\tlearn: 0.5049829\ttotal: 24.6s\tremaining: 2.37s\n912:\tlearn: 0.5047691\ttotal: 24.6s\tremaining: 2.34s\n913:\tlearn: 0.5044265\ttotal: 24.6s\tremaining: 2.31s\n914:\tlearn: 0.5041511\ttotal: 24.6s\tremaining: 2.28s\n915:\tlearn: 0.5039466\ttotal: 24.6s\tremaining: 2.26s\n916:\tlearn: 0.5036040\ttotal: 24.6s\tremaining: 2.23s\n917:\tlearn: 0.5032642\ttotal: 24.6s\tremaining: 2.2s\n918:\tlearn: 0.5030076\ttotal: 24.6s\tremaining: 2.17s\n919:\tlearn: 0.5028235\ttotal: 24.6s\tremaining: 2.14s\n920:\tlearn: 0.5025941\ttotal: 24.6s\tremaining: 2.11s\n921:\tlearn: 0.5024857\ttotal: 24.7s\tremaining: 2.09s\n922:\tlearn: 0.5020563\ttotal: 24.7s\tremaining: 2.06s\n923:\tlearn: 0.5017886\ttotal: 24.7s\tremaining: 2.03s\n924:\tlearn: 0.5015748\ttotal: 24.7s\tremaining: 2s\n925:\tlearn: 0.5011627\ttotal: 24.7s\tremaining: 1.97s\n926:\tlearn: 0.5009289\ttotal: 24.7s\tremaining: 1.95s\n927:\tlearn: 0.5005968\ttotal: 24.7s\tremaining: 1.92s\n928:\tlearn: 0.5002982\ttotal: 24.7s\tremaining: 1.89s\n929:\tlearn: 0.4999409\ttotal: 24.7s\tremaining: 1.86s\n930:\tlearn: 0.4997515\ttotal: 24.7s\tremaining: 1.83s\n931:\tlearn: 0.4995581\ttotal: 24.8s\tremaining: 1.8s\n932:\tlearn: 0.4993515\ttotal: 24.8s\tremaining: 1.78s\n933:\tlearn: 0.4990698\ttotal: 24.8s\tremaining: 1.75s\n934:\tlearn: 0.4989608\ttotal: 24.8s\tremaining: 1.72s\n935:\tlearn: 0.4986775\ttotal: 24.8s\tremaining: 1.69s\n936:\tlearn: 0.4984042\ttotal: 24.8s\tremaining: 1.67s\n937:\tlearn: 0.4981542\ttotal: 24.8s\tremaining: 1.64s\n938:\tlearn: 0.4980841\ttotal: 24.8s\tremaining: 1.61s\n939:\tlearn: 0.4978516\ttotal: 24.8s\tremaining: 1.58s\n940:\tlearn: 0.4976772\ttotal: 24.8s\tremaining: 1.56s\n941:\tlearn: 0.4973915\ttotal: 24.8s\tremaining: 1.53s\n942:\tlearn: 0.4970728\ttotal: 24.9s\tremaining: 1.5s\n943:\tlearn: 0.4967933\ttotal: 24.9s\tremaining: 1.47s\n944:\tlearn: 0.4966206\ttotal: 24.9s\tremaining: 1.45s\n945:\tlearn: 0.4961165\ttotal: 24.9s\tremaining: 1.42s\n946:\tlearn: 0.4959634\ttotal: 24.9s\tremaining: 1.39s\n947:\tlearn: 0.4957463\ttotal: 24.9s\tremaining: 1.36s\n948:\tlearn: 0.4954472\ttotal: 24.9s\tremaining: 1.34s\n949:\tlearn: 0.4951299\ttotal: 24.9s\tremaining: 1.31s\n950:\tlearn: 0.4950076\ttotal: 24.9s\tremaining: 1.28s\n951:\tlearn: 0.4949296\ttotal: 24.9s\tremaining: 1.26s\n952:\tlearn: 0.4947474\ttotal: 24.9s\tremaining: 1.23s\n953:\tlearn: 0.4944934\ttotal: 24.9s\tremaining: 1.2s\n954:\tlearn: 0.4943036\ttotal: 25s\tremaining: 1.18s\n955:\tlearn: 0.4940323\ttotal: 25s\tremaining: 1.15s\n956:\tlearn: 0.4937501\ttotal: 25s\tremaining: 1.12s\n957:\tlearn: 0.4934987\ttotal: 25s\tremaining: 1.09s\n958:\tlearn: 0.4931323\ttotal: 25s\tremaining: 1.07s\n959:\tlearn: 0.4927930\ttotal: 25s\tremaining: 1.04s\n960:\tlearn: 0.4925265\ttotal: 25s\tremaining: 1.01s\n961:\tlearn: 0.4921266\ttotal: 25s\tremaining: 989ms\n962:\tlearn: 0.4918949\ttotal: 25s\tremaining: 962ms\n963:\tlearn: 0.4915450\ttotal: 25s\tremaining: 935ms\n964:\tlearn: 0.4912951\ttotal: 25.1s\tremaining: 909ms\n965:\tlearn: 0.4909782\ttotal: 25.1s\tremaining: 882ms\n966:\tlearn: 0.4908375\ttotal: 25.1s\tremaining: 856ms\n967:\tlearn: 0.4905896\ttotal: 25.1s\tremaining: 829ms\n968:\tlearn: 0.4903108\ttotal: 25.1s\tremaining: 803ms\n969:\tlearn: 0.4901254\ttotal: 25.1s\tremaining: 776ms\n970:\tlearn: 0.4898230\ttotal: 25.1s\tremaining: 750ms\n971:\tlearn: 0.4895775\ttotal: 25.1s\tremaining: 724ms\n972:\tlearn: 0.4893116\ttotal: 25.1s\tremaining: 697ms\n973:\tlearn: 0.4891540\ttotal: 25.1s\tremaining: 671ms\n974:\tlearn: 0.4888989\ttotal: 25.1s\tremaining: 645ms\n975:\tlearn: 0.4886825\ttotal: 25.2s\tremaining: 619ms\n976:\tlearn: 0.4884494\ttotal: 25.2s\tremaining: 592ms\n977:\tlearn: 0.4882688\ttotal: 25.2s\tremaining: 566ms\n978:\tlearn: 0.4880610\ttotal: 25.2s\tremaining: 540ms\n979:\tlearn: 0.4876777\ttotal: 25.2s\tremaining: 514ms\n980:\tlearn: 0.4873881\ttotal: 25.2s\tremaining: 488ms\n981:\tlearn: 0.4872105\ttotal: 25.2s\tremaining: 462ms\n982:\tlearn: 0.4869047\ttotal: 25.2s\tremaining: 436ms\n983:\tlearn: 0.4865837\ttotal: 25.2s\tremaining: 410ms\n984:\tlearn: 0.4863822\ttotal: 25.2s\tremaining: 384ms\n985:\tlearn: 0.4860443\ttotal: 25.2s\tremaining: 358ms\n986:\tlearn: 0.4858193\ttotal: 25.3s\tremaining: 333ms\n987:\tlearn: 0.4856289\ttotal: 25.3s\tremaining: 307ms\n988:\tlearn: 0.4854340\ttotal: 25.3s\tremaining: 281ms\n989:\tlearn: 0.4850914\ttotal: 25.3s\tremaining: 255ms\n990:\tlearn: 0.4850003\ttotal: 25.3s\tremaining: 230ms\n991:\tlearn: 0.4845876\ttotal: 25.3s\tremaining: 204ms\n992:\tlearn: 0.4843292\ttotal: 25.3s\tremaining: 178ms\n993:\tlearn: 0.4839680\ttotal: 25.3s\tremaining: 153ms\n994:\tlearn: 0.4837390\ttotal: 25.3s\tremaining: 127ms\n995:\tlearn: 0.4834783\ttotal: 25.3s\tremaining: 102ms\n996:\tlearn: 0.4832424\ttotal: 25.3s\tremaining: 76.3ms\n997:\tlearn: 0.4830412\ttotal: 25.4s\tremaining: 50.8ms\n998:\tlearn: 0.4827251\ttotal: 25.4s\tremaining: 25.4ms\n999:\tlearn: 0.4824255\ttotal: 25.4s\tremaining: 0us\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<catboost.core.CatBoostClassifier at 0x7c8f9af12920>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"test_pred = clf.predict(test_embeds)\ntest_proba = clf.predict_proba(test_embeds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:41:48.004345Z","iopub.execute_input":"2025-05-10T14:41:48.004735Z","iopub.status.idle":"2025-05-10T14:41:48.057027Z","shell.execute_reply.started":"2025-05-10T14:41:48.004701Z","shell.execute_reply":"2025-05-10T14:41:48.056326Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"print(\"Accuracy:\", accuracy_score(target_test, test_pred))\nprint(\"ROC-AUC:\", roc_auc_score(target_test, test_proba, average=\"weighted\", multi_class=\"ovr\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:41:49.302349Z","iopub.execute_input":"2025-05-10T14:41:49.303206Z","iopub.status.idle":"2025-05-10T14:41:49.324348Z","shell.execute_reply.started":"2025-05-10T14:41:49.303164Z","shell.execute_reply":"2025-05-10T14:41:49.323203Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.5866666666666667\nROC-AUC: 0.8418938072777914\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"arr = np.array([0.8418938072777914, 0.8433480466040607, 0.8435011133618263])\n\narr.mean(), arr.std()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T16:17:26.085461Z","iopub.execute_input":"2025-05-10T16:17:26.085855Z","iopub.status.idle":"2025-05-10T16:17:26.093501Z","shell.execute_reply.started":"2025-05-10T16:17:26.085822Z","shell.execute_reply":"2025-05-10T16:17:26.092354Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(0.8429143224145594, 0.0007243137977252656)"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"- COLES embeds + Catboost:\n  - `Accuracy: 0.6133333333333333`, `0.606`, `0.5933333333333334`, avg: `0.6042 +- 0.0083`\n  -  `ROC-AUC: 0.8490542004456147`, `0.848260886697585`, `0.8472952867923927`, avg: `0.8482 +- 0.0007`\n\n---\n\n<!-- - COLES embeds w/ SWIN_Agg seq_enc + Catboost:\n  - Accuracy: `0.593`, `0.5906666666666667`, `0.5983333333333334`, avg: `0.594 +- 0.0032`\n  - ROC-AUC: `0.8430090651959065`, `0.8425861754433439`, `0.8393266734968265`, avg: `0.8416 +- 0.0016` -->\n\n- COLES embeds w/ SWIN_Agg seq_enc + Catboost:\n  - Accuracy: `0.5993333333333334`, `0.5916666666666667`, `0.5993333333333334`, avg: `0.5968 +- 0.0036`\n  - ROC-AUC: `0.8435234374762616`, `0.8419468094194725`, `0.8457581938901253`, avg: `0.8437 +- 0.0016`\n\n---\n\n- COLES embeds w/ SWIN_Agg seq_enc & ConvAgg (3 trx) + Catboost:\n  - Accuracy: `0.5866666666666667`, `0.5933333333333334`, `0.598`, avg: `0.5927 +- 0.0047`\n  - ROC-AUC: `0.8418938072777914`, `0.8433480466040607`, `0.8435011133618263`, avg: `0.8429 +- 0.0007`\n\n---\n\n<!-- - COLES embeds w/ SWIN_Agg seq_enc + ConvAgg (3 trx) + Catboost:\n  - Accuracy: `0.5903333333333334`, `0.5883333333333334`, `0.5846666666666667`, avg: `0.5878 +- 0.0023`\n  - ROC-AUC: `0.8423263115862589`, `0.8404208025778663`, `0.8384572507278364`, avg: `0.8404 +- 0.0016`\n\n--- -->\n\n**Вывод:** для CoLES качество при замене RNN энкодера на SWIN энкодер значительно ухудшается, как по accuracy, так и по ROC-AUC. При добавлении свёрточной агрегации качество становится ещё хуже.\n\n**Конфигурация, лучшая по метрикам:**\n\n- COLES embeds w/ SWIN_Agg seq_enc + Catboost:\n  - Accuracy: `0.5993333333333334`, `0.5916666666666667`, `0.5993333333333334`, avg: `0.5968 +- 0.0036`\n  - ROC-AUC: `0.8435234374762616`, `0.8419468094194725`, `0.8457581938901253`, avg: `0.8437 +- 0.0016`","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Train sequences lengths check:**","metadata":{}},{"cell_type":"code","source":"agg_encoder_params = dict(\n    embeddings_noise=0.003,\n    numeric_values={\"amount_rur\": \"log\"},\n    embeddings={\n        \"trans_date\": {\"in\": 800, \"out\": 16},\n        \"small_group\": {\"in\": 250, \"out\": 16},\n    },\n    linear_projection_size=260\n)\n\ntrx_encoder = TrxEncoder(**agg_encoder_params)\ntrx_encoder.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T09:22:38.521695Z","iopub.execute_input":"2025-05-06T09:22:38.522165Z","iopub.status.idle":"2025-05-06T09:22:38.828703Z","shell.execute_reply.started":"2025-05-06T09:22:38.522119Z","shell.execute_reply":"2025-05-06T09:22:38.827955Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrxEncoder(\n  (embeddings): ModuleDict(\n    (trans_date): NoisyEmbedding(\n      800, 16, padding_idx=0\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (small_group): NoisyEmbedding(\n      250, 16, padding_idx=0\n      (dropout): Dropout(p=0, inplace=False)\n    )\n  )\n  (custom_embeddings): ModuleDict(\n    (amount_rur): LogScaler()\n  )\n  (custom_embedding_batch_norm): RBatchNorm(\n    (bn): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (linear_projection_head): Linear(in_features=33, out_features=260, bias=True)\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"train_loader = inference_data_loader(data_train, num_workers=0, batch_size=128)\n\ntrx_encoder.eval()\n\nseq_lens = []\n\nfor batch in tqdm(train_loader):\n    embeds_batch = trx_encoder(batch.to(\"cuda\"))\n    seq_lens += [embeds_batch.seq_lens.detach().cpu().numpy()]\n\nseq_lens = np.concatenate(seq_lens)\n\nthreshold = int(np.quantile(seq_lens, 0.6))\n\nprint(\"Max Length:\", threshold)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T09:23:04.912705Z","iopub.execute_input":"2025-05-06T09:23:04.913009Z","iopub.status.idle":"2025-05-06T09:23:06.802109Z","shell.execute_reply.started":"2025-05-06T09:23:04.912979Z","shell.execute_reply":"2025-05-06T09:23:06.801179Z"}},"outputs":[{"name":"stderr","text":"211it [00:01, 112.30it/s]","output_type":"stream"},{"name":"stdout","text":"Max Length: 863\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"- **CPC modeling:**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**Скорректируем класс CpcModule так, чтобы при работе CPC не было даталиков:**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\nfrom ptls.data_load.padded_batch import PaddedBatch\n\n\nclass CPC_ShiftedLoss(nn.Module):\n    def __init__(self, n_negatives=None, n_forward_steps=None, shift=0):\n        super().__init__()\n        self.n_negatives = n_negatives\n        self.n_forward_steps = n_forward_steps\n        self.shift = shift\n\n    def _get_preds(self, base_embeddings, mapped_ctx_embeddings):\n        batch_size, max_seq_len, emb_size = base_embeddings.payload.shape\n        _, _, _, n_forward_steps = mapped_ctx_embeddings.payload.shape\n        seq_lens = mapped_ctx_embeddings.seq_lens\n        device = mapped_ctx_embeddings.payload.device\n\n        # num_additional_samples = mapped_ctx_embeddings.payload.shape[1] - max_seq_len\n        # if num_additional_samples > 0:\n        #     additional_samples = torch.zeros((batch_size, num_additional_samples, emb_size), device=device)\n        #     base_embeddings = PaddedBatch(torch.cat((base_embeddings.payload, additional_samples), dim=1), base_embeddings.seq_lens)\n        #     max_seq_len += num_additional_samples               \n        \n        #mapped_ctx_embeddings = mapped_ctx_embeddings.payload\n            \n        mapped_ctx_embeddings = mapped_ctx_embeddings.payload[:, :max_seq_len, :, :]\n\n        len_mask = torch.arange(max_seq_len).unsqueeze(0).expand(batch_size, -1).to(device)\n        len_mask = (len_mask < seq_lens.unsqueeze(1).expand(-1, max_seq_len)).float()\n        \n        possible_negatives = base_embeddings.payload.reshape(batch_size * max_seq_len, emb_size)\n\n        mask = len_mask.unsqueeze(0).expand(batch_size, *len_mask.shape).clone()\n\n        mask = mask.reshape(batch_size, -1)\n        sample_ids = torch.multinomial(mask, self.n_negatives)\n        neg_samples = possible_negatives[sample_ids]\n\n        positive_preds, neg_preds = [], []\n        len_mask_exp = len_mask.unsqueeze(-1).unsqueeze(-1).to(device).expand(-1, -1, emb_size, n_forward_steps)\n        trimmed_mce = mapped_ctx_embeddings.mul(len_mask_exp)  # zero context vectors by sequence lengths\n        for i in range(1, n_forward_steps + 1):\n            ce_i = trimmed_mce[:, 0:(max_seq_len - i - self.shift), :, i - 1]\n            be_i = base_embeddings.payload[:, (i + self.shift):max_seq_len]\n\n            positive_pred_i = ce_i.mul(be_i).sum(axis=-1)\n            positive_preds.append(positive_pred_i)\n\n            neg_pred_i = ce_i.matmul(neg_samples.transpose(-2, -1))\n            neg_preds.append(neg_pred_i)\n\n        return positive_preds, neg_preds\n\n    def forward(self, embeddings, _):\n        base_embeddings, _, mapped_ctx_embeddings = embeddings\n        device = mapped_ctx_embeddings.payload.device\n        positive_preds, neg_preds = self._get_preds(base_embeddings, mapped_ctx_embeddings)\n\n        step_losses = []\n        for positive_pred_i, neg_pred_i in zip(positive_preds, neg_preds):\n            step_loss = -F.log_softmax(torch.cat([positive_pred_i.unsqueeze(-1), neg_pred_i], dim=-1), dim=-1)[:, :, 0].mean()\n            step_losses.append(step_loss)\n\n        loss = torch.stack(step_losses).mean()\n        return loss\n\n    def cpc_accuracy(self, embeddings, _):\n        base_embeddings, _, mapped_ctx_embeddings = embeddings\n        positive_preds, neg_preds = self._get_preds(base_embeddings, mapped_ctx_embeddings)\n\n        batch_size, max_seq_len, emb_size = base_embeddings.payload.shape\n        #max_seq_len = mapped_ctx_embeddings.payload.shape[1]\n        seq_lens = mapped_ctx_embeddings.seq_lens\n        device = mapped_ctx_embeddings.payload.device\n\n        len_mask = torch.arange(max_seq_len).unsqueeze(0).expand(batch_size, -1).to(device)\n        len_mask = (len_mask < seq_lens.unsqueeze(1).expand(-1, max_seq_len)).float()\n\n        total, accurate = 0, 0\n        \n        for i, (positive_pred_i, neg_pred_i) in enumerate(zip(positive_preds, neg_preds)):\n            i_mask = len_mask[:, (self.shift + i + 1):max_seq_len].to(device)\n            total += i_mask.sum().item()\n            accurate += (((positive_pred_i.unsqueeze(-1).expand(*neg_pred_i.shape) > neg_pred_i) \\\n                          .sum(dim=-1) == self.n_negatives) * i_mask).sum().item()\n        return accurate / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:55:04.125946Z","iopub.execute_input":"2025-05-11T10:55:04.126623Z","iopub.status.idle":"2025-05-11T10:55:04.142993Z","shell.execute_reply.started":"2025-05-11T10:55:04.126589Z","shell.execute_reply":"2025-05-11T10:55:04.142022Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import torch\n\nfrom ptls.frames.abs_module import ABSModule\nfrom ptls.frames.cpc.metrics.cpc_accuracy import CpcAccuracy\nfrom ptls.nn.seq_encoder import RnnSeqEncoder\nfrom ptls.data_load.padded_batch import PaddedBatch\n\n\nclass CpcModule(ABSModule):\n    \"\"\"Contrastive Predictive Coding ([CPC](https://arxiv.org/abs/1807.03748))\n\n    Original sequence are encoded by `TrxEncoder`.\n    Hidden representation `z` is an embedding for each individual transaction.\n    Next `RnnEncoder` used for `context` calculation from `z`.\n    Linear predictors are used to predict next trx embedding by context.\n    The loss function tends to make future trx embedding and they predict closer.\n    Negative sampling are used to avoid trivial solution.\n\n    Parameters\n        seq_encoder:\n            Model which calculate embeddings for original raw transaction sequences\n            `seq_encoder` is trained by `CoLESModule` to get better representations of input sequences\n        head:\n            Not used\n        loss:\n            Keep None. CPCLoss used by default\n        validation_metric:\n            Keep None. CPCAccuracy used by default\n        optimizer_partial:\n            optimizer init partial. Network parameters are missed.\n        lr_scheduler_partial:\n            scheduler init partial. Optimizer are missed.\n\n    \"\"\"\n    def __init__(self, validation_metric=None,\n                       seq_encoder=None,\n                       head=None,\n                       n_negatives=40, n_forward_steps=6, shift='none',\n                       optimizer_partial=None,\n                       lr_scheduler_partial=None):\n\n        self.save_hyperparameters('n_negatives', 'n_forward_steps')\n\n        if shift == 'add':\n            loss = CPC_ShiftedLoss(n_negatives=n_negatives, n_forward_steps=n_forward_steps, shift=(seq_encoder.trx_encoder.agg_samples - 1))\n        else:\n            loss = CPC_ShiftedLoss(n_negatives=n_negatives, n_forward_steps=n_forward_steps, shift=0)\n\n        if validation_metric is None:\n            validation_metric = CpcAccuracy(loss)\n\n        seq_encoder.seq_encoder.is_reduce_sequence = False\n\n        super().__init__(validation_metric,\n                         seq_encoder,\n                         loss,\n                         optimizer_partial,\n                         lr_scheduler_partial)\n\n        linear_size = self.seq_encoder.trx_encoder.output_size\n        embedding_size = self.seq_encoder.embedding_size\n        self._linears = torch.nn.ModuleList([torch.nn.Linear(embedding_size, linear_size)\n                                             for _ in range(loss.n_forward_steps)])\n\n    @property\n    def metric_name(self):\n        return 'cpc_accuracy'\n\n    @property\n    def is_requires_reduced_sequence(self):\n        return False\n\n    def shared_step(self, x, y):\n        trx_encoder = self._seq_encoder.trx_encoder\n        swin_fusion = self._seq_encoder.swin_fusion\n        seq_encoder = self._seq_encoder.seq_encoder\n\n        base_embeddings = trx_encoder(x)\n        context_embeddings = seq_encoder(swin_fusion(base_embeddings))\n        \n        me = []\n        for l in self._linears:\n            me.append(l(context_embeddings.payload))\n        mapped_ctx_embeddings = PaddedBatch(torch.stack(me, dim=3), context_embeddings.seq_lens)\n\n        return (base_embeddings, context_embeddings, mapped_ctx_embeddings), y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:55:07.484250Z","iopub.execute_input":"2025-05-11T10:55:07.484592Z","iopub.status.idle":"2025-05-11T10:55:07.494209Z","shell.execute_reply.started":"2025-05-11T10:55:07.484563Z","shell.execute_reply":"2025-05-11T10:55:07.493320Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"seed_everything(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:55:11.872930Z","iopub.execute_input":"2025-05-11T10:55:11.873273Z","iopub.status.idle":"2025-05-11T10:55:11.883728Z","shell.execute_reply.started":"2025-05-11T10:55:11.873243Z","shell.execute_reply":"2025-05-11T10:55:11.882914Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"**DataLoaders:**","metadata":{}},{"cell_type":"code","source":"data = PtlsDataModule(\n    train_data=CpcDataset(\n        MemoryMapDataset(data=data_train),\n        min_len=863,\n        max_len=904\n    ),\n    train_num_workers=4,\n    train_batch_size=32,\n    valid_data=CpcDataset(\n        MemoryMapDataset(data=data_test),\n        min_len=863,\n        max_len=904\n    ),\n    valid_num_workers=4,\n    valid_batch_size=32\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:55:16.917737Z","iopub.execute_input":"2025-05-11T10:55:16.918582Z","iopub.status.idle":"2025-05-11T10:55:16.924745Z","shell.execute_reply.started":"2025-05-11T10:55:16.918546Z","shell.execute_reply":"2025-05-11T10:55:16.923763Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"**Модель:**","metadata":{}},{"cell_type":"code","source":"N_EPOCHS = 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:55:18.839149Z","iopub.execute_input":"2025-05-11T10:55:18.839851Z","iopub.status.idle":"2025-05-11T10:55:18.843516Z","shell.execute_reply.started":"2025-05-11T10:55:18.839810Z","shell.execute_reply":"2025-05-11T10:55:18.842599Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"trx_encoder_params = dict(\n    embeddings_noise=0.003,\n    numeric_values={\"amount_rur\": \"log\"},\n    embeddings={\n        \"trans_date\": {\"in\": 800, \"out\": 128},\n        \"small_group\": {\"in\": 250, \"out\": 128},\n    },\n    linear_projection_size=272,\n    agg_samples=3, # 3, 5, 7\n    use_window_attention=False\n)\n\n#trx_encoder = TrxEncoder(**trx_encoder_params)\ntrx_encoder = ConvAggregator(**trx_encoder_params)\n\nseq_encoder = SWIN_RNN_SeqEncoder(\n    trx_encoder=trx_encoder,\n    swin_depths=[2, 2, 6, 2],\n    swin_num_heads=[2, 4, 8, 16],\n    swin_start_window_size=4,\n    swin_window_size_mult=2,\n    swin_drop=0.1,\n    swin_attn_drop=0.1,\n    swin_drop_path=0.1,\n    swin_decoder=True,\n    swin_start_end_fusion=False,\n    hidden_size=512,\n    type=\"gru\"\n)\n\ncpc = CpcModule(\n    seq_encoder=seq_encoder,\n    n_forward_steps=6,\n    n_negatives=40,\n    shift='add', # 'none' / 'add'\n    optimizer_partial=partial(torch.optim.Adam, lr=1e-3),\n    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=5, gamma=0.5)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:55:53.087228Z","iopub.execute_input":"2025-05-11T10:55:53.087959Z","iopub.status.idle":"2025-05-11T10:55:53.302704Z","shell.execute_reply.started":"2025-05-11T10:55:53.087923Z","shell.execute_reply":"2025-05-11T10:55:53.301846Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"**Обучение:**","metadata":{}},{"cell_type":"code","source":"logger = CometLogger(project_name=\"EvS_SSL\", experiment_name=\"CPC_modeling_SWIN_agg (w/ conv_agg, 3trx)\")\n\ntrainer = pl.Trainer(\n    logger=logger,\n    max_epochs=N_EPOCHS,\n    accelerator=\"gpu\",\n    devices=1,\n    enable_progress_bar=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:55:59.271518Z","iopub.execute_input":"2025-05-11T10:55:59.272493Z","iopub.status.idle":"2025-05-11T10:55:59.372221Z","shell.execute_reply.started":"2025-05-11T10:55:59.272455Z","shell.execute_reply":"2025-05-11T10:55:59.371552Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"trainer.fit(cpc, data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T10:56:00.714621Z","iopub.execute_input":"2025-05-11T10:56:00.715307Z","iopub.status.idle":"2025-05-11T14:32:50.028132Z","shell.execute_reply.started":"2025-05-11T10:56:00.715271Z","shell.execute_reply":"2025-05-11T14:32:50.027351Z"}},"outputs":[{"name":"stderr","text":"\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/askoro/evs-ssl/27d0b7749e7948ad9679c03c5258e06c\n\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/kaggle/working' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c612c09ef0ce4e56bbc7e96ed33ca40c"}},"metadata":{}},{"name":"stderr","text":"\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Error retrieving Conda package as an explicit file\n\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Command '['conda', 'list', '--explicit', '--md5']' returned non-zero exit status 109.\nThe following arguments were not expected: --md5 --explicit\nRun with --help for more information.\n\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : CPC_modeling_SWIN_agg (w/ conv_agg, 3trx)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/askoro/evs-ssl/27d0b7749e7948ad9679c03c5258e06c\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [2025]             : (0.23935692012310028, 4.475934982299805)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seq_len [337]           : (799.71875, 857.46875)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid/cpc_accuracy [20] : (0.9094287157058716, 0.9480034708976746)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : CPC_modeling_SWIN_agg (w/ conv_agg, 3trx)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_batch_size   : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_drop_last    : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_num_workers  : None\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_batch_size  : 32\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_drop_last   : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_num_workers : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid_batch_size  : 32\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid_drop_last   : False\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     valid_num_workers : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"trainer.logged_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:37:03.862310Z","iopub.execute_input":"2025-05-11T14:37:03.863120Z","iopub.status.idle":"2025-05-11T14:37:03.876735Z","shell.execute_reply.started":"2025-05-11T14:37:03.863084Z","shell.execute_reply":"2025-05-11T14:37:03.875875Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'loss': tensor(0.4193),\n 'seq_len': tensor(828.7084),\n 'valid/cpc_accuracy': tensor(0.9478)}"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"#torch.save(seq_encoder.state_dict(), \"cpc_enc_win_agg_trx20.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T12:53:51.620847Z","iopub.execute_input":"2025-05-06T12:53:51.621143Z","iopub.status.idle":"2025-05-06T12:53:51.701233Z","shell.execute_reply.started":"2025-05-06T12:53:51.621120Z","shell.execute_reply":"2025-05-06T12:53:51.700591Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"**Измерим качество на тесте (catboost поверх эмбеддингов):**","metadata":{}},{"cell_type":"code","source":"# !gdown \"https://drive.google.com/uc?export=download&id=1iuJJfsZpvco2VAgEMxnLlv8LW9cAcUEY\" -O \"cpc_enc_win_agg_trx20.pt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T18:03:33.795273Z","iopub.execute_input":"2025-05-06T18:03:33.795598Z","iopub.status.idle":"2025-05-06T18:03:35.691343Z","shell.execute_reply.started":"2025-05-06T18:03:33.795575Z","shell.execute_reply":"2025-05-06T18:03:35.690487Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?export=download&id=1iuJJfsZpvco2VAgEMxnLlv8LW9cAcUEY\nFrom (redirected): https://drive.google.com/uc?export=download&id=1iuJJfsZpvco2VAgEMxnLlv8LW9cAcUEY&confirm=t&uuid=c3968f85-d4c1-450a-a997-c3c531e5d2fc\nTo: /kaggle/working/cpc_enc_win_agg_trx20.pt\n100%|██████████████████████████████████████| 44.8M/44.8M [00:00<00:00, 75.6MB/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#state_dict = torch.load(\"./cpc_enc_win_agg_trx20.pt\", weights_only=False)\n\n#seq_encoder.load_state_dict(state_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T18:06:26.011983Z","iopub.execute_input":"2025-05-06T18:06:26.012280Z","iopub.status.idle":"2025-05-06T18:06:26.316666Z","shell.execute_reply.started":"2025-05-06T18:06:26.012260Z","shell.execute_reply":"2025-05-06T18:06:26.315768Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"import gc\n\ngc.collect()\n\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:37:18.402498Z","iopub.execute_input":"2025-05-11T14:37:18.402860Z","iopub.status.idle":"2025-05-11T14:37:18.821428Z","shell.execute_reply.started":"2025-05-11T14:37:18.402827Z","shell.execute_reply":"2025-05-11T14:37:18.820559Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"encoder = cpc.seq_encoder\n\ndevice = \"cuda:0\"\n\nencoder.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:37:15.742555Z","iopub.execute_input":"2025-05-11T14:37:15.743252Z","iopub.status.idle":"2025-05-11T14:37:15.775817Z","shell.execute_reply.started":"2025-05-11T14:37:15.743215Z","shell.execute_reply":"2025-05-11T14:37:15.774845Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"SWIN_RNN_SeqEncoder(\n  (trx_encoder): ConvAggregator(\n    (embeddings): ModuleDict(\n      (trans_date): NoisyEmbedding(\n        800, 128, padding_idx=0\n        (dropout): Dropout(p=0, inplace=False)\n      )\n      (small_group): NoisyEmbedding(\n        250, 128, padding_idx=0\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (custom_embeddings): ModuleDict(\n      (amount_rur): LogScaler()\n    )\n    (linear_projection_head): Linear(in_features=257, out_features=272, bias=True)\n    (conv): Conv1d(272, 272, kernel_size=(3,), stride=(1,), padding=(2,), bias=False)\n  )\n  (seq_encoder): RnnEncoder(\n    (rnn): GRU(272, 512, batch_first=True)\n    (reducer): LastStepEncoder()\n  )\n  (swin_fusion): SwinTransformerBackbone(\n    (backbone): ModuleList(\n      (0): SwinTransformerLayer(\n        dim=272, depth=2, num_heads=2, window_size=4\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=272, num_heads=2, window_size=4, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=272, window_size=4, num_heads=2\n              (qkv): Linear(in_features=272, out_features=816, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=272, out_features=272, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=272, out_features=1088, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1088, out_features=272, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=272, num_heads=2, window_size=4, shift_size=2, mlp_ratio=4.0\n            (norm1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=272, window_size=4, num_heads=2\n              (qkv): Linear(in_features=272, out_features=816, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=272, out_features=272, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=272, out_features=1088, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1088, out_features=272, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (1): SwinTransformerLayer(\n        dim=272, depth=2, num_heads=4, window_size=8\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=272, num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=272, window_size=8, num_heads=4\n              (qkv): Linear(in_features=272, out_features=816, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=272, out_features=272, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=272, out_features=1088, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1088, out_features=272, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=272, num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0\n            (norm1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=272, window_size=8, num_heads=4\n              (qkv): Linear(in_features=272, out_features=816, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=272, out_features=272, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=272, out_features=1088, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1088, out_features=272, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (2): SwinTransformerLayer(\n        dim=272, depth=6, num_heads=8, window_size=16\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=272, num_heads=8, window_size=16, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=272, window_size=16, num_heads=8\n              (qkv): Linear(in_features=272, out_features=816, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=272, out_features=272, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=272, out_features=1088, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1088, out_features=272, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=272, num_heads=8, window_size=16, shift_size=8, mlp_ratio=4.0\n            (norm1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=272, window_size=16, num_heads=8\n              (qkv): Linear(in_features=272, out_features=816, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=272, out_features=272, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=272, out_features=1088, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1088, out_features=272, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): SwinTransformerBlock(\n            dim=272, num_heads=8, window_size=16, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=272, window_size=16, num_heads=8\n              (qkv): Linear(in_features=272, out_features=816, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=272, out_features=272, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=272, out_features=1088, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1088, out_features=272, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): SwinTransformerBlock(\n            dim=272, num_heads=8, window_size=16, shift_size=8, mlp_ratio=4.0\n            (norm1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=272, window_size=16, num_heads=8\n              (qkv): Linear(in_features=272, out_features=816, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=272, out_features=272, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=272, out_features=1088, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1088, out_features=272, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): SwinTransformerBlock(\n            dim=272, num_heads=8, window_size=16, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=272, window_size=16, num_heads=8\n              (qkv): Linear(in_features=272, out_features=816, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=272, out_features=272, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=272, out_features=1088, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1088, out_features=272, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): SwinTransformerBlock(\n            dim=272, num_heads=8, window_size=16, shift_size=8, mlp_ratio=4.0\n            (norm1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=272, window_size=16, num_heads=8\n              (qkv): Linear(in_features=272, out_features=816, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=272, out_features=272, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=272, out_features=1088, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1088, out_features=272, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (3): SwinTransformerLayer(\n        dim=272, depth=2, num_heads=16, window_size=32\n        (blocks): ModuleList(\n          (0): SwinTransformerBlock(\n            dim=272, num_heads=16, window_size=32, shift_size=0, mlp_ratio=4.0\n            (norm1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=272, window_size=32, num_heads=16\n              (qkv): Linear(in_features=272, out_features=816, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=272, out_features=272, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=272, out_features=1088, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1088, out_features=272, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): SwinTransformerBlock(\n            dim=272, num_heads=16, window_size=32, shift_size=16, mlp_ratio=4.0\n            (norm1): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (attn): WindowAttention(\n              dim=272, window_size=32, num_heads=16\n              (qkv): Linear(in_features=272, out_features=816, bias=True)\n              (attn_drop): Dropout(p=0.1, inplace=False)\n              (proj): Linear(in_features=272, out_features=272, bias=True)\n              (proj_drop): Dropout(p=0.1, inplace=False)\n              (softmax): Softmax(dim=-1)\n            )\n            (drop_path): DropPath()\n            (norm2): LayerNorm((272,), eps=1e-05, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=272, out_features=1088, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1088, out_features=272, bias=True)\n              (drop): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"encoder.seq_encoder.is_reduce_sequence = True\nchange_to_enc(encoder.swin_fusion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:37:22.088950Z","iopub.execute_input":"2025-05-11T14:37:22.089591Z","iopub.status.idle":"2025-05-11T14:37:22.093346Z","shell.execute_reply.started":"2025-05-11T14:37:22.089557Z","shell.execute_reply":"2025-05-11T14:37:22.092501Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# change_to_dec(encoder.swin_fusion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T21:12:25.789690Z","iopub.execute_input":"2025-05-10T21:12:25.790571Z","iopub.status.idle":"2025-05-10T21:12:25.794886Z","shell.execute_reply.started":"2025-05-10T21:12:25.790534Z","shell.execute_reply":"2025-05-10T21:12:25.793840Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"from tqdm import tqdm\n\nseed_everything(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:55:53.699514Z","iopub.execute_input":"2025-05-11T14:55:53.700440Z","iopub.status.idle":"2025-05-11T14:55:53.705544Z","shell.execute_reply.started":"2025-05-11T14:55:53.700403Z","shell.execute_reply":"2025-05-11T14:55:53.704710Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"train_loader = inference_data_loader(data_train, num_workers=0, batch_size=16)\nencoder.eval()\ntrain_embeds = None\n\nfor i, batch in tqdm(enumerate(train_loader)):\n    train_embeds_batch = encoder(batch.to(device))\n    if i == 0:\n        train_embeds = train_embeds_batch.detach().cpu().numpy()\n    else:\n        train_embeds = np.concatenate([train_embeds, train_embeds_batch.detach().cpu().numpy()], axis=0)\n    \ntrain_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T14:56:17.851887Z","iopub.execute_input":"2025-05-11T14:56:17.852267Z","iopub.status.idle":"2025-05-11T15:00:07.734016Z","shell.execute_reply.started":"2025-05-11T14:56:17.852237Z","shell.execute_reply":"2025-05-11T15:00:07.733116Z"}},"outputs":[{"name":"stderr","text":"1688it [03:49,  7.34it/s]\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"array([[ 0.50206935,  0.5502154 ,  0.21881583, ..., -0.47690132,\n        -0.00952161,  0.17128213],\n       [ 0.58404934,  0.5412656 ,  0.28523195, ..., -0.6154343 ,\n         0.42086115,  0.56868637],\n       [-0.08817012,  0.6637562 , -0.27675313, ..., -0.5988691 ,\n         0.18293908, -0.01829041],\n       ...,\n       [ 0.22068627,  0.2929967 , -0.5602638 , ..., -0.45553845,\n         0.02662687,  0.20506336],\n       [ 0.561322  , -0.01245022,  0.1761291 , ..., -0.5325867 ,\n         0.12356527, -0.00991007],\n       [ 0.27731666,  0.34375715,  0.00902353, ..., -0.708862  ,\n         0.15119435,  0.33666897]], dtype=float32)"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"test_loader = inference_data_loader(data_test, num_workers=0, batch_size=16)\nencoder.eval()\ntest_embeds = None\n\nfor i, batch in tqdm(enumerate(test_loader)):\n    test_embeds_batch = encoder(batch.to(device))\n    if i == 0:\n        test_embeds = test_embeds_batch.detach().cpu().numpy()\n    else:\n        test_embeds = np.concatenate([test_embeds, test_embeds_batch.detach().cpu().numpy()], axis=0)\n    \ntest_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:00:11.699069Z","iopub.execute_input":"2025-05-11T15:00:11.699426Z","iopub.status.idle":"2025-05-11T15:00:35.653170Z","shell.execute_reply.started":"2025-05-11T15:00:11.699394Z","shell.execute_reply":"2025-05-11T15:00:35.652194Z"}},"outputs":[{"name":"stderr","text":"188it [00:23,  7.85it/s]\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"array([[ 0.4124494 ,  0.16320945, -0.06075403, ..., -0.7835697 ,\n         0.23924844,  0.18599585],\n       [ 0.48923135,  0.5581738 , -0.06504323, ..., -0.7392166 ,\n         0.21464413,  0.11534584],\n       [ 0.46545008,  0.14906222, -0.07318704, ..., -0.46150145,\n         0.26937625,  0.15991728],\n       ...,\n       [ 0.4191622 ,  0.11884288,  0.10891388, ..., -0.7017476 ,\n         0.04949887,  0.27504486],\n       [ 0.25224152,  0.28723678, -0.0979318 , ..., -0.6043854 ,\n         0.5650005 ,  0.22921675],\n       [ 0.1502997 ,  0.03001654, -0.00657668, ..., -0.5688363 ,\n         0.5408161 ,  0.42914337]], dtype=float32)"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"clf = CatBoostClassifier(loss_function='MultiClass', task_type=\"GPU\", devices='0', random_state=0)\n\nclf.fit(train_embeds, target_train, plot_file=\"catboost_log.html\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:00:37.701315Z","iopub.execute_input":"2025-05-11T15:00:37.702004Z","iopub.status.idle":"2025-05-11T15:01:07.248047Z","shell.execute_reply.started":"2025-05-11T15:00:37.701967Z","shell.execute_reply":"2025-05-11T15:01:07.247112Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Warning: less than 75% GPU memory available for training. Free: 2709.125 Total: 16269.25\n","output_type":"stream"},{"name":"stdout","text":"Learning rate set to 0.12714\n0:\tlearn: 1.3084508\ttotal: 17s\tremaining: 4h 42m 33s\n1:\tlearn: 1.2512947\ttotal: 17s\tremaining: 2h 21m 13s\n2:\tlearn: 1.2066016\ttotal: 17s\tremaining: 1h 34m 6s\n3:\tlearn: 1.1698312\ttotal: 17s\tremaining: 1h 10m 33s\n4:\tlearn: 1.1421062\ttotal: 17s\tremaining: 56m 25s\n5:\tlearn: 1.1173501\ttotal: 17s\tremaining: 46m 59s\n6:\tlearn: 1.0959187\ttotal: 17s\tremaining: 40m 16s\n7:\tlearn: 1.0780401\ttotal: 17s\tremaining: 35m 13s\n8:\tlearn: 1.0628451\ttotal: 17.1s\tremaining: 31m 17s\n9:\tlearn: 1.0501766\ttotal: 17.1s\tremaining: 28m 9s\n10:\tlearn: 1.0388875\ttotal: 17.1s\tremaining: 25m 35s\n11:\tlearn: 1.0278922\ttotal: 17.1s\tremaining: 23m 26s\n12:\tlearn: 1.0184868\ttotal: 17.1s\tremaining: 21m 37s\n13:\tlearn: 1.0106912\ttotal: 17.1s\tremaining: 20m 4s\n14:\tlearn: 1.0037609\ttotal: 17.1s\tremaining: 18m 43s\n15:\tlearn: 0.9979188\ttotal: 17.1s\tremaining: 17m 33s\n16:\tlearn: 0.9923271\ttotal: 17.1s\tremaining: 16m 30s\n17:\tlearn: 0.9873362\ttotal: 17.1s\tremaining: 15m 35s\n18:\tlearn: 0.9830680\ttotal: 17.2s\tremaining: 14m 45s\n19:\tlearn: 0.9791662\ttotal: 17.2s\tremaining: 14m 1s\n20:\tlearn: 0.9750561\ttotal: 17.2s\tremaining: 13m 20s\n21:\tlearn: 0.9714586\ttotal: 17.2s\tremaining: 12m 43s\n22:\tlearn: 0.9676465\ttotal: 17.2s\tremaining: 12m 10s\n23:\tlearn: 0.9641841\ttotal: 17.2s\tremaining: 11m 39s\n24:\tlearn: 0.9607982\ttotal: 17.2s\tremaining: 11m 11s\n25:\tlearn: 0.9583121\ttotal: 17.2s\tremaining: 10m 45s\n26:\tlearn: 0.9559884\ttotal: 17.2s\tremaining: 10m 21s\n27:\tlearn: 0.9529257\ttotal: 17.2s\tremaining: 9m 58s\n28:\tlearn: 0.9505293\ttotal: 17.3s\tremaining: 9m 37s\n29:\tlearn: 0.9478827\ttotal: 17.3s\tremaining: 9m 18s\n30:\tlearn: 0.9454158\ttotal: 17.3s\tremaining: 9m\n31:\tlearn: 0.9430265\ttotal: 17.3s\tremaining: 8m 42s\n32:\tlearn: 0.9406775\ttotal: 17.3s\tremaining: 8m 26s\n33:\tlearn: 0.9390234\ttotal: 17.3s\tremaining: 8m 11s\n34:\tlearn: 0.9372198\ttotal: 17.3s\tremaining: 7m 57s\n35:\tlearn: 0.9350285\ttotal: 17.3s\tremaining: 7m 44s\n36:\tlearn: 0.9330189\ttotal: 17.3s\tremaining: 7m 31s\n37:\tlearn: 0.9316411\ttotal: 17.3s\tremaining: 7m 19s\n38:\tlearn: 0.9301653\ttotal: 17.4s\tremaining: 7m 7s\n39:\tlearn: 0.9286372\ttotal: 17.4s\tremaining: 6m 56s\n40:\tlearn: 0.9269866\ttotal: 17.4s\tremaining: 6m 46s\n41:\tlearn: 0.9254387\ttotal: 17.4s\tremaining: 6m 36s\n42:\tlearn: 0.9239665\ttotal: 17.4s\tremaining: 6m 27s\n43:\tlearn: 0.9220574\ttotal: 17.4s\tremaining: 6m 18s\n44:\tlearn: 0.9207417\ttotal: 17.4s\tremaining: 6m 9s\n45:\tlearn: 0.9191765\ttotal: 17.4s\tremaining: 6m 1s\n46:\tlearn: 0.9179130\ttotal: 17.4s\tremaining: 5m 53s\n47:\tlearn: 0.9166722\ttotal: 17.4s\tremaining: 5m 46s\n48:\tlearn: 0.9154034\ttotal: 17.5s\tremaining: 5m 38s\n49:\tlearn: 0.9146947\ttotal: 17.5s\tremaining: 5m 31s\n50:\tlearn: 0.9137112\ttotal: 17.5s\tremaining: 5m 25s\n51:\tlearn: 0.9128358\ttotal: 17.5s\tremaining: 5m 18s\n52:\tlearn: 0.9116327\ttotal: 17.5s\tremaining: 5m 12s\n53:\tlearn: 0.9103268\ttotal: 17.5s\tremaining: 5m 6s\n54:\tlearn: 0.9095484\ttotal: 17.5s\tremaining: 5m\n55:\tlearn: 0.9082466\ttotal: 17.5s\tremaining: 4m 55s\n56:\tlearn: 0.9071985\ttotal: 17.5s\tremaining: 4m 50s\n57:\tlearn: 0.9061361\ttotal: 17.5s\tremaining: 4m 44s\n58:\tlearn: 0.9052271\ttotal: 17.6s\tremaining: 4m 40s\n59:\tlearn: 0.9036401\ttotal: 17.6s\tremaining: 4m 35s\n60:\tlearn: 0.9027457\ttotal: 17.6s\tremaining: 4m 30s\n61:\tlearn: 0.9018526\ttotal: 17.6s\tremaining: 4m 26s\n62:\tlearn: 0.9003417\ttotal: 17.6s\tremaining: 4m 21s\n63:\tlearn: 0.8989280\ttotal: 17.6s\tremaining: 4m 17s\n64:\tlearn: 0.8978492\ttotal: 17.6s\tremaining: 4m 13s\n65:\tlearn: 0.8967433\ttotal: 17.6s\tremaining: 4m 9s\n66:\tlearn: 0.8957691\ttotal: 17.6s\tremaining: 4m 5s\n67:\tlearn: 0.8946701\ttotal: 17.6s\tremaining: 4m 1s\n68:\tlearn: 0.8933934\ttotal: 17.7s\tremaining: 3m 58s\n69:\tlearn: 0.8923777\ttotal: 17.7s\tremaining: 3m 54s\n70:\tlearn: 0.8916250\ttotal: 17.7s\tremaining: 3m 51s\n71:\tlearn: 0.8904076\ttotal: 17.7s\tremaining: 3m 47s\n72:\tlearn: 0.8891995\ttotal: 17.7s\tremaining: 3m 44s\n73:\tlearn: 0.8882328\ttotal: 17.7s\tremaining: 3m 41s\n74:\tlearn: 0.8875075\ttotal: 17.7s\tremaining: 3m 38s\n75:\tlearn: 0.8868234\ttotal: 17.7s\tremaining: 3m 35s\n76:\tlearn: 0.8859415\ttotal: 17.7s\tremaining: 3m 32s\n77:\tlearn: 0.8850822\ttotal: 17.7s\tremaining: 3m 29s\n78:\tlearn: 0.8844072\ttotal: 17.8s\tremaining: 3m 26s\n79:\tlearn: 0.8837980\ttotal: 17.8s\tremaining: 3m 24s\n80:\tlearn: 0.8831241\ttotal: 17.8s\tremaining: 3m 21s\n81:\tlearn: 0.8823701\ttotal: 17.8s\tremaining: 3m 19s\n82:\tlearn: 0.8816498\ttotal: 17.8s\tremaining: 3m 16s\n83:\tlearn: 0.8807355\ttotal: 17.8s\tremaining: 3m 14s\n84:\tlearn: 0.8796553\ttotal: 17.8s\tremaining: 3m 11s\n85:\tlearn: 0.8787855\ttotal: 17.8s\tremaining: 3m 9s\n86:\tlearn: 0.8780161\ttotal: 17.8s\tremaining: 3m 7s\n87:\tlearn: 0.8769321\ttotal: 17.8s\tremaining: 3m 4s\n88:\tlearn: 0.8759500\ttotal: 17.9s\tremaining: 3m 2s\n89:\tlearn: 0.8751355\ttotal: 17.9s\tremaining: 3m\n90:\tlearn: 0.8741943\ttotal: 17.9s\tremaining: 2m 58s\n91:\tlearn: 0.8732938\ttotal: 17.9s\tremaining: 2m 56s\n92:\tlearn: 0.8722859\ttotal: 17.9s\tremaining: 2m 54s\n93:\tlearn: 0.8710230\ttotal: 17.9s\tremaining: 2m 52s\n94:\tlearn: 0.8700590\ttotal: 17.9s\tremaining: 2m 50s\n95:\tlearn: 0.8695205\ttotal: 17.9s\tremaining: 2m 48s\n96:\tlearn: 0.8685122\ttotal: 17.9s\tremaining: 2m 46s\n97:\tlearn: 0.8675959\ttotal: 17.9s\tremaining: 2m 45s\n98:\tlearn: 0.8668751\ttotal: 18s\tremaining: 2m 43s\n99:\tlearn: 0.8661209\ttotal: 18s\tremaining: 2m 41s\n100:\tlearn: 0.8654528\ttotal: 18s\tremaining: 2m 39s\n101:\tlearn: 0.8645781\ttotal: 18s\tremaining: 2m 38s\n102:\tlearn: 0.8637831\ttotal: 18s\tremaining: 2m 36s\n103:\tlearn: 0.8628889\ttotal: 18s\tremaining: 2m 35s\n104:\tlearn: 0.8619965\ttotal: 18s\tremaining: 2m 33s\n105:\tlearn: 0.8612700\ttotal: 18s\tremaining: 2m 32s\n106:\tlearn: 0.8600811\ttotal: 18s\tremaining: 2m 30s\n107:\tlearn: 0.8589876\ttotal: 18s\tremaining: 2m 29s\n108:\tlearn: 0.8581347\ttotal: 18.1s\tremaining: 2m 27s\n109:\tlearn: 0.8571207\ttotal: 18.1s\tremaining: 2m 26s\n110:\tlearn: 0.8563322\ttotal: 18.1s\tremaining: 2m 24s\n111:\tlearn: 0.8556382\ttotal: 18.1s\tremaining: 2m 23s\n112:\tlearn: 0.8551864\ttotal: 18.1s\tremaining: 2m 22s\n113:\tlearn: 0.8545354\ttotal: 18.1s\tremaining: 2m 20s\n114:\tlearn: 0.8537818\ttotal: 18.1s\tremaining: 2m 19s\n115:\tlearn: 0.8529151\ttotal: 18.1s\tremaining: 2m 18s\n116:\tlearn: 0.8522284\ttotal: 18.1s\tremaining: 2m 16s\n117:\tlearn: 0.8509438\ttotal: 18.1s\tremaining: 2m 15s\n118:\tlearn: 0.8502446\ttotal: 18.1s\tremaining: 2m 14s\n119:\tlearn: 0.8494769\ttotal: 18.2s\tremaining: 2m 13s\n120:\tlearn: 0.8487182\ttotal: 18.2s\tremaining: 2m 11s\n121:\tlearn: 0.8479347\ttotal: 18.2s\tremaining: 2m 10s\n122:\tlearn: 0.8472992\ttotal: 18.2s\tremaining: 2m 9s\n123:\tlearn: 0.8462194\ttotal: 18.2s\tremaining: 2m 8s\n124:\tlearn: 0.8453966\ttotal: 18.2s\tremaining: 2m 7s\n125:\tlearn: 0.8446714\ttotal: 18.2s\tremaining: 2m 6s\n126:\tlearn: 0.8442121\ttotal: 18.2s\tremaining: 2m 5s\n127:\tlearn: 0.8433564\ttotal: 18.2s\tremaining: 2m 4s\n128:\tlearn: 0.8427174\ttotal: 18.3s\tremaining: 2m 3s\n129:\tlearn: 0.8421493\ttotal: 18.3s\tremaining: 2m 2s\n130:\tlearn: 0.8412910\ttotal: 18.3s\tremaining: 2m 1s\n131:\tlearn: 0.8402776\ttotal: 18.3s\tremaining: 2m\n132:\tlearn: 0.8397616\ttotal: 18.3s\tremaining: 1m 59s\n133:\tlearn: 0.8389561\ttotal: 18.3s\tremaining: 1m 58s\n134:\tlearn: 0.8383634\ttotal: 18.3s\tremaining: 1m 57s\n135:\tlearn: 0.8378281\ttotal: 18.3s\tremaining: 1m 56s\n136:\tlearn: 0.8368985\ttotal: 18.3s\tremaining: 1m 55s\n137:\tlearn: 0.8363003\ttotal: 18.3s\tremaining: 1m 54s\n138:\tlearn: 0.8357214\ttotal: 18.3s\tremaining: 1m 53s\n139:\tlearn: 0.8350817\ttotal: 18.4s\tremaining: 1m 52s\n140:\tlearn: 0.8346619\ttotal: 18.4s\tremaining: 1m 51s\n141:\tlearn: 0.8340397\ttotal: 18.4s\tremaining: 1m 51s\n142:\tlearn: 0.8334348\ttotal: 18.4s\tremaining: 1m 50s\n143:\tlearn: 0.8326055\ttotal: 18.4s\tremaining: 1m 49s\n144:\tlearn: 0.8320914\ttotal: 18.4s\tremaining: 1m 48s\n145:\tlearn: 0.8314634\ttotal: 18.4s\tremaining: 1m 47s\n146:\tlearn: 0.8308465\ttotal: 18.4s\tremaining: 1m 46s\n147:\tlearn: 0.8300864\ttotal: 18.4s\tremaining: 1m 46s\n148:\tlearn: 0.8293492\ttotal: 18.4s\tremaining: 1m 45s\n149:\tlearn: 0.8287614\ttotal: 18.5s\tremaining: 1m 44s\n150:\tlearn: 0.8280834\ttotal: 18.5s\tremaining: 1m 43s\n151:\tlearn: 0.8272890\ttotal: 18.5s\tremaining: 1m 43s\n152:\tlearn: 0.8266642\ttotal: 18.5s\tremaining: 1m 42s\n153:\tlearn: 0.8261408\ttotal: 18.5s\tremaining: 1m 41s\n154:\tlearn: 0.8254780\ttotal: 18.5s\tremaining: 1m 40s\n155:\tlearn: 0.8247908\ttotal: 18.5s\tremaining: 1m 40s\n156:\tlearn: 0.8237883\ttotal: 18.5s\tremaining: 1m 39s\n157:\tlearn: 0.8231999\ttotal: 18.5s\tremaining: 1m 38s\n158:\tlearn: 0.8225454\ttotal: 18.5s\tremaining: 1m 38s\n159:\tlearn: 0.8218063\ttotal: 18.6s\tremaining: 1m 37s\n160:\tlearn: 0.8212094\ttotal: 18.6s\tremaining: 1m 36s\n161:\tlearn: 0.8204202\ttotal: 18.6s\tremaining: 1m 36s\n162:\tlearn: 0.8195114\ttotal: 18.6s\tremaining: 1m 35s\n163:\tlearn: 0.8187923\ttotal: 18.6s\tremaining: 1m 34s\n164:\tlearn: 0.8182214\ttotal: 18.6s\tremaining: 1m 34s\n165:\tlearn: 0.8174986\ttotal: 18.6s\tremaining: 1m 33s\n166:\tlearn: 0.8169450\ttotal: 18.6s\tremaining: 1m 32s\n167:\tlearn: 0.8161772\ttotal: 18.6s\tremaining: 1m 32s\n168:\tlearn: 0.8156553\ttotal: 18.6s\tremaining: 1m 31s\n169:\tlearn: 0.8149230\ttotal: 18.6s\tremaining: 1m 31s\n170:\tlearn: 0.8142005\ttotal: 18.7s\tremaining: 1m 30s\n171:\tlearn: 0.8136170\ttotal: 18.7s\tremaining: 1m 29s\n172:\tlearn: 0.8128851\ttotal: 18.7s\tremaining: 1m 29s\n173:\tlearn: 0.8120378\ttotal: 18.7s\tremaining: 1m 28s\n174:\tlearn: 0.8114146\ttotal: 18.7s\tremaining: 1m 28s\n175:\tlearn: 0.8107828\ttotal: 18.7s\tremaining: 1m 27s\n176:\tlearn: 0.8102529\ttotal: 18.7s\tremaining: 1m 27s\n177:\tlearn: 0.8096693\ttotal: 18.7s\tremaining: 1m 26s\n178:\tlearn: 0.8090328\ttotal: 18.7s\tremaining: 1m 25s\n179:\tlearn: 0.8084532\ttotal: 18.7s\tremaining: 1m 25s\n180:\tlearn: 0.8078555\ttotal: 18.8s\tremaining: 1m 24s\n181:\tlearn: 0.8071020\ttotal: 18.8s\tremaining: 1m 24s\n182:\tlearn: 0.8063131\ttotal: 18.8s\tremaining: 1m 23s\n183:\tlearn: 0.8059272\ttotal: 18.8s\tremaining: 1m 23s\n184:\tlearn: 0.8052786\ttotal: 18.8s\tremaining: 1m 22s\n185:\tlearn: 0.8045514\ttotal: 18.8s\tremaining: 1m 22s\n186:\tlearn: 0.8040761\ttotal: 18.8s\tremaining: 1m 21s\n187:\tlearn: 0.8034118\ttotal: 18.8s\tremaining: 1m 21s\n188:\tlearn: 0.8027669\ttotal: 18.8s\tremaining: 1m 20s\n189:\tlearn: 0.8020408\ttotal: 18.8s\tremaining: 1m 20s\n190:\tlearn: 0.8012341\ttotal: 18.9s\tremaining: 1m 19s\n191:\tlearn: 0.8006630\ttotal: 18.9s\tremaining: 1m 19s\n192:\tlearn: 0.7998828\ttotal: 18.9s\tremaining: 1m 18s\n193:\tlearn: 0.7992459\ttotal: 18.9s\tremaining: 1m 18s\n194:\tlearn: 0.7983356\ttotal: 18.9s\tremaining: 1m 17s\n195:\tlearn: 0.7979727\ttotal: 18.9s\tremaining: 1m 17s\n196:\tlearn: 0.7972059\ttotal: 18.9s\tremaining: 1m 17s\n197:\tlearn: 0.7965667\ttotal: 18.9s\tremaining: 1m 16s\n198:\tlearn: 0.7958126\ttotal: 18.9s\tremaining: 1m 16s\n199:\tlearn: 0.7951670\ttotal: 18.9s\tremaining: 1m 15s\n200:\tlearn: 0.7946305\ttotal: 19s\tremaining: 1m 15s\n201:\tlearn: 0.7940295\ttotal: 19s\tremaining: 1m 14s\n202:\tlearn: 0.7933660\ttotal: 19s\tremaining: 1m 14s\n203:\tlearn: 0.7927344\ttotal: 19s\tremaining: 1m 14s\n204:\tlearn: 0.7918748\ttotal: 19s\tremaining: 1m 13s\n205:\tlearn: 0.7911298\ttotal: 19s\tremaining: 1m 13s\n206:\tlearn: 0.7906412\ttotal: 19s\tremaining: 1m 12s\n207:\tlearn: 0.7898443\ttotal: 19s\tremaining: 1m 12s\n208:\tlearn: 0.7890783\ttotal: 19s\tremaining: 1m 12s\n209:\tlearn: 0.7881369\ttotal: 19s\tremaining: 1m 11s\n210:\tlearn: 0.7873096\ttotal: 19.1s\tremaining: 1m 11s\n211:\tlearn: 0.7864803\ttotal: 19.1s\tremaining: 1m 10s\n212:\tlearn: 0.7857925\ttotal: 19.1s\tremaining: 1m 10s\n213:\tlearn: 0.7852050\ttotal: 19.1s\tremaining: 1m 10s\n214:\tlearn: 0.7846108\ttotal: 19.1s\tremaining: 1m 9s\n215:\tlearn: 0.7837019\ttotal: 19.1s\tremaining: 1m 9s\n216:\tlearn: 0.7830895\ttotal: 19.1s\tremaining: 1m 8s\n217:\tlearn: 0.7825790\ttotal: 19.1s\tremaining: 1m 8s\n218:\tlearn: 0.7819385\ttotal: 19.1s\tremaining: 1m 8s\n219:\tlearn: 0.7811847\ttotal: 19.1s\tremaining: 1m 7s\n220:\tlearn: 0.7805195\ttotal: 19.2s\tremaining: 1m 7s\n221:\tlearn: 0.7800904\ttotal: 19.2s\tremaining: 1m 7s\n222:\tlearn: 0.7795140\ttotal: 19.2s\tremaining: 1m 6s\n223:\tlearn: 0.7789876\ttotal: 19.2s\tremaining: 1m 6s\n224:\tlearn: 0.7782200\ttotal: 19.2s\tremaining: 1m 6s\n225:\tlearn: 0.7777403\ttotal: 19.2s\tremaining: 1m 5s\n226:\tlearn: 0.7771716\ttotal: 19.2s\tremaining: 1m 5s\n227:\tlearn: 0.7764710\ttotal: 19.2s\tremaining: 1m 5s\n228:\tlearn: 0.7757767\ttotal: 19.2s\tremaining: 1m 4s\n229:\tlearn: 0.7751438\ttotal: 19.2s\tremaining: 1m 4s\n230:\tlearn: 0.7745982\ttotal: 19.2s\tremaining: 1m 4s\n231:\tlearn: 0.7740134\ttotal: 19.3s\tremaining: 1m 3s\n232:\tlearn: 0.7733957\ttotal: 19.3s\tremaining: 1m 3s\n233:\tlearn: 0.7730443\ttotal: 19.3s\tremaining: 1m 3s\n234:\tlearn: 0.7726454\ttotal: 19.3s\tremaining: 1m 2s\n235:\tlearn: 0.7718756\ttotal: 19.3s\tremaining: 1m 2s\n236:\tlearn: 0.7712318\ttotal: 19.3s\tremaining: 1m 2s\n237:\tlearn: 0.7707984\ttotal: 19.3s\tremaining: 1m 1s\n238:\tlearn: 0.7701241\ttotal: 19.3s\tremaining: 1m 1s\n239:\tlearn: 0.7696488\ttotal: 19.3s\tremaining: 1m 1s\n240:\tlearn: 0.7691162\ttotal: 19.3s\tremaining: 1m\n241:\tlearn: 0.7684240\ttotal: 19.4s\tremaining: 1m\n242:\tlearn: 0.7678241\ttotal: 19.4s\tremaining: 1m\n243:\tlearn: 0.7670971\ttotal: 19.4s\tremaining: 1m\n244:\tlearn: 0.7662880\ttotal: 19.4s\tremaining: 59.7s\n245:\tlearn: 0.7656005\ttotal: 19.4s\tremaining: 59.4s\n246:\tlearn: 0.7646602\ttotal: 19.4s\tremaining: 59.2s\n247:\tlearn: 0.7641819\ttotal: 19.4s\tremaining: 58.9s\n248:\tlearn: 0.7636209\ttotal: 19.4s\tremaining: 58.6s\n249:\tlearn: 0.7630475\ttotal: 19.4s\tremaining: 58.3s\n250:\tlearn: 0.7624877\ttotal: 19.4s\tremaining: 58s\n251:\tlearn: 0.7619654\ttotal: 19.5s\tremaining: 57.7s\n252:\tlearn: 0.7615684\ttotal: 19.5s\tremaining: 57.5s\n253:\tlearn: 0.7605938\ttotal: 19.5s\tremaining: 57.2s\n254:\tlearn: 0.7600118\ttotal: 19.5s\tremaining: 56.9s\n255:\tlearn: 0.7594361\ttotal: 19.5s\tremaining: 56.7s\n256:\tlearn: 0.7590098\ttotal: 19.5s\tremaining: 56.4s\n257:\tlearn: 0.7585518\ttotal: 19.5s\tremaining: 56.1s\n258:\tlearn: 0.7580752\ttotal: 19.5s\tremaining: 55.9s\n259:\tlearn: 0.7575757\ttotal: 19.5s\tremaining: 55.6s\n260:\tlearn: 0.7569795\ttotal: 19.5s\tremaining: 55.3s\n261:\tlearn: 0.7563395\ttotal: 19.6s\tremaining: 55.1s\n262:\tlearn: 0.7557728\ttotal: 19.6s\tremaining: 54.8s\n263:\tlearn: 0.7551181\ttotal: 19.6s\tremaining: 54.6s\n264:\tlearn: 0.7546011\ttotal: 19.6s\tremaining: 54.3s\n265:\tlearn: 0.7539640\ttotal: 19.6s\tremaining: 54.1s\n266:\tlearn: 0.7534905\ttotal: 19.6s\tremaining: 53.8s\n267:\tlearn: 0.7527748\ttotal: 19.6s\tremaining: 53.6s\n268:\tlearn: 0.7524687\ttotal: 19.6s\tremaining: 53.3s\n269:\tlearn: 0.7519303\ttotal: 19.6s\tremaining: 53.1s\n270:\tlearn: 0.7511670\ttotal: 19.6s\tremaining: 52.8s\n271:\tlearn: 0.7505344\ttotal: 19.6s\tremaining: 52.6s\n272:\tlearn: 0.7499255\ttotal: 19.7s\tremaining: 52.4s\n273:\tlearn: 0.7492864\ttotal: 19.7s\tremaining: 52.1s\n274:\tlearn: 0.7486769\ttotal: 19.7s\tremaining: 51.9s\n275:\tlearn: 0.7480422\ttotal: 19.7s\tremaining: 51.6s\n276:\tlearn: 0.7476241\ttotal: 19.7s\tremaining: 51.4s\n277:\tlearn: 0.7470022\ttotal: 19.7s\tremaining: 51.2s\n278:\tlearn: 0.7462134\ttotal: 19.7s\tremaining: 51s\n279:\tlearn: 0.7456641\ttotal: 19.7s\tremaining: 50.7s\n280:\tlearn: 0.7450731\ttotal: 19.7s\tremaining: 50.5s\n281:\tlearn: 0.7444143\ttotal: 19.7s\tremaining: 50.3s\n282:\tlearn: 0.7436931\ttotal: 19.8s\tremaining: 50.1s\n283:\tlearn: 0.7431273\ttotal: 19.8s\tremaining: 49.8s\n284:\tlearn: 0.7426361\ttotal: 19.8s\tremaining: 49.6s\n285:\tlearn: 0.7419731\ttotal: 19.8s\tremaining: 49.4s\n286:\tlearn: 0.7415093\ttotal: 19.8s\tremaining: 49.2s\n287:\tlearn: 0.7408599\ttotal: 19.8s\tremaining: 49s\n288:\tlearn: 0.7401387\ttotal: 19.8s\tremaining: 48.8s\n289:\tlearn: 0.7397128\ttotal: 19.8s\tremaining: 48.5s\n290:\tlearn: 0.7392672\ttotal: 19.8s\tremaining: 48.3s\n291:\tlearn: 0.7384905\ttotal: 19.9s\tremaining: 48.1s\n292:\tlearn: 0.7378425\ttotal: 19.9s\tremaining: 47.9s\n293:\tlearn: 0.7374423\ttotal: 19.9s\tremaining: 47.7s\n294:\tlearn: 0.7368915\ttotal: 19.9s\tremaining: 47.5s\n295:\tlearn: 0.7361071\ttotal: 19.9s\tremaining: 47.3s\n296:\tlearn: 0.7352246\ttotal: 19.9s\tremaining: 47.1s\n297:\tlearn: 0.7346324\ttotal: 19.9s\tremaining: 46.9s\n298:\tlearn: 0.7341105\ttotal: 19.9s\tremaining: 46.7s\n299:\tlearn: 0.7337735\ttotal: 19.9s\tremaining: 46.5s\n300:\tlearn: 0.7333030\ttotal: 19.9s\tremaining: 46.3s\n301:\tlearn: 0.7328649\ttotal: 19.9s\tremaining: 46.1s\n302:\tlearn: 0.7322200\ttotal: 20s\tremaining: 45.9s\n303:\tlearn: 0.7316889\ttotal: 20s\tremaining: 45.7s\n304:\tlearn: 0.7311039\ttotal: 20s\tremaining: 45.5s\n305:\tlearn: 0.7305931\ttotal: 20s\tremaining: 45.3s\n306:\tlearn: 0.7299894\ttotal: 20s\tremaining: 45.2s\n307:\tlearn: 0.7293266\ttotal: 20s\tremaining: 45s\n308:\tlearn: 0.7287938\ttotal: 20s\tremaining: 44.8s\n309:\tlearn: 0.7280263\ttotal: 20s\tremaining: 44.6s\n310:\tlearn: 0.7274590\ttotal: 20s\tremaining: 44.4s\n311:\tlearn: 0.7269450\ttotal: 20.1s\tremaining: 44.2s\n312:\tlearn: 0.7261939\ttotal: 20.1s\tremaining: 44s\n313:\tlearn: 0.7257130\ttotal: 20.1s\tremaining: 43.9s\n314:\tlearn: 0.7250897\ttotal: 20.1s\tremaining: 43.7s\n315:\tlearn: 0.7245700\ttotal: 20.1s\tremaining: 43.5s\n316:\tlearn: 0.7240808\ttotal: 20.1s\tremaining: 43.3s\n317:\tlearn: 0.7235575\ttotal: 20.1s\tremaining: 43.1s\n318:\tlearn: 0.7228399\ttotal: 20.1s\tremaining: 43s\n319:\tlearn: 0.7223249\ttotal: 20.1s\tremaining: 42.8s\n320:\tlearn: 0.7216746\ttotal: 20.1s\tremaining: 42.6s\n321:\tlearn: 0.7212531\ttotal: 20.2s\tremaining: 42.4s\n322:\tlearn: 0.7206461\ttotal: 20.2s\tremaining: 42.3s\n323:\tlearn: 0.7198846\ttotal: 20.2s\tremaining: 42.1s\n324:\tlearn: 0.7190506\ttotal: 20.2s\tremaining: 41.9s\n325:\tlearn: 0.7185122\ttotal: 20.2s\tremaining: 41.8s\n326:\tlearn: 0.7180885\ttotal: 20.2s\tremaining: 41.6s\n327:\tlearn: 0.7174771\ttotal: 20.2s\tremaining: 41.4s\n328:\tlearn: 0.7170964\ttotal: 20.2s\tremaining: 41.3s\n329:\tlearn: 0.7165783\ttotal: 20.2s\tremaining: 41.1s\n330:\tlearn: 0.7161613\ttotal: 20.2s\tremaining: 40.9s\n331:\tlearn: 0.7156853\ttotal: 20.3s\tremaining: 40.8s\n332:\tlearn: 0.7148573\ttotal: 20.3s\tremaining: 40.6s\n333:\tlearn: 0.7143437\ttotal: 20.3s\tremaining: 40.4s\n334:\tlearn: 0.7137201\ttotal: 20.3s\tremaining: 40.3s\n335:\tlearn: 0.7130586\ttotal: 20.3s\tremaining: 40.1s\n336:\tlearn: 0.7124387\ttotal: 20.3s\tremaining: 39.9s\n337:\tlearn: 0.7118133\ttotal: 20.3s\tremaining: 39.8s\n338:\tlearn: 0.7112364\ttotal: 20.3s\tremaining: 39.6s\n339:\tlearn: 0.7105828\ttotal: 20.3s\tremaining: 39.5s\n340:\tlearn: 0.7100365\ttotal: 20.3s\tremaining: 39.3s\n341:\tlearn: 0.7093146\ttotal: 20.4s\tremaining: 39.2s\n342:\tlearn: 0.7088780\ttotal: 20.4s\tremaining: 39s\n343:\tlearn: 0.7082433\ttotal: 20.4s\tremaining: 38.9s\n344:\tlearn: 0.7077920\ttotal: 20.4s\tremaining: 38.7s\n345:\tlearn: 0.7072697\ttotal: 20.4s\tremaining: 38.6s\n346:\tlearn: 0.7065412\ttotal: 20.4s\tremaining: 38.4s\n347:\tlearn: 0.7057773\ttotal: 20.4s\tremaining: 38.3s\n348:\tlearn: 0.7051785\ttotal: 20.4s\tremaining: 38.1s\n349:\tlearn: 0.7047060\ttotal: 20.4s\tremaining: 38s\n350:\tlearn: 0.7040656\ttotal: 20.4s\tremaining: 37.8s\n351:\tlearn: 0.7035329\ttotal: 20.5s\tremaining: 37.7s\n352:\tlearn: 0.7031276\ttotal: 20.5s\tremaining: 37.5s\n353:\tlearn: 0.7024597\ttotal: 20.5s\tremaining: 37.4s\n354:\tlearn: 0.7017422\ttotal: 20.5s\tremaining: 37.2s\n355:\tlearn: 0.7010551\ttotal: 20.5s\tremaining: 37.1s\n356:\tlearn: 0.7006574\ttotal: 20.5s\tremaining: 36.9s\n357:\tlearn: 0.7003322\ttotal: 20.5s\tremaining: 36.8s\n358:\tlearn: 0.6997920\ttotal: 20.5s\tremaining: 36.6s\n359:\tlearn: 0.6991949\ttotal: 20.5s\tremaining: 36.5s\n360:\tlearn: 0.6986710\ttotal: 20.5s\tremaining: 36.4s\n361:\tlearn: 0.6980394\ttotal: 20.6s\tremaining: 36.2s\n362:\tlearn: 0.6973875\ttotal: 20.6s\tremaining: 36.1s\n363:\tlearn: 0.6967464\ttotal: 20.6s\tremaining: 36s\n364:\tlearn: 0.6961081\ttotal: 20.6s\tremaining: 35.8s\n365:\tlearn: 0.6954872\ttotal: 20.6s\tremaining: 35.7s\n366:\tlearn: 0.6948812\ttotal: 20.6s\tremaining: 35.5s\n367:\tlearn: 0.6942665\ttotal: 20.6s\tremaining: 35.4s\n368:\tlearn: 0.6938475\ttotal: 20.6s\tremaining: 35.3s\n369:\tlearn: 0.6932190\ttotal: 20.6s\tremaining: 35.1s\n370:\tlearn: 0.6926883\ttotal: 20.6s\tremaining: 35s\n371:\tlearn: 0.6923627\ttotal: 20.7s\tremaining: 34.9s\n372:\tlearn: 0.6917723\ttotal: 20.7s\tremaining: 34.7s\n373:\tlearn: 0.6913283\ttotal: 20.7s\tremaining: 34.6s\n374:\tlearn: 0.6908301\ttotal: 20.7s\tremaining: 34.5s\n375:\tlearn: 0.6905069\ttotal: 20.7s\tremaining: 34.3s\n376:\tlearn: 0.6901145\ttotal: 20.7s\tremaining: 34.2s\n377:\tlearn: 0.6896376\ttotal: 20.7s\tremaining: 34.1s\n378:\tlearn: 0.6890380\ttotal: 20.7s\tremaining: 34s\n379:\tlearn: 0.6884726\ttotal: 20.7s\tremaining: 33.8s\n380:\tlearn: 0.6880082\ttotal: 20.7s\tremaining: 33.7s\n381:\tlearn: 0.6875872\ttotal: 20.8s\tremaining: 33.6s\n382:\tlearn: 0.6870179\ttotal: 20.8s\tremaining: 33.5s\n383:\tlearn: 0.6864168\ttotal: 20.8s\tremaining: 33.3s\n384:\tlearn: 0.6859852\ttotal: 20.8s\tremaining: 33.2s\n385:\tlearn: 0.6854139\ttotal: 20.8s\tremaining: 33.1s\n386:\tlearn: 0.6849185\ttotal: 20.8s\tremaining: 33s\n387:\tlearn: 0.6843623\ttotal: 20.8s\tremaining: 32.8s\n388:\tlearn: 0.6838362\ttotal: 20.8s\tremaining: 32.7s\n389:\tlearn: 0.6834900\ttotal: 20.8s\tremaining: 32.6s\n390:\tlearn: 0.6829899\ttotal: 20.8s\tremaining: 32.5s\n391:\tlearn: 0.6824789\ttotal: 20.9s\tremaining: 32.3s\n392:\tlearn: 0.6820145\ttotal: 20.9s\tremaining: 32.2s\n393:\tlearn: 0.6814984\ttotal: 20.9s\tremaining: 32.1s\n394:\tlearn: 0.6810026\ttotal: 20.9s\tremaining: 32s\n395:\tlearn: 0.6802980\ttotal: 20.9s\tremaining: 31.9s\n396:\tlearn: 0.6796038\ttotal: 20.9s\tremaining: 31.8s\n397:\tlearn: 0.6788785\ttotal: 20.9s\tremaining: 31.6s\n398:\tlearn: 0.6780827\ttotal: 20.9s\tremaining: 31.5s\n399:\tlearn: 0.6776959\ttotal: 20.9s\tremaining: 31.4s\n400:\tlearn: 0.6772781\ttotal: 20.9s\tremaining: 31.3s\n401:\tlearn: 0.6767754\ttotal: 21s\tremaining: 31.2s\n402:\tlearn: 0.6762365\ttotal: 21s\tremaining: 31.1s\n403:\tlearn: 0.6758372\ttotal: 21s\tremaining: 30.9s\n404:\tlearn: 0.6752938\ttotal: 21s\tremaining: 30.8s\n405:\tlearn: 0.6746442\ttotal: 21s\tremaining: 30.7s\n406:\tlearn: 0.6742325\ttotal: 21s\tremaining: 30.6s\n407:\tlearn: 0.6736073\ttotal: 21s\tremaining: 30.5s\n408:\tlearn: 0.6732189\ttotal: 21s\tremaining: 30.4s\n409:\tlearn: 0.6728728\ttotal: 21s\tremaining: 30.3s\n410:\tlearn: 0.6722948\ttotal: 21s\tremaining: 30.2s\n411:\tlearn: 0.6716659\ttotal: 21.1s\tremaining: 30.1s\n412:\tlearn: 0.6712546\ttotal: 21.1s\tremaining: 29.9s\n413:\tlearn: 0.6707982\ttotal: 21.1s\tremaining: 29.8s\n414:\tlearn: 0.6700488\ttotal: 21.1s\tremaining: 29.7s\n415:\tlearn: 0.6694327\ttotal: 21.1s\tremaining: 29.6s\n416:\tlearn: 0.6688885\ttotal: 21.1s\tremaining: 29.5s\n417:\tlearn: 0.6684014\ttotal: 21.1s\tremaining: 29.4s\n418:\tlearn: 0.6679256\ttotal: 21.1s\tremaining: 29.3s\n419:\tlearn: 0.6674176\ttotal: 21.1s\tremaining: 29.2s\n420:\tlearn: 0.6667704\ttotal: 21.1s\tremaining: 29.1s\n421:\tlearn: 0.6662464\ttotal: 21.2s\tremaining: 29s\n422:\tlearn: 0.6657773\ttotal: 21.2s\tremaining: 28.9s\n423:\tlearn: 0.6653677\ttotal: 21.2s\tremaining: 28.8s\n424:\tlearn: 0.6647531\ttotal: 21.2s\tremaining: 28.7s\n425:\tlearn: 0.6642862\ttotal: 21.2s\tremaining: 28.6s\n426:\tlearn: 0.6638297\ttotal: 21.2s\tremaining: 28.5s\n427:\tlearn: 0.6631567\ttotal: 21.2s\tremaining: 28.4s\n428:\tlearn: 0.6627154\ttotal: 21.2s\tremaining: 28.2s\n429:\tlearn: 0.6619248\ttotal: 21.2s\tremaining: 28.1s\n430:\tlearn: 0.6614789\ttotal: 21.2s\tremaining: 28s\n431:\tlearn: 0.6609101\ttotal: 21.3s\tremaining: 27.9s\n432:\tlearn: 0.6603793\ttotal: 21.3s\tremaining: 27.8s\n433:\tlearn: 0.6597797\ttotal: 21.3s\tremaining: 27.7s\n434:\tlearn: 0.6593746\ttotal: 21.3s\tremaining: 27.6s\n435:\tlearn: 0.6589568\ttotal: 21.3s\tremaining: 27.5s\n436:\tlearn: 0.6583226\ttotal: 21.3s\tremaining: 27.4s\n437:\tlearn: 0.6577696\ttotal: 21.3s\tremaining: 27.3s\n438:\tlearn: 0.6572903\ttotal: 21.3s\tremaining: 27.2s\n439:\tlearn: 0.6568234\ttotal: 21.3s\tremaining: 27.2s\n440:\tlearn: 0.6563955\ttotal: 21.3s\tremaining: 27.1s\n441:\tlearn: 0.6559679\ttotal: 21.4s\tremaining: 27s\n442:\tlearn: 0.6555875\ttotal: 21.4s\tremaining: 26.9s\n443:\tlearn: 0.6551489\ttotal: 21.4s\tremaining: 26.8s\n444:\tlearn: 0.6545201\ttotal: 21.4s\tremaining: 26.7s\n445:\tlearn: 0.6541934\ttotal: 21.4s\tremaining: 26.6s\n446:\tlearn: 0.6537141\ttotal: 21.4s\tremaining: 26.5s\n447:\tlearn: 0.6532351\ttotal: 21.4s\tremaining: 26.4s\n448:\tlearn: 0.6527860\ttotal: 21.4s\tremaining: 26.3s\n449:\tlearn: 0.6522656\ttotal: 21.4s\tremaining: 26.2s\n450:\tlearn: 0.6517115\ttotal: 21.4s\tremaining: 26.1s\n451:\tlearn: 0.6510671\ttotal: 21.4s\tremaining: 26s\n452:\tlearn: 0.6505064\ttotal: 21.5s\tremaining: 25.9s\n453:\tlearn: 0.6501555\ttotal: 21.5s\tremaining: 25.8s\n454:\tlearn: 0.6495811\ttotal: 21.5s\tremaining: 25.7s\n455:\tlearn: 0.6488614\ttotal: 21.5s\tremaining: 25.6s\n456:\tlearn: 0.6483279\ttotal: 21.5s\tremaining: 25.5s\n457:\tlearn: 0.6480542\ttotal: 21.5s\tremaining: 25.5s\n458:\tlearn: 0.6476610\ttotal: 21.5s\tremaining: 25.4s\n459:\tlearn: 0.6473135\ttotal: 21.5s\tremaining: 25.3s\n460:\tlearn: 0.6469991\ttotal: 21.5s\tremaining: 25.2s\n461:\tlearn: 0.6465305\ttotal: 21.5s\tremaining: 25.1s\n462:\tlearn: 0.6461243\ttotal: 21.6s\tremaining: 25s\n463:\tlearn: 0.6453859\ttotal: 21.6s\tremaining: 24.9s\n464:\tlearn: 0.6448874\ttotal: 21.6s\tremaining: 24.8s\n465:\tlearn: 0.6444298\ttotal: 21.6s\tremaining: 24.7s\n466:\tlearn: 0.6438577\ttotal: 21.6s\tremaining: 24.6s\n467:\tlearn: 0.6434799\ttotal: 21.6s\tremaining: 24.6s\n468:\tlearn: 0.6430733\ttotal: 21.6s\tremaining: 24.5s\n469:\tlearn: 0.6427080\ttotal: 21.6s\tremaining: 24.4s\n470:\tlearn: 0.6421427\ttotal: 21.6s\tremaining: 24.3s\n471:\tlearn: 0.6416202\ttotal: 21.6s\tremaining: 24.2s\n472:\tlearn: 0.6412067\ttotal: 21.7s\tremaining: 24.1s\n473:\tlearn: 0.6407294\ttotal: 21.7s\tremaining: 24s\n474:\tlearn: 0.6401871\ttotal: 21.7s\tremaining: 24s\n475:\tlearn: 0.6398835\ttotal: 21.7s\tremaining: 23.9s\n476:\tlearn: 0.6394123\ttotal: 21.7s\tremaining: 23.8s\n477:\tlearn: 0.6390581\ttotal: 21.7s\tremaining: 23.7s\n478:\tlearn: 0.6386933\ttotal: 21.7s\tremaining: 23.6s\n479:\tlearn: 0.6384653\ttotal: 21.7s\tremaining: 23.5s\n480:\tlearn: 0.6380170\ttotal: 21.7s\tremaining: 23.5s\n481:\tlearn: 0.6375400\ttotal: 21.7s\tremaining: 23.4s\n482:\tlearn: 0.6369874\ttotal: 21.8s\tremaining: 23.3s\n483:\tlearn: 0.6365698\ttotal: 21.8s\tremaining: 23.2s\n484:\tlearn: 0.6360147\ttotal: 21.8s\tremaining: 23.1s\n485:\tlearn: 0.6356039\ttotal: 21.8s\tremaining: 23s\n486:\tlearn: 0.6350477\ttotal: 21.8s\tremaining: 23s\n487:\tlearn: 0.6347502\ttotal: 21.8s\tremaining: 22.9s\n488:\tlearn: 0.6343761\ttotal: 21.8s\tremaining: 22.8s\n489:\tlearn: 0.6341185\ttotal: 21.8s\tremaining: 22.7s\n490:\tlearn: 0.6336474\ttotal: 21.8s\tremaining: 22.6s\n491:\tlearn: 0.6331573\ttotal: 21.8s\tremaining: 22.6s\n492:\tlearn: 0.6327130\ttotal: 21.9s\tremaining: 22.5s\n493:\tlearn: 0.6323318\ttotal: 21.9s\tremaining: 22.4s\n494:\tlearn: 0.6319009\ttotal: 21.9s\tremaining: 22.3s\n495:\tlearn: 0.6314048\ttotal: 21.9s\tremaining: 22.2s\n496:\tlearn: 0.6309711\ttotal: 21.9s\tremaining: 22.2s\n497:\tlearn: 0.6305987\ttotal: 21.9s\tremaining: 22.1s\n498:\tlearn: 0.6301616\ttotal: 21.9s\tremaining: 22s\n499:\tlearn: 0.6297588\ttotal: 21.9s\tremaining: 21.9s\n500:\tlearn: 0.6292817\ttotal: 21.9s\tremaining: 21.8s\n501:\tlearn: 0.6287047\ttotal: 21.9s\tremaining: 21.8s\n502:\tlearn: 0.6283562\ttotal: 22s\tremaining: 21.7s\n503:\tlearn: 0.6278662\ttotal: 22s\tremaining: 21.6s\n504:\tlearn: 0.6273282\ttotal: 22s\tremaining: 21.5s\n505:\tlearn: 0.6270174\ttotal: 22s\tremaining: 21.5s\n506:\tlearn: 0.6265583\ttotal: 22s\tremaining: 21.4s\n507:\tlearn: 0.6260740\ttotal: 22s\tremaining: 21.3s\n508:\tlearn: 0.6257116\ttotal: 22s\tremaining: 21.2s\n509:\tlearn: 0.6251634\ttotal: 22s\tremaining: 21.2s\n510:\tlearn: 0.6247294\ttotal: 22s\tremaining: 21.1s\n511:\tlearn: 0.6241449\ttotal: 22s\tremaining: 21s\n512:\tlearn: 0.6235524\ttotal: 22.1s\tremaining: 20.9s\n513:\tlearn: 0.6231105\ttotal: 22.1s\tremaining: 20.9s\n514:\tlearn: 0.6227139\ttotal: 22.1s\tremaining: 20.8s\n515:\tlearn: 0.6223245\ttotal: 22.1s\tremaining: 20.7s\n516:\tlearn: 0.6220147\ttotal: 22.1s\tremaining: 20.6s\n517:\tlearn: 0.6215946\ttotal: 22.1s\tremaining: 20.6s\n518:\tlearn: 0.6211557\ttotal: 22.1s\tremaining: 20.5s\n519:\tlearn: 0.6207141\ttotal: 22.1s\tremaining: 20.4s\n520:\tlearn: 0.6201464\ttotal: 22.1s\tremaining: 20.4s\n521:\tlearn: 0.6197312\ttotal: 22.1s\tremaining: 20.3s\n522:\tlearn: 0.6193111\ttotal: 22.2s\tremaining: 20.2s\n523:\tlearn: 0.6187770\ttotal: 22.2s\tremaining: 20.1s\n524:\tlearn: 0.6183003\ttotal: 22.2s\tremaining: 20.1s\n525:\tlearn: 0.6178057\ttotal: 22.2s\tremaining: 20s\n526:\tlearn: 0.6173392\ttotal: 22.2s\tremaining: 19.9s\n527:\tlearn: 0.6168541\ttotal: 22.2s\tremaining: 19.9s\n528:\tlearn: 0.6164452\ttotal: 22.2s\tremaining: 19.8s\n529:\tlearn: 0.6160451\ttotal: 22.2s\tremaining: 19.7s\n530:\tlearn: 0.6155960\ttotal: 22.2s\tremaining: 19.6s\n531:\tlearn: 0.6150808\ttotal: 22.2s\tremaining: 19.6s\n532:\tlearn: 0.6146797\ttotal: 22.3s\tremaining: 19.5s\n533:\tlearn: 0.6142220\ttotal: 22.3s\tremaining: 19.4s\n534:\tlearn: 0.6138110\ttotal: 22.3s\tremaining: 19.4s\n535:\tlearn: 0.6133942\ttotal: 22.3s\tremaining: 19.3s\n536:\tlearn: 0.6130697\ttotal: 22.3s\tremaining: 19.2s\n537:\tlearn: 0.6125961\ttotal: 22.3s\tremaining: 19.2s\n538:\tlearn: 0.6120647\ttotal: 22.3s\tremaining: 19.1s\n539:\tlearn: 0.6115487\ttotal: 22.3s\tremaining: 19s\n540:\tlearn: 0.6111055\ttotal: 22.3s\tremaining: 19s\n541:\tlearn: 0.6107416\ttotal: 22.3s\tremaining: 18.9s\n542:\tlearn: 0.6103458\ttotal: 22.4s\tremaining: 18.8s\n543:\tlearn: 0.6099549\ttotal: 22.4s\tremaining: 18.7s\n544:\tlearn: 0.6095096\ttotal: 22.4s\tremaining: 18.7s\n545:\tlearn: 0.6090482\ttotal: 22.4s\tremaining: 18.6s\n546:\tlearn: 0.6085728\ttotal: 22.4s\tremaining: 18.5s\n547:\tlearn: 0.6082261\ttotal: 22.4s\tremaining: 18.5s\n548:\tlearn: 0.6079730\ttotal: 22.4s\tremaining: 18.4s\n549:\tlearn: 0.6075014\ttotal: 22.4s\tremaining: 18.3s\n550:\tlearn: 0.6069456\ttotal: 22.4s\tremaining: 18.3s\n551:\tlearn: 0.6065744\ttotal: 22.4s\tremaining: 18.2s\n552:\tlearn: 0.6061739\ttotal: 22.5s\tremaining: 18.1s\n553:\tlearn: 0.6056374\ttotal: 22.5s\tremaining: 18.1s\n554:\tlearn: 0.6050785\ttotal: 22.5s\tremaining: 18s\n555:\tlearn: 0.6047863\ttotal: 22.5s\tremaining: 18s\n556:\tlearn: 0.6043235\ttotal: 22.5s\tremaining: 17.9s\n557:\tlearn: 0.6038325\ttotal: 22.5s\tremaining: 17.8s\n558:\tlearn: 0.6033943\ttotal: 22.5s\tremaining: 17.8s\n559:\tlearn: 0.6029749\ttotal: 22.5s\tremaining: 17.7s\n560:\tlearn: 0.6026725\ttotal: 22.5s\tremaining: 17.6s\n561:\tlearn: 0.6023482\ttotal: 22.5s\tremaining: 17.6s\n562:\tlearn: 0.6019726\ttotal: 22.6s\tremaining: 17.5s\n563:\tlearn: 0.6014337\ttotal: 22.6s\tremaining: 17.4s\n564:\tlearn: 0.6008242\ttotal: 22.6s\tremaining: 17.4s\n565:\tlearn: 0.6004697\ttotal: 22.6s\tremaining: 17.3s\n566:\tlearn: 0.5999215\ttotal: 22.6s\tremaining: 17.3s\n567:\tlearn: 0.5993790\ttotal: 22.6s\tremaining: 17.2s\n568:\tlearn: 0.5988895\ttotal: 22.6s\tremaining: 17.1s\n569:\tlearn: 0.5986054\ttotal: 22.6s\tremaining: 17.1s\n570:\tlearn: 0.5983190\ttotal: 22.6s\tremaining: 17s\n571:\tlearn: 0.5979337\ttotal: 22.6s\tremaining: 16.9s\n572:\tlearn: 0.5974107\ttotal: 22.6s\tremaining: 16.9s\n573:\tlearn: 0.5969640\ttotal: 22.7s\tremaining: 16.8s\n574:\tlearn: 0.5965272\ttotal: 22.7s\tremaining: 16.8s\n575:\tlearn: 0.5961009\ttotal: 22.7s\tremaining: 16.7s\n576:\tlearn: 0.5957446\ttotal: 22.7s\tremaining: 16.6s\n577:\tlearn: 0.5954039\ttotal: 22.7s\tremaining: 16.6s\n578:\tlearn: 0.5950210\ttotal: 22.7s\tremaining: 16.5s\n579:\tlearn: 0.5946752\ttotal: 22.7s\tremaining: 16.5s\n580:\tlearn: 0.5943351\ttotal: 22.7s\tremaining: 16.4s\n581:\tlearn: 0.5938714\ttotal: 22.7s\tremaining: 16.3s\n582:\tlearn: 0.5933665\ttotal: 22.7s\tremaining: 16.3s\n583:\tlearn: 0.5930180\ttotal: 22.8s\tremaining: 16.2s\n584:\tlearn: 0.5925913\ttotal: 22.8s\tremaining: 16.2s\n585:\tlearn: 0.5921998\ttotal: 22.8s\tremaining: 16.1s\n586:\tlearn: 0.5916400\ttotal: 22.8s\tremaining: 16s\n587:\tlearn: 0.5912425\ttotal: 22.8s\tremaining: 16s\n588:\tlearn: 0.5909237\ttotal: 22.8s\tremaining: 15.9s\n589:\tlearn: 0.5905087\ttotal: 22.8s\tremaining: 15.9s\n590:\tlearn: 0.5902231\ttotal: 22.8s\tremaining: 15.8s\n591:\tlearn: 0.5898410\ttotal: 22.8s\tremaining: 15.7s\n592:\tlearn: 0.5894639\ttotal: 22.8s\tremaining: 15.7s\n593:\tlearn: 0.5889771\ttotal: 22.9s\tremaining: 15.6s\n594:\tlearn: 0.5884733\ttotal: 22.9s\tremaining: 15.6s\n595:\tlearn: 0.5879162\ttotal: 22.9s\tremaining: 15.5s\n596:\tlearn: 0.5872508\ttotal: 22.9s\tremaining: 15.4s\n597:\tlearn: 0.5868199\ttotal: 22.9s\tremaining: 15.4s\n598:\tlearn: 0.5863692\ttotal: 22.9s\tremaining: 15.3s\n599:\tlearn: 0.5859838\ttotal: 22.9s\tremaining: 15.3s\n600:\tlearn: 0.5854583\ttotal: 22.9s\tremaining: 15.2s\n601:\tlearn: 0.5850275\ttotal: 22.9s\tremaining: 15.2s\n602:\tlearn: 0.5847316\ttotal: 22.9s\tremaining: 15.1s\n603:\tlearn: 0.5845263\ttotal: 23s\tremaining: 15.1s\n604:\tlearn: 0.5840611\ttotal: 23s\tremaining: 15s\n605:\tlearn: 0.5837581\ttotal: 23s\tremaining: 14.9s\n606:\tlearn: 0.5833321\ttotal: 23s\tremaining: 14.9s\n607:\tlearn: 0.5826989\ttotal: 23s\tremaining: 14.8s\n608:\tlearn: 0.5822665\ttotal: 23s\tremaining: 14.8s\n609:\tlearn: 0.5817933\ttotal: 23s\tremaining: 14.7s\n610:\tlearn: 0.5814083\ttotal: 23s\tremaining: 14.7s\n611:\tlearn: 0.5809144\ttotal: 23s\tremaining: 14.6s\n612:\tlearn: 0.5804598\ttotal: 23s\tremaining: 14.5s\n613:\tlearn: 0.5802140\ttotal: 23.1s\tremaining: 14.5s\n614:\tlearn: 0.5799437\ttotal: 23.1s\tremaining: 14.4s\n615:\tlearn: 0.5795365\ttotal: 23.1s\tremaining: 14.4s\n616:\tlearn: 0.5791911\ttotal: 23.1s\tremaining: 14.3s\n617:\tlearn: 0.5786089\ttotal: 23.1s\tremaining: 14.3s\n618:\tlearn: 0.5780315\ttotal: 23.1s\tremaining: 14.2s\n619:\tlearn: 0.5775430\ttotal: 23.1s\tremaining: 14.2s\n620:\tlearn: 0.5771540\ttotal: 23.1s\tremaining: 14.1s\n621:\tlearn: 0.5767025\ttotal: 23.1s\tremaining: 14.1s\n622:\tlearn: 0.5763770\ttotal: 23.1s\tremaining: 14s\n623:\tlearn: 0.5760591\ttotal: 23.2s\tremaining: 14s\n624:\tlearn: 0.5757140\ttotal: 23.2s\tremaining: 13.9s\n625:\tlearn: 0.5752711\ttotal: 23.2s\tremaining: 13.8s\n626:\tlearn: 0.5748694\ttotal: 23.2s\tremaining: 13.8s\n627:\tlearn: 0.5743830\ttotal: 23.2s\tremaining: 13.7s\n628:\tlearn: 0.5739285\ttotal: 23.2s\tremaining: 13.7s\n629:\tlearn: 0.5735267\ttotal: 23.2s\tremaining: 13.6s\n630:\tlearn: 0.5731935\ttotal: 23.2s\tremaining: 13.6s\n631:\tlearn: 0.5728621\ttotal: 23.2s\tremaining: 13.5s\n632:\tlearn: 0.5722874\ttotal: 23.2s\tremaining: 13.5s\n633:\tlearn: 0.5717856\ttotal: 23.3s\tremaining: 13.4s\n634:\tlearn: 0.5714180\ttotal: 23.3s\tremaining: 13.4s\n635:\tlearn: 0.5709264\ttotal: 23.3s\tremaining: 13.3s\n636:\tlearn: 0.5704270\ttotal: 23.3s\tremaining: 13.3s\n637:\tlearn: 0.5701225\ttotal: 23.3s\tremaining: 13.2s\n638:\tlearn: 0.5697022\ttotal: 23.3s\tremaining: 13.2s\n639:\tlearn: 0.5691987\ttotal: 23.3s\tremaining: 13.1s\n640:\tlearn: 0.5688251\ttotal: 23.3s\tremaining: 13.1s\n641:\tlearn: 0.5684307\ttotal: 23.3s\tremaining: 13s\n642:\tlearn: 0.5679429\ttotal: 23.3s\tremaining: 13s\n643:\tlearn: 0.5674371\ttotal: 23.4s\tremaining: 12.9s\n644:\tlearn: 0.5668513\ttotal: 23.4s\tremaining: 12.9s\n645:\tlearn: 0.5666358\ttotal: 23.4s\tremaining: 12.8s\n646:\tlearn: 0.5662036\ttotal: 23.4s\tremaining: 12.8s\n647:\tlearn: 0.5657869\ttotal: 23.4s\tremaining: 12.7s\n648:\tlearn: 0.5654025\ttotal: 23.4s\tremaining: 12.7s\n649:\tlearn: 0.5648758\ttotal: 23.4s\tremaining: 12.6s\n650:\tlearn: 0.5645073\ttotal: 23.4s\tremaining: 12.6s\n651:\tlearn: 0.5641363\ttotal: 23.4s\tremaining: 12.5s\n652:\tlearn: 0.5637086\ttotal: 23.4s\tremaining: 12.5s\n653:\tlearn: 0.5631191\ttotal: 23.4s\tremaining: 12.4s\n654:\tlearn: 0.5627884\ttotal: 23.5s\tremaining: 12.4s\n655:\tlearn: 0.5624888\ttotal: 23.5s\tremaining: 12.3s\n656:\tlearn: 0.5621701\ttotal: 23.5s\tremaining: 12.3s\n657:\tlearn: 0.5618407\ttotal: 23.5s\tremaining: 12.2s\n658:\tlearn: 0.5616684\ttotal: 23.5s\tremaining: 12.2s\n659:\tlearn: 0.5613006\ttotal: 23.5s\tremaining: 12.1s\n660:\tlearn: 0.5610277\ttotal: 23.5s\tremaining: 12.1s\n661:\tlearn: 0.5606608\ttotal: 23.5s\tremaining: 12s\n662:\tlearn: 0.5603196\ttotal: 23.5s\tremaining: 12s\n663:\tlearn: 0.5599569\ttotal: 23.5s\tremaining: 11.9s\n664:\tlearn: 0.5595430\ttotal: 23.6s\tremaining: 11.9s\n665:\tlearn: 0.5591088\ttotal: 23.6s\tremaining: 11.8s\n666:\tlearn: 0.5586672\ttotal: 23.6s\tremaining: 11.8s\n667:\tlearn: 0.5583628\ttotal: 23.6s\tremaining: 11.7s\n668:\tlearn: 0.5581479\ttotal: 23.6s\tremaining: 11.7s\n669:\tlearn: 0.5576972\ttotal: 23.6s\tremaining: 11.6s\n670:\tlearn: 0.5572678\ttotal: 23.6s\tremaining: 11.6s\n671:\tlearn: 0.5567628\ttotal: 23.6s\tremaining: 11.5s\n672:\tlearn: 0.5563853\ttotal: 23.6s\tremaining: 11.5s\n673:\tlearn: 0.5559196\ttotal: 23.6s\tremaining: 11.4s\n674:\tlearn: 0.5555778\ttotal: 23.7s\tremaining: 11.4s\n675:\tlearn: 0.5551848\ttotal: 23.7s\tremaining: 11.3s\n676:\tlearn: 0.5547833\ttotal: 23.7s\tremaining: 11.3s\n677:\tlearn: 0.5541699\ttotal: 23.7s\tremaining: 11.2s\n678:\tlearn: 0.5538421\ttotal: 23.7s\tremaining: 11.2s\n679:\tlearn: 0.5534731\ttotal: 23.7s\tremaining: 11.2s\n680:\tlearn: 0.5530861\ttotal: 23.7s\tremaining: 11.1s\n681:\tlearn: 0.5527934\ttotal: 23.7s\tremaining: 11.1s\n682:\tlearn: 0.5525336\ttotal: 23.7s\tremaining: 11s\n683:\tlearn: 0.5519648\ttotal: 23.7s\tremaining: 11s\n684:\tlearn: 0.5515995\ttotal: 23.8s\tremaining: 10.9s\n685:\tlearn: 0.5512503\ttotal: 23.8s\tremaining: 10.9s\n686:\tlearn: 0.5506586\ttotal: 23.8s\tremaining: 10.8s\n687:\tlearn: 0.5503891\ttotal: 23.8s\tremaining: 10.8s\n688:\tlearn: 0.5498565\ttotal: 23.8s\tremaining: 10.7s\n689:\tlearn: 0.5495496\ttotal: 23.8s\tremaining: 10.7s\n690:\tlearn: 0.5493096\ttotal: 23.8s\tremaining: 10.6s\n691:\tlearn: 0.5490409\ttotal: 23.8s\tremaining: 10.6s\n692:\tlearn: 0.5486427\ttotal: 23.8s\tremaining: 10.6s\n693:\tlearn: 0.5480679\ttotal: 23.8s\tremaining: 10.5s\n694:\tlearn: 0.5476820\ttotal: 23.9s\tremaining: 10.5s\n695:\tlearn: 0.5472971\ttotal: 23.9s\tremaining: 10.4s\n696:\tlearn: 0.5469815\ttotal: 23.9s\tremaining: 10.4s\n697:\tlearn: 0.5466399\ttotal: 23.9s\tremaining: 10.3s\n698:\tlearn: 0.5462897\ttotal: 23.9s\tremaining: 10.3s\n699:\tlearn: 0.5458377\ttotal: 23.9s\tremaining: 10.2s\n700:\tlearn: 0.5454495\ttotal: 23.9s\tremaining: 10.2s\n701:\tlearn: 0.5450795\ttotal: 23.9s\tremaining: 10.2s\n702:\tlearn: 0.5447905\ttotal: 23.9s\tremaining: 10.1s\n703:\tlearn: 0.5443217\ttotal: 23.9s\tremaining: 10.1s\n704:\tlearn: 0.5437687\ttotal: 24s\tremaining: 10s\n705:\tlearn: 0.5433580\ttotal: 24s\tremaining: 9.98s\n706:\tlearn: 0.5428984\ttotal: 24s\tremaining: 9.94s\n707:\tlearn: 0.5425082\ttotal: 24s\tremaining: 9.89s\n708:\tlearn: 0.5422233\ttotal: 24s\tremaining: 9.85s\n709:\tlearn: 0.5419128\ttotal: 24s\tremaining: 9.8s\n710:\tlearn: 0.5415767\ttotal: 24s\tremaining: 9.76s\n711:\tlearn: 0.5412206\ttotal: 24s\tremaining: 9.72s\n712:\tlearn: 0.5407841\ttotal: 24s\tremaining: 9.68s\n713:\tlearn: 0.5404953\ttotal: 24s\tremaining: 9.63s\n714:\tlearn: 0.5400703\ttotal: 24.1s\tremaining: 9.59s\n715:\tlearn: 0.5396652\ttotal: 24.1s\tremaining: 9.55s\n716:\tlearn: 0.5393315\ttotal: 24.1s\tremaining: 9.5s\n717:\tlearn: 0.5390374\ttotal: 24.1s\tremaining: 9.46s\n718:\tlearn: 0.5386292\ttotal: 24.1s\tremaining: 9.42s\n719:\tlearn: 0.5382531\ttotal: 24.1s\tremaining: 9.37s\n720:\tlearn: 0.5377719\ttotal: 24.1s\tremaining: 9.33s\n721:\tlearn: 0.5374809\ttotal: 24.1s\tremaining: 9.29s\n722:\tlearn: 0.5371240\ttotal: 24.1s\tremaining: 9.25s\n723:\tlearn: 0.5368275\ttotal: 24.1s\tremaining: 9.2s\n724:\tlearn: 0.5365243\ttotal: 24.2s\tremaining: 9.16s\n725:\tlearn: 0.5362422\ttotal: 24.2s\tremaining: 9.12s\n726:\tlearn: 0.5358731\ttotal: 24.2s\tremaining: 9.08s\n727:\tlearn: 0.5354715\ttotal: 24.2s\tremaining: 9.04s\n728:\tlearn: 0.5350686\ttotal: 24.2s\tremaining: 8.99s\n729:\tlearn: 0.5348138\ttotal: 24.2s\tremaining: 8.95s\n730:\tlearn: 0.5342865\ttotal: 24.2s\tremaining: 8.91s\n731:\tlearn: 0.5339547\ttotal: 24.2s\tremaining: 8.87s\n732:\tlearn: 0.5336213\ttotal: 24.2s\tremaining: 8.83s\n733:\tlearn: 0.5333104\ttotal: 24.2s\tremaining: 8.79s\n734:\tlearn: 0.5329573\ttotal: 24.3s\tremaining: 8.74s\n735:\tlearn: 0.5326016\ttotal: 24.3s\tremaining: 8.7s\n736:\tlearn: 0.5321606\ttotal: 24.3s\tremaining: 8.66s\n737:\tlearn: 0.5316339\ttotal: 24.3s\tremaining: 8.62s\n738:\tlearn: 0.5313454\ttotal: 24.3s\tremaining: 8.58s\n739:\tlearn: 0.5308567\ttotal: 24.3s\tremaining: 8.54s\n740:\tlearn: 0.5305939\ttotal: 24.3s\tremaining: 8.5s\n741:\tlearn: 0.5301370\ttotal: 24.3s\tremaining: 8.46s\n742:\tlearn: 0.5298104\ttotal: 24.3s\tremaining: 8.42s\n743:\tlearn: 0.5293531\ttotal: 24.3s\tremaining: 8.38s\n744:\tlearn: 0.5288668\ttotal: 24.4s\tremaining: 8.34s\n745:\tlearn: 0.5282614\ttotal: 24.4s\tremaining: 8.3s\n746:\tlearn: 0.5278968\ttotal: 24.4s\tremaining: 8.26s\n747:\tlearn: 0.5274530\ttotal: 24.4s\tremaining: 8.22s\n748:\tlearn: 0.5269223\ttotal: 24.4s\tremaining: 8.18s\n749:\tlearn: 0.5266437\ttotal: 24.4s\tremaining: 8.13s\n750:\tlearn: 0.5263830\ttotal: 24.4s\tremaining: 8.1s\n751:\tlearn: 0.5260388\ttotal: 24.4s\tremaining: 8.06s\n752:\tlearn: 0.5254644\ttotal: 24.4s\tremaining: 8.02s\n753:\tlearn: 0.5250832\ttotal: 24.4s\tremaining: 7.98s\n754:\tlearn: 0.5248145\ttotal: 24.5s\tremaining: 7.94s\n755:\tlearn: 0.5244374\ttotal: 24.5s\tremaining: 7.9s\n756:\tlearn: 0.5242479\ttotal: 24.5s\tremaining: 7.86s\n757:\tlearn: 0.5238834\ttotal: 24.5s\tremaining: 7.82s\n758:\tlearn: 0.5233624\ttotal: 24.5s\tremaining: 7.78s\n759:\tlearn: 0.5230107\ttotal: 24.5s\tremaining: 7.74s\n760:\tlearn: 0.5227133\ttotal: 24.5s\tremaining: 7.7s\n761:\tlearn: 0.5224040\ttotal: 24.5s\tremaining: 7.66s\n762:\tlearn: 0.5222109\ttotal: 24.5s\tremaining: 7.62s\n763:\tlearn: 0.5218207\ttotal: 24.6s\tremaining: 7.58s\n764:\tlearn: 0.5216288\ttotal: 24.6s\tremaining: 7.54s\n765:\tlearn: 0.5213479\ttotal: 24.6s\tremaining: 7.5s\n766:\tlearn: 0.5210199\ttotal: 24.6s\tremaining: 7.47s\n767:\tlearn: 0.5206102\ttotal: 24.6s\tremaining: 7.43s\n768:\tlearn: 0.5203508\ttotal: 24.6s\tremaining: 7.39s\n769:\tlearn: 0.5198888\ttotal: 24.6s\tremaining: 7.35s\n770:\tlearn: 0.5195302\ttotal: 24.6s\tremaining: 7.31s\n771:\tlearn: 0.5190494\ttotal: 24.6s\tremaining: 7.27s\n772:\tlearn: 0.5185468\ttotal: 24.6s\tremaining: 7.24s\n773:\tlearn: 0.5180808\ttotal: 24.6s\tremaining: 7.2s\n774:\tlearn: 0.5177081\ttotal: 24.7s\tremaining: 7.16s\n775:\tlearn: 0.5173287\ttotal: 24.7s\tremaining: 7.12s\n776:\tlearn: 0.5168595\ttotal: 24.7s\tremaining: 7.08s\n777:\tlearn: 0.5165625\ttotal: 24.7s\tremaining: 7.04s\n778:\tlearn: 0.5161550\ttotal: 24.7s\tremaining: 7.01s\n779:\tlearn: 0.5158176\ttotal: 24.7s\tremaining: 6.97s\n780:\tlearn: 0.5155392\ttotal: 24.7s\tremaining: 6.93s\n781:\tlearn: 0.5152387\ttotal: 24.7s\tremaining: 6.89s\n782:\tlearn: 0.5147736\ttotal: 24.7s\tremaining: 6.86s\n783:\tlearn: 0.5144506\ttotal: 24.8s\tremaining: 6.82s\n784:\tlearn: 0.5140075\ttotal: 24.8s\tremaining: 6.78s\n785:\tlearn: 0.5137034\ttotal: 24.8s\tremaining: 6.74s\n786:\tlearn: 0.5134159\ttotal: 24.8s\tremaining: 6.71s\n787:\tlearn: 0.5130530\ttotal: 24.8s\tremaining: 6.67s\n788:\tlearn: 0.5126605\ttotal: 24.8s\tremaining: 6.63s\n789:\tlearn: 0.5122390\ttotal: 24.8s\tremaining: 6.59s\n790:\tlearn: 0.5118776\ttotal: 24.8s\tremaining: 6.56s\n791:\tlearn: 0.5115879\ttotal: 24.8s\tremaining: 6.52s\n792:\tlearn: 0.5111616\ttotal: 24.8s\tremaining: 6.48s\n793:\tlearn: 0.5107109\ttotal: 24.9s\tremaining: 6.45s\n794:\tlearn: 0.5103962\ttotal: 24.9s\tremaining: 6.41s\n795:\tlearn: 0.5099387\ttotal: 24.9s\tremaining: 6.37s\n796:\tlearn: 0.5095837\ttotal: 24.9s\tremaining: 6.34s\n797:\tlearn: 0.5090412\ttotal: 24.9s\tremaining: 6.3s\n798:\tlearn: 0.5088614\ttotal: 24.9s\tremaining: 6.26s\n799:\tlearn: 0.5085331\ttotal: 24.9s\tremaining: 6.23s\n800:\tlearn: 0.5082125\ttotal: 24.9s\tremaining: 6.19s\n801:\tlearn: 0.5078857\ttotal: 24.9s\tremaining: 6.16s\n802:\tlearn: 0.5075876\ttotal: 24.9s\tremaining: 6.12s\n803:\tlearn: 0.5070687\ttotal: 25s\tremaining: 6.08s\n804:\tlearn: 0.5066872\ttotal: 25s\tremaining: 6.05s\n805:\tlearn: 0.5063226\ttotal: 25s\tremaining: 6.01s\n806:\tlearn: 0.5060016\ttotal: 25s\tremaining: 5.97s\n807:\tlearn: 0.5055982\ttotal: 25s\tremaining: 5.94s\n808:\tlearn: 0.5052986\ttotal: 25s\tremaining: 5.9s\n809:\tlearn: 0.5047834\ttotal: 25s\tremaining: 5.87s\n810:\tlearn: 0.5043232\ttotal: 25s\tremaining: 5.83s\n811:\tlearn: 0.5040640\ttotal: 25s\tremaining: 5.79s\n812:\tlearn: 0.5038351\ttotal: 25s\tremaining: 5.76s\n813:\tlearn: 0.5035070\ttotal: 25s\tremaining: 5.72s\n814:\tlearn: 0.5031877\ttotal: 25.1s\tremaining: 5.69s\n815:\tlearn: 0.5025839\ttotal: 25.1s\tremaining: 5.65s\n816:\tlearn: 0.5020933\ttotal: 25.1s\tremaining: 5.62s\n817:\tlearn: 0.5018456\ttotal: 25.1s\tremaining: 5.58s\n818:\tlearn: 0.5016393\ttotal: 25.1s\tremaining: 5.55s\n819:\tlearn: 0.5013559\ttotal: 25.1s\tremaining: 5.51s\n820:\tlearn: 0.5009476\ttotal: 25.1s\tremaining: 5.48s\n821:\tlearn: 0.5006973\ttotal: 25.1s\tremaining: 5.44s\n822:\tlearn: 0.5004350\ttotal: 25.1s\tremaining: 5.41s\n823:\tlearn: 0.5001339\ttotal: 25.1s\tremaining: 5.37s\n824:\tlearn: 0.4997618\ttotal: 25.2s\tremaining: 5.34s\n825:\tlearn: 0.4993834\ttotal: 25.2s\tremaining: 5.3s\n826:\tlearn: 0.4990565\ttotal: 25.2s\tremaining: 5.27s\n827:\tlearn: 0.4986067\ttotal: 25.2s\tremaining: 5.23s\n828:\tlearn: 0.4982606\ttotal: 25.2s\tremaining: 5.2s\n829:\tlearn: 0.4979674\ttotal: 25.2s\tremaining: 5.16s\n830:\tlearn: 0.4977245\ttotal: 25.2s\tremaining: 5.13s\n831:\tlearn: 0.4973766\ttotal: 25.2s\tremaining: 5.09s\n832:\tlearn: 0.4969028\ttotal: 25.2s\tremaining: 5.06s\n833:\tlearn: 0.4966620\ttotal: 25.2s\tremaining: 5.03s\n834:\tlearn: 0.4963412\ttotal: 25.3s\tremaining: 4.99s\n835:\tlearn: 0.4959427\ttotal: 25.3s\tremaining: 4.96s\n836:\tlearn: 0.4955153\ttotal: 25.3s\tremaining: 4.92s\n837:\tlearn: 0.4951906\ttotal: 25.3s\tremaining: 4.89s\n838:\tlearn: 0.4950029\ttotal: 25.3s\tremaining: 4.85s\n839:\tlearn: 0.4946341\ttotal: 25.3s\tremaining: 4.82s\n840:\tlearn: 0.4942161\ttotal: 25.3s\tremaining: 4.79s\n841:\tlearn: 0.4939372\ttotal: 25.3s\tremaining: 4.75s\n842:\tlearn: 0.4936591\ttotal: 25.3s\tremaining: 4.72s\n843:\tlearn: 0.4934259\ttotal: 25.3s\tremaining: 4.68s\n844:\tlearn: 0.4931018\ttotal: 25.4s\tremaining: 4.65s\n845:\tlearn: 0.4927033\ttotal: 25.4s\tremaining: 4.62s\n846:\tlearn: 0.4923641\ttotal: 25.4s\tremaining: 4.58s\n847:\tlearn: 0.4920129\ttotal: 25.4s\tremaining: 4.55s\n848:\tlearn: 0.4915547\ttotal: 25.4s\tremaining: 4.52s\n849:\tlearn: 0.4911226\ttotal: 25.4s\tremaining: 4.48s\n850:\tlearn: 0.4907593\ttotal: 25.4s\tremaining: 4.45s\n851:\tlearn: 0.4903659\ttotal: 25.4s\tremaining: 4.42s\n852:\tlearn: 0.4901304\ttotal: 25.4s\tremaining: 4.38s\n853:\tlearn: 0.4898750\ttotal: 25.4s\tremaining: 4.35s\n854:\tlearn: 0.4895321\ttotal: 25.5s\tremaining: 4.32s\n855:\tlearn: 0.4891614\ttotal: 25.5s\tremaining: 4.28s\n856:\tlearn: 0.4889430\ttotal: 25.5s\tremaining: 4.25s\n857:\tlearn: 0.4884800\ttotal: 25.5s\tremaining: 4.22s\n858:\tlearn: 0.4881683\ttotal: 25.5s\tremaining: 4.18s\n859:\tlearn: 0.4876933\ttotal: 25.5s\tremaining: 4.15s\n860:\tlearn: 0.4872568\ttotal: 25.5s\tremaining: 4.12s\n861:\tlearn: 0.4869554\ttotal: 25.5s\tremaining: 4.09s\n862:\tlearn: 0.4866187\ttotal: 25.5s\tremaining: 4.05s\n863:\tlearn: 0.4861741\ttotal: 25.5s\tremaining: 4.02s\n864:\tlearn: 0.4858508\ttotal: 25.6s\tremaining: 3.99s\n865:\tlearn: 0.4856505\ttotal: 25.6s\tremaining: 3.96s\n866:\tlearn: 0.4853948\ttotal: 25.6s\tremaining: 3.92s\n867:\tlearn: 0.4849226\ttotal: 25.6s\tremaining: 3.89s\n868:\tlearn: 0.4846037\ttotal: 25.6s\tremaining: 3.86s\n869:\tlearn: 0.4841251\ttotal: 25.6s\tremaining: 3.83s\n870:\tlearn: 0.4837770\ttotal: 25.6s\tremaining: 3.79s\n871:\tlearn: 0.4834252\ttotal: 25.6s\tremaining: 3.76s\n872:\tlearn: 0.4831171\ttotal: 25.6s\tremaining: 3.73s\n873:\tlearn: 0.4827874\ttotal: 25.6s\tremaining: 3.7s\n874:\tlearn: 0.4824155\ttotal: 25.7s\tremaining: 3.66s\n875:\tlearn: 0.4819911\ttotal: 25.7s\tremaining: 3.63s\n876:\tlearn: 0.4816414\ttotal: 25.7s\tremaining: 3.6s\n877:\tlearn: 0.4813609\ttotal: 25.7s\tremaining: 3.57s\n878:\tlearn: 0.4811049\ttotal: 25.7s\tremaining: 3.54s\n879:\tlearn: 0.4808853\ttotal: 25.7s\tremaining: 3.5s\n880:\tlearn: 0.4805557\ttotal: 25.7s\tremaining: 3.47s\n881:\tlearn: 0.4802064\ttotal: 25.7s\tremaining: 3.44s\n882:\tlearn: 0.4800008\ttotal: 25.7s\tremaining: 3.41s\n883:\tlearn: 0.4796555\ttotal: 25.7s\tremaining: 3.38s\n884:\tlearn: 0.4794393\ttotal: 25.7s\tremaining: 3.35s\n885:\tlearn: 0.4790423\ttotal: 25.8s\tremaining: 3.31s\n886:\tlearn: 0.4788696\ttotal: 25.8s\tremaining: 3.28s\n887:\tlearn: 0.4786051\ttotal: 25.8s\tremaining: 3.25s\n888:\tlearn: 0.4783801\ttotal: 25.8s\tremaining: 3.22s\n889:\tlearn: 0.4780057\ttotal: 25.8s\tremaining: 3.19s\n890:\tlearn: 0.4775736\ttotal: 25.8s\tremaining: 3.16s\n891:\tlearn: 0.4772547\ttotal: 25.8s\tremaining: 3.13s\n892:\tlearn: 0.4767469\ttotal: 25.8s\tremaining: 3.09s\n893:\tlearn: 0.4764989\ttotal: 25.8s\tremaining: 3.06s\n894:\tlearn: 0.4761433\ttotal: 25.8s\tremaining: 3.03s\n895:\tlearn: 0.4759145\ttotal: 25.9s\tremaining: 3s\n896:\tlearn: 0.4756510\ttotal: 25.9s\tremaining: 2.97s\n897:\tlearn: 0.4753507\ttotal: 25.9s\tremaining: 2.94s\n898:\tlearn: 0.4749611\ttotal: 25.9s\tremaining: 2.91s\n899:\tlearn: 0.4746538\ttotal: 25.9s\tremaining: 2.88s\n900:\tlearn: 0.4744965\ttotal: 25.9s\tremaining: 2.85s\n901:\tlearn: 0.4742055\ttotal: 25.9s\tremaining: 2.81s\n902:\tlearn: 0.4739421\ttotal: 25.9s\tremaining: 2.78s\n903:\tlearn: 0.4734473\ttotal: 25.9s\tremaining: 2.75s\n904:\tlearn: 0.4731547\ttotal: 25.9s\tremaining: 2.72s\n905:\tlearn: 0.4729038\ttotal: 26s\tremaining: 2.69s\n906:\tlearn: 0.4726546\ttotal: 26s\tremaining: 2.66s\n907:\tlearn: 0.4723404\ttotal: 26s\tremaining: 2.63s\n908:\tlearn: 0.4720512\ttotal: 26s\tremaining: 2.6s\n909:\tlearn: 0.4715894\ttotal: 26s\tremaining: 2.57s\n910:\tlearn: 0.4713457\ttotal: 26s\tremaining: 2.54s\n911:\tlearn: 0.4709994\ttotal: 26s\tremaining: 2.51s\n912:\tlearn: 0.4708240\ttotal: 26s\tremaining: 2.48s\n913:\tlearn: 0.4703301\ttotal: 26s\tremaining: 2.45s\n914:\tlearn: 0.4701316\ttotal: 26s\tremaining: 2.42s\n915:\tlearn: 0.4698884\ttotal: 26s\tremaining: 2.39s\n916:\tlearn: 0.4696379\ttotal: 26.1s\tremaining: 2.36s\n917:\tlearn: 0.4693258\ttotal: 26.1s\tremaining: 2.33s\n918:\tlearn: 0.4689432\ttotal: 26.1s\tremaining: 2.3s\n919:\tlearn: 0.4686196\ttotal: 26.1s\tremaining: 2.27s\n920:\tlearn: 0.4683944\ttotal: 26.1s\tremaining: 2.24s\n921:\tlearn: 0.4679873\ttotal: 26.1s\tremaining: 2.21s\n922:\tlearn: 0.4676510\ttotal: 26.1s\tremaining: 2.18s\n923:\tlearn: 0.4672352\ttotal: 26.1s\tremaining: 2.15s\n924:\tlearn: 0.4668670\ttotal: 26.1s\tremaining: 2.12s\n925:\tlearn: 0.4665260\ttotal: 26.1s\tremaining: 2.09s\n926:\tlearn: 0.4661956\ttotal: 26.2s\tremaining: 2.06s\n927:\tlearn: 0.4658812\ttotal: 26.2s\tremaining: 2.03s\n928:\tlearn: 0.4656195\ttotal: 26.2s\tremaining: 2s\n929:\tlearn: 0.4653349\ttotal: 26.2s\tremaining: 1.97s\n930:\tlearn: 0.4649447\ttotal: 26.2s\tremaining: 1.94s\n931:\tlearn: 0.4644839\ttotal: 26.2s\tremaining: 1.91s\n932:\tlearn: 0.4640556\ttotal: 26.2s\tremaining: 1.88s\n933:\tlearn: 0.4636502\ttotal: 26.2s\tremaining: 1.85s\n934:\tlearn: 0.4632208\ttotal: 26.2s\tremaining: 1.82s\n935:\tlearn: 0.4628271\ttotal: 26.2s\tremaining: 1.79s\n936:\tlearn: 0.4624809\ttotal: 26.3s\tremaining: 1.76s\n937:\tlearn: 0.4621751\ttotal: 26.3s\tremaining: 1.74s\n938:\tlearn: 0.4619697\ttotal: 26.3s\tremaining: 1.71s\n939:\tlearn: 0.4615754\ttotal: 26.3s\tremaining: 1.68s\n940:\tlearn: 0.4613380\ttotal: 26.3s\tremaining: 1.65s\n941:\tlearn: 0.4610169\ttotal: 26.3s\tremaining: 1.62s\n942:\tlearn: 0.4607673\ttotal: 26.3s\tremaining: 1.59s\n943:\tlearn: 0.4604536\ttotal: 26.3s\tremaining: 1.56s\n944:\tlearn: 0.4600617\ttotal: 26.3s\tremaining: 1.53s\n945:\tlearn: 0.4595761\ttotal: 26.3s\tremaining: 1.5s\n946:\tlearn: 0.4592482\ttotal: 26.4s\tremaining: 1.48s\n947:\tlearn: 0.4589500\ttotal: 26.4s\tremaining: 1.45s\n948:\tlearn: 0.4586003\ttotal: 26.4s\tremaining: 1.42s\n949:\tlearn: 0.4584502\ttotal: 26.4s\tremaining: 1.39s\n950:\tlearn: 0.4580535\ttotal: 26.4s\tremaining: 1.36s\n951:\tlearn: 0.4578643\ttotal: 26.4s\tremaining: 1.33s\n952:\tlearn: 0.4575604\ttotal: 26.4s\tremaining: 1.3s\n953:\tlearn: 0.4571339\ttotal: 26.4s\tremaining: 1.27s\n954:\tlearn: 0.4567754\ttotal: 26.4s\tremaining: 1.25s\n955:\tlearn: 0.4564819\ttotal: 26.4s\tremaining: 1.22s\n956:\tlearn: 0.4562198\ttotal: 26.5s\tremaining: 1.19s\n957:\tlearn: 0.4559293\ttotal: 26.5s\tremaining: 1.16s\n958:\tlearn: 0.4555220\ttotal: 26.5s\tremaining: 1.13s\n959:\tlearn: 0.4553048\ttotal: 26.5s\tremaining: 1.1s\n960:\tlearn: 0.4549766\ttotal: 26.5s\tremaining: 1.07s\n961:\tlearn: 0.4546578\ttotal: 26.5s\tremaining: 1.05s\n962:\tlearn: 0.4542634\ttotal: 26.5s\tremaining: 1.02s\n963:\tlearn: 0.4540565\ttotal: 26.5s\tremaining: 991ms\n964:\tlearn: 0.4537900\ttotal: 26.5s\tremaining: 963ms\n965:\tlearn: 0.4535172\ttotal: 26.6s\tremaining: 935ms\n966:\tlearn: 0.4529546\ttotal: 26.6s\tremaining: 907ms\n967:\tlearn: 0.4526094\ttotal: 26.6s\tremaining: 879ms\n968:\tlearn: 0.4522125\ttotal: 26.6s\tremaining: 851ms\n969:\tlearn: 0.4518887\ttotal: 26.6s\tremaining: 823ms\n970:\tlearn: 0.4516068\ttotal: 26.6s\tremaining: 795ms\n971:\tlearn: 0.4514402\ttotal: 26.6s\tremaining: 767ms\n972:\tlearn: 0.4512332\ttotal: 26.6s\tremaining: 739ms\n973:\tlearn: 0.4508822\ttotal: 26.6s\tremaining: 711ms\n974:\tlearn: 0.4506923\ttotal: 26.7s\tremaining: 683ms\n975:\tlearn: 0.4504558\ttotal: 26.7s\tremaining: 656ms\n976:\tlearn: 0.4499289\ttotal: 26.7s\tremaining: 628ms\n977:\tlearn: 0.4497142\ttotal: 26.7s\tremaining: 600ms\n978:\tlearn: 0.4496049\ttotal: 26.7s\tremaining: 573ms\n979:\tlearn: 0.4494096\ttotal: 26.7s\tremaining: 545ms\n980:\tlearn: 0.4490094\ttotal: 26.7s\tremaining: 517ms\n981:\tlearn: 0.4487298\ttotal: 26.7s\tremaining: 490ms\n982:\tlearn: 0.4483787\ttotal: 26.7s\tremaining: 462ms\n983:\tlearn: 0.4480101\ttotal: 26.7s\tremaining: 435ms\n984:\tlearn: 0.4477130\ttotal: 26.8s\tremaining: 407ms\n985:\tlearn: 0.4474928\ttotal: 26.8s\tremaining: 380ms\n986:\tlearn: 0.4471898\ttotal: 26.8s\tremaining: 353ms\n987:\tlearn: 0.4468906\ttotal: 26.8s\tremaining: 325ms\n988:\tlearn: 0.4464745\ttotal: 26.8s\tremaining: 298ms\n989:\tlearn: 0.4461531\ttotal: 26.8s\tremaining: 271ms\n990:\tlearn: 0.4458197\ttotal: 26.8s\tremaining: 243ms\n991:\tlearn: 0.4455325\ttotal: 26.8s\tremaining: 216ms\n992:\tlearn: 0.4452281\ttotal: 26.8s\tremaining: 189ms\n993:\tlearn: 0.4450337\ttotal: 26.8s\tremaining: 162ms\n994:\tlearn: 0.4446855\ttotal: 26.9s\tremaining: 135ms\n995:\tlearn: 0.4443014\ttotal: 26.9s\tremaining: 108ms\n996:\tlearn: 0.4440416\ttotal: 26.9s\tremaining: 80.9ms\n997:\tlearn: 0.4437877\ttotal: 26.9s\tremaining: 53.9ms\n998:\tlearn: 0.4435564\ttotal: 26.9s\tremaining: 26.9ms\n999:\tlearn: 0.4431650\ttotal: 26.9s\tremaining: 0us\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"<catboost.core.CatBoostClassifier at 0x7b72d691ae60>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"test_pred = clf.predict(test_embeds)\ntest_proba = clf.predict_proba(test_embeds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:01:08.552137Z","iopub.execute_input":"2025-05-11T15:01:08.552879Z","iopub.status.idle":"2025-05-11T15:01:08.610988Z","shell.execute_reply.started":"2025-05-11T15:01:08.552844Z","shell.execute_reply":"2025-05-11T15:01:08.610224Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"print(\"Accuracy:\", accuracy_score(target_test, test_pred))\nprint(\"ROC-AUC:\", roc_auc_score(target_test, test_proba, average=\"weighted\", multi_class=\"ovr\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:01:10.495713Z","iopub.execute_input":"2025-05-11T15:01:10.496592Z","iopub.status.idle":"2025-05-11T15:01:10.513291Z","shell.execute_reply.started":"2025-05-11T15:01:10.496558Z","shell.execute_reply":"2025-05-11T15:01:10.512417Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.57\nROC-AUC: 0.8227406048367972\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T21:40:36.836392Z","iopub.execute_input":"2025-05-10T21:40:36.836823Z","iopub.status.idle":"2025-05-10T21:40:36.873908Z","shell.execute_reply.started":"2025-05-10T21:40:36.836783Z","shell.execute_reply":"2025-05-10T21:40:36.872820Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"arr = np.array([0.8227406048367972, 0.8311162237991545, 0.8247213858065967])\n\narr.mean(), arr.std()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:07:46.561548Z","iopub.execute_input":"2025-05-11T15:07:46.561927Z","iopub.status.idle":"2025-05-11T15:07:46.568426Z","shell.execute_reply.started":"2025-05-11T15:07:46.561898Z","shell.execute_reply":"2025-05-11T15:07:46.567517Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(0.8261927381475161, 0.0035741112213618035)"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"- CPC context embeds + Catboost:\n   - `Accuracy: 0.5773333333333334`, `0.5686666666666667`, `0.5826666666666667`, avg: `0.5762 +- 0.0058`\n   - ` ROC-AUC: 0.830123007110738`, `0.8271157616313021`, `0.8343491131233265`, avg: `0.8305 +- 0.003`\n\n---\n\n<!-- - CPC context embeds w/ SWIN_Agg seq_enc + Catboost:\n  - Accuracy: `0.581`, `0.5646666666666667`, `0.5726666666666667`, avg: `0.5727 +- 0.0067`\n  - ROC-AUC: `0.8291802131654565`, `0.8185063563468156`, `0.8215564372620066`, avg: `0.8231 +- 0.0045`\n  \n---\n-->\n\n- CPC context embeds w/ SWIN_Agg seq_enc + Catboost:\n   - `Accuracy: 0.59`, `0.5726666666666667`, `0.5736666666666667`, avg: `0.5788 +- 0.0079`\n   - `ROC-AUC: 0.8291468943509576`, `0.8264141264285515`, `0.8222129523643`, avg: `0.8259 +- 0.0029`\n\n<!-- ---\n\n- CPC context embeds w/ SWIN_Agg seq_enc (w/ look-ahead mask) + Catboost:\n  - Accuracy: `0.579`, `0.554`, `0.569`, avg: `0.5673 +- 0.0103`\n  - ROC-AUC: `0.8309040879417245`, `0.817863458479023`, `0.8264009551174533`, avg: `0.8251 +- 0.0054` -->\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc (w/ look-ahead mask) + Catboost:\n  - Accuracy: `0.5806666666666667`, `0.5723333333333334`, `0.5636666666666666`, avg: `0.5722 +- 0.0069`\n  - ROC-AUC: `0.8274773162162774`, `0.8273407125045609`, `0.8207535939209638`, avg: `0.8252 +- 0.0031`\n\n---\n\n- CPC context embeds w/ SWIN_Agg seq_enc & ConvAgg (3 trx) + Catboost:\n  - Accuracy: `0.57`, `0.5756666666666667`, `0.5756666666666667`, avg: `0.5738 +- 0.0027`\n  - ROC-AUC: `0.8227406048367972`, `0.8311162237991545`, `0.8247213858065967`, avg: `0.8262 +- 0.0036`\n\n---\n\n**Вывод:** Для CPC замена энкодера на SWIN-трансформер приводит к значительному повышению accuracy и к сильному спаду по ROC-AUC в сравнении с бейзлайном. При этом сетап с attn-маской на инференсе демонстрирует худшие результаты по сравнению с вариантом без неё.\n\nСетап со свёрточной агрегацией и SWIN-энкодером демонстрирует несколько лучший ROC-AUC, чем в случае с обычным транзакционным энкодером и SWIN-энкодером, который тем не менее всё ещё хуже, чем в случае бейзлайна, но также - значительный спад по accuracy по сравнению со случаем с обычным транзакционным энкодером и SWIN-энкодером и даже - по сравнению с бейзлайном.\n\n\n**Конфигурация, лучшая по метрикам:** \n\n- CPC context embeds w/ SWIN_Agg seq_enc + Catboost:\n   - `Accuracy: 0.59`, `0.5726666666666667`, `0.5736666666666667`, avg: `0.5788 +- 0.0079`\n   - `ROC-AUC: 0.8291468943509576`, `0.8264141264285515`, `0.8222129523643`, avg: `0.8259 +- 0.0029`","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Итоги.","metadata":{}},{"cell_type":"markdown","source":"| Method|Accuracy|ROC-AUC|\n| --- |:---:|:---:|\n| **Flattened Sequences**                   | 0.4921 ± 0.005        | 0.76 ± 0.0012   |\n| **GRU (+ MLP)**                           | 0.6066 ± 0.0019       | 0.8479 ± 0.0013 |\n| **CoLES**                                 | 0.6042 ± 0.0083       | 0.8482 ± 0.0007 |\n| **COLES embeds w/ SWIN Agg encoder**      | 0.5968 ± 0.0036       | 0.8437 ± 0.0016 |\n| **CPC Modeling**                          | 0.5762 ± 0.0058       | 0.8305 ± 0.003  |\n| **CPC Modeling w/ SWIN Agg encoder**      | 0.5788 ± 0.0079       | 0.8259 ± 0.0029 |\n| **GPT2**                                  | 0.6146 ± 0.0075       | 0.852 ± 0.0029  |","metadata":{}}]}